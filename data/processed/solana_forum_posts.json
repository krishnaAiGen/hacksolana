{
    "Governance": [
        {
            "id": 484,
            "title": "A Framework for Governance - Introduction",
            "url": "https://forum.solana.com/t/a-framework-for-governance-introduction/484",
            "created_at": "2023-08-31T13:04:31.101Z",
            "posts_count": 4,
            "views": 1986,
            "reply_count": 0,
            "last_posted_at": "2024-07-23T11:47:35.494Z",
            "category_id": 11,
            "category_name": "Governance",
            "description": "Solana has reached a certain level of maturity, with almost 3.5yrs of Mainnet-Beta, the development of an independent validator client (Firedancer), alternative releases of the default client (Jito), a growing distribution of stake (NC of 30) and a vibrant community of validators, developers, NFT communities and more.\nWith this in mind, it is proposed to develop a formalized framework and implementation for governance on Solana.\nThis is necessary to foster continued decentralization of power in the network, moving away from single decision-making authorities and enabling sustainable collaboration between client teams.\nGovernance on Solana currently exists minimally and is used only rarely. Primarily protocol-altering changes go through levels of social consensus with ultimate authority resting with the core engineers at Solana Labs.\nOn-chain voting is possible through the feature program, however lacks functionality and visibility and has barely been used in practice.\nA group of interested community members is developing ideas around governance to identify the most suitable path forward.\nAd-hoc discussion takes place in the Staking Alliance Discord and ideas are being captures on this Gitbook\nThis forum will be used for more specific and formulated proposals to be discussed and finalized, with the intention of then putting these to a vote.\nThe goal is to develop a framework for governance, with further discussion taking place at Breakpoint 2023 in Amsterdam, and beginning an implementation in Q1 2024.\nTo ensure low friction and a smooth transition to an active governance ecosystem the initial scope should be limited in scope to the most critical aspects to maintain focus and avoid governance fatigue amongst participants.",
            "comments": "[laine]: \n\n[cfl0ws]: Thanks for posting these. To help orient those new to the discussions, we’ve been focusing conversations on the following topics -\nWhy\nThere’s been a general agreement among discussion participants to-date that a more formalized and inclusive governance process is necessary for reasons described in the “Why” section of the Gitbook linked above (I’m limited to only two links as a new user.)\nWho\n“Who” and “What” are the areas we’ve been focused on defining first, as pre-requisites for determining the “How”.\nCurrently three “Who” proposals are on the table and can be discussed here\nWhat\nCurrently two “What” proposals are on the table and can be discussed here\nHow\nThere’s been a general agreement among discussion participants to-date that a more formalized and inclusive governance process.\n\n[Gabynto]: Hey there,\nThanks for sharing these thoughts on Solana’s governance! It’s exciting to see the community gearing up for more structured decision-making. Moving towards a formalized framework makes a lot of sense given Solana’s growth and diversity of stakeholders.\nIt’s clear there’s already a solid foundation with nearly 3.5 years on Mainnet-Beta, alternative client developments like Firedancer and Jito, and a vibrant mix of validators, developers, and NFT communities.\nYour point about decentralizing power and fostering collaboration between client teams is crucial for long-term sustainability. I agree that while on-chain voting exists, it’s not yet fully utilized. Transitioning to a more active governance model could really empower the community and streamline decision-making.\nI’m glad to hear there’s momentum building around this, with discussions happening in places like the Staking Alliance Discord and documented ideas on Gitbook. And planning to finalize proposals at Breakpoint 2023 sounds like a great step forward.\nStarting small and focused makes a lot of sense to avoid overload and keep everyone engaged. This approach will help in laying down a solid foundation without overwhelming participants.\nLooking forward to seeing how these ideas evolve and contribute to Solana’s future. Let’s keep the momentum going!\n\n",
            "comment_count": 3,
            "original_poster": "laine",
            "activity": "2024-07-23T11:47:35.494Z"
        },
        {
            "id": 436,
            "title": "About the Governance category",
            "url": "https://forum.solana.com/t/about-the-governance-category/436",
            "created_at": "2023-08-07T15:50:19.071Z",
            "posts_count": 1,
            "views": 462,
            "reply_count": 0,
            "last_posted_at": null,
            "category_id": 11,
            "category_name": "Governance",
            "description": "A place for discussion on governance for the Solana network. This can include feature activations, validator voting mechanisms, and more.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "jacobcreech",
            "activity": null
        },
        {
            "id": 3295,
            "title": "Proposal for an In-Protocol Distribution of Block Rewards to Stakers",
            "url": "https://forum.solana.com/t/proposal-for-an-in-protocol-distribution-of-block-rewards-to-stakers/3295",
            "created_at": "2025-02-25T00:30:45.989Z",
            "posts_count": 25,
            "views": 1528,
            "reply_count": 6,
            "last_posted_at": "2025-03-14T00:25:12.002Z",
            "category_id": 11,
            "category_name": "Governance",
            "description": "Summary\nA new mechanism is proposed to allow validators to set a block reward commission and share part of their block revenue with their delegators and to receive their own block rewards to an account of their choice.\nCommission rates from validator vote accounts will be used by the protocol to calculate post-commission rewards that will be automatically distributed to delegated stake accounts at the end of each epoch\nBlock rewards after commission will be distributed to an account of the validators choosing. The default will be the validator identity account. If a validator takes no action then block rewards will continue go to the Identity account.\nMotivation\nDelegated stake directly increases the number of blocks that a validator is allocated in an epoch leader schedule but the core protocol doesn’t support diverting any of that extra revenue to stake delegators.\nDue to the lack of core protocol support for distributing block revenue to stakers, validators have developed their own solutions which are not enforced by the core protocol. For example, some validators use NFTs or LSTs to distribute some amount of their block revenue, however this requires trust in the validator’s honesty and accuracy, while making it difficult to surface this information and accurately track resulting yields.\nWith the option to specify a collector account validators can improve operational security by diverting their revenue into a multisig or cold wallet rather than the identity hot wallet that sits on their servers.\nAdditionally the ability to specify arbitrary collector accounts, including PDAs, means that additional custom functionality and distribution mechanisms can be built on top of this, such as auto-conversion to USDC or a validator LST, or deployment to Defi.\nChanges in the spirit of this proposal\nShould any changes be necessary to ensure a safe and functioning implementation, such changes will be permitted without further governance requirements so long as the spirit of the proposal is maintained.\nVoting Process\nThe voting process will proceed as follows:\nDiscussion period: Validators are encouraged to participate in discussions to address any concerns.\nStake weight collection period: Stake weights will be captured and published for voting. Validators will have the opportunity to verify these weights.\nVote token distribution will require validators to utilize the adapted Jito Merkle Distributor tool (available at GitHub - laine-sa/solgov-distributor: A merkle-based token distributor for the Solana network that allows distributing a combination of unlocked and linearly unlocked tokens.) to claim the vote tokens corresponding to their stake weights.\nThree token destination accounts will be created for voting choices: Yes, No, and Abstain.\nValidators will have a designated period to vote by sending their tokens to the respective addresses.\nAfter the voting period, if the sum of Yes votes is equal to or greater than 2/3 of the total sum of Yes + No votes, the proposal will pass.\nThe proposal has a quorum threshold of 33%, abstentions count towards the quorum.\nAll announcements regarding this process will be made in the Governance category of the Solana Developer Forums.\nStake weights and a tally script will be available at solgov-distributor/votes at master · laine-sa/solgov-distributor · GitHub\nTimeline\nEpoch 747 - 751: Discussion period\nEpoch 752: Stake weights captured and published, discussion/confirmation of stake weights\nEpochs 753 - 755: Voting tokens available to claim, voting completes at the end of epoch 755\nDiscussion\nActive participation in discussions about this proposal is crucial. Discussions may also take place on the Solana Developer Forums or on Discord Governance channel. It’s encouraged to consolidate discussions to ensure broad participation and minimize redundancy.\nReferences\n \n github.com/solana-foundation/solana-improvement-documents\n \n \n \n \n \n \n \n \n SIMD-0123: Block Revenue Distribution\n \n \n main ← jstarry:block-fee-distribution\n \n \n \n opened 08:20AM - 10 Mar 24 UTC\n \n \n \n \n jstarry\n \n \n \n \n +174\n -0\n \n \n \n \n \n Delegated stake directly increases the number of blocks that a validator is allo…cated in an epoch leader schedule but the core protocol doesn't support diverting any of that extra revenue to stake delegators. This proposal introduces a way to set a commission for validator collected block fees and tips and then have the rest of the fees distributed to delegated stake accounts.\n \n \n \n \n \n \n \n \n github.com\n \n \n \n \n GitHub - laine-sa/solgov-distributor: A merkle-based token distributor for the Solana...\n \nA merkle-based token distributor for the Solana network that allows distributing a combination of unlocked and linearly unlocked tokens.",
            "comments": "[byse]: This change will affect small validators greatly. They will be ineviteably forced by the stake pools to set their share of block rewards to 0 level. The only thing that will be left - side deals like sandwiching.\nAt the same time this change does not affect big private validators. They even still charge commission for inflation rewards with no outflow of stake noticed.\nSo the outcome for the network will be net bad:\nStakers possibly get +1% APY max. Which IMO does not matter considering the volatility of an asset.\nNo additional competition between stake pools as they will all have the same “selling point”. Nothing special to drive a lot of retail stake there really.\nNo effect on big private validators as they have no incentive to share block rewards\nGreat decrease in earnings for the validators depending on stake pools. This will cause decrease of number of those validators and their qualuity.\n\n[Leapfrog]: This proposal will no doubtably deter the casual developers / independent technologists / students / enthusiasts etc. from making the jump into Solana Validation (which has a fairly high bar as it stands) and becoming part of the active independent group of validators that run Solana. It will also suffocate existing validators and reduce the validator group to small subset of what it is.\nValidator earnings for an independent who is trying to compete for stake through offering a competitive APY will have no sustainable way of surviving if they are forced to give away even their block rewards from the small amount of stake they obtain. (This leaves no revenue stream other than offering their mempool out to sandwiches)\nNew / current Validators already have large running costs. And so making block rewards yet another race to zero competitive game will strangle the network and deter interested small participants who will simply invest their time and money into alternative networks. With them go their social networks too.\nI believe we need every validator we can get, so VOTE NO to this proposal, and so keep and encourage the vitality of Solana!\n\n[meyerbro]: The only goal of this proposal is to make some validators happy with legal implications of holding rewards of stakers for a moment before sending them after every epoch. They never had issues before while holding 50% of block rewards in the past though.\nBut the consequences of it will be the end of hundreds or even thousands of small validators as they will lose the last source of income they currently have.\nStake continues going to whales instead of small validators or pools and there are just a feel pools that currently support a great number of validators in a fair way. And these require all validators to be running on maximum commission of ZERO. If this proposal gets approved, pools will squeeze validators to the maximum and push them to negative income, while pools still enjoy a healthy 5% fee or more.\nThis SIMD only came because of SIMD-96 was approved when most validators voted yes to get block rewards that were being burnt! But now, with SIMD-123 we mostly regret this decision as if this gets approved we will not just lose this 50%, but possibly even more.\nThe approval of this SIMD-123 will cause great loss to the Solana community and even more centralization of stake and this is one of the reasons big validators (the usual influencers) are keen on getting this approved.\nPlease vote NO to this SIMD and keep us alive, everyone deserves to survive.\n\n[bullmoosesystems]: I do think this would be a nice feature, but also agree that it will completely wipe out the independent validator community. If this goes through, I would bet on &lt;300 total validators in the entire Solana ecosystem come the next bear market. Solana already struggles with a decentralization narrative given the performance requirement of machines, and this will make it worse.\nMaybe that’s ok and no one actually cares about decentralization that much, but seems like the risk is pretty high for a pretty small benefit to stakers.\n\n[mariaeverstake]: Hello everyone!\nWe would like to emphasize our support for the concerns surrounding this proposal, particularly its potential impact on the validator ecosystem. Implementing this proposal could result in the reduction of many small validators, significantly decreasing the total number of active validators on the network.\nThis reduction could, in turn, result in a higher degree of centralization. Such an outcome would contradict Solana’s broader vision of fostering a decentralized and resilient blockchain ecosystem.\nDecentralization is a fundamental principle that ensures the security, censorship resistance, and long-term sustainability of the network. We believe it is crucial to carefully consider the implications of this proposal to avoid unintended consequences that may hinder Solana’s growth and decentralization efforts.\n\n[adi_vb]: We will vote NO!\nAs most of other validators already commented this is not going to help the small validators. Currently only 150/1400 validators have more than 300k stake, which means that around 90% are small validators relying on stake pools and SFDP and they will be impacted negatively by this proposal if it goes live.\nIt is already hard to survive, very competive stake pools and more and more expensive the hardware.\n\n[dev_null]: While the intent of this SIMD is good, it would be naïve to overlook the potential side effects.\nSeveral features of this SIMD align with practices I already follow manually, and I support the automation and transparency of that portion. The ability to programmatically direct 5% of block rewards to a collector account would greatly improve our transparency. We donate to animal welfare every month, and this is a manual process that could be improved with this SIMD. Additionally, there’s currently little traceability regarding where the funds ultimately go. This SIMD would allow us to build a front-end to display this in a provable and transparent way.\nThere are other pros mentioned that I won’t dive into for the sake of brevity, but I want to emphasize that I understand and appreciate the motives behind this feature.\nNow for the downside. As others have pointed out, this change gives sandwichers and large validators a clear advantage. It also enables a framework for stake pools to squeeze validators harder. While I agree that relying on stake pools is not a sustainable business model, they nonetheless do exist, and we must acknowledge the impact on small/medium-sized validators. I also anticipate that the barrier to entry will rise with this change. It can be tough to break even already without putting block rewards on the table. I guess break even has changed drastically since I last looked so forget that point.\n\n[ily-validator]: I oppose this proposal.\nCurrently, many small-scale validators are operating their validator servers at the break-even point and are struggling to cover monthly server costs.\nIf SIMD-123 is passed, as was the case with JITO MEV, it will inevitably be added to the SFDP requirements. In that scenario, I believe that any validators other than the major ones will not be able to survive.\nFurthermore, since the same functionality can be achieved with sanctum LST and the Jito tip router, there are no benefits other than from a legal standpoint. Wouldn’t the side effects of SIMD-123 be too great?\n\n[Grigorii]: It is obvious to all validators that this proposal is directed against small and medium-sized validators. The strategies for delegating stakes pools now look like this, so that a stakes gender-dependent validator earns as little as possible with a 0% commission. And they want to take away this earnings from him in the form of priority fees. And it doesn’t seem to matter at all how small and medium-sized validators vote for this proposal, as long as the interest of large validators is obvious. Here are statistics on the number of validators and their total steak in this range.\n100 - 100 000 SOL\t-\t903 validators, total stake - 28 073 960 SOL\n100 001 - 300 000 SOL\t-\t288 validators, total stake - 53 675 219 SOL\n300 001 - 1 000 000 SOL\t-\t67 validators, total stake - 35 556 740 SOL\n1 000 000+ SOL \t\t-\t84 validators, total stake - 266 573 515 SOL\nIf the sum of Yes votes is equal to or greater than 2/3 of the total sum of Yes + No votes, the proposal will pass.\nLet’s assume that small and medium-sized validators reasonably vote against according to the size of their stake. That is, 1191 validators disagree with this proposal. But their total stake is less than 82 million SOL. While 151 validators have a total stake of over 300 million SOL. There is no scenario where small and medium validators could reject this proposal if a group of large validators disagrees with their opinion.\n\n[stakerRash]: The proposal may seem appealing on the surface, but in reality, it poses a serious threat to the sustainability of smaller validators and the health of Solana’s validator ecosystem.\nRight now, validators already face pressure to lower commission rates to attract stake. Many small validators set their commissions to 0% to stay competitive. If this proposal passes, it will push them to give up 100% of their block production rewards as well, making it impossible to sustain operations.\nkey risks:\nRace to 0% commission on all income – Small validators will have no choice but to forgo earnings from block production just to compete, worsening centralization risks.\nUnfair advantage for larger validators – Those with deep pockets can afford to operate at lower margins, further concentrating stake among the biggest players.\nLong-term validator instability – Without sustainable rewards, many small validators will shut down, reducing network resilience.\nWhile the proposal brings interesting composability and functionality, the cost is too high.\nSolana needs a robust, decentralized validator set – not an incentive structure that forces validators to operate at unsustainable margins.\nVote NO on this proposal and protect the future of decentralization on Solana.\n\n[relpmis]: I’m small validator with just SFDP stake. This proposal will kill me.\n\n[zantetsu]: I completely sympathize with the concerns raised in this forum. They are all legitimate concerns. Greasing the wheels on sharing block rewards will serve to increase the competitiveness on block rewards sharing which will make it harder or impossible for small validators to break even and reduce the profitability of all but the private validators and those with institutional stake agreements.\nOn the other hand, if a mechanism for sharing block rewards is not implemented, then block reward sharing can (and will) still happen, but in a bespoke manner that is more dangerous for stakers. One example would be that stake pools (like the JITO stake pool especially) could just require validators to deposit half of their block rewards into their JITO tip account every epoch, which will then be distributed to stakers according to the normal JITO tip distribution process. This would be a way to accomplish block rewards sharing but in an objectively worse manner than a well thought out, well implemented, more secure chain native fashion.\nAt this point, I can see both sides of the debate here and I can’t say I have a clear answer as to how I will vote. I don’t want Solana to offer technically inferior solutions to stakers; but I also don’t want to create a system which puts even more friction in front of validators just getting started. I will be watching arguments in public forums and making my choice based on the nature of arguments I see there. If I come to a definitive conclusion, I will post my final thoughts in this thread.\n\n[laine]: We have known for a long time that this proposal was coming, and there already exist multitude of manual and out-of-protocol mechanisms for sharing block rewards. This includes LSTs as well as manual airdrops, USDC airdrops, NFTs and most recently Jito’s TipRouter.\nIt is inevitable that stakers will demand their fair share of block rewards, and out-of-protocol solutions are universally poorer options, in terms of friction, trust assumptions and transparency.\nWhile I completely understand the concerns validators have with this (concerns that I share), refusing to implement this kind of mechanism and forcing inefficient side-deals is blatantly worse for stakers and for the overall user experience of the staking product.\nAs an ecosystem focused on rapid accelerationism and innovation it is imperative that we continue to innovate and push forward.\nI sincerely hope that the result of this is a moderate and balanced equilibrium of block rewards sharing, where both validators and stakers get to enjoy the fruits of their labour and investments.\nI will be voting in favour of this proposal.\n\n[zantetsu]: I’m curious - if 4% inflation paid out as stake rewards represents capital inefficiency, why doesn’t block rewards paid out to stakers represent the same thing?\n\n[Ben.Hawkins]: @stakerRash @Grigorii @ily-validator @adi_vb @mariaeverstake @Leapfrog\nI understand and share your concerns about the potential impact on small validators. However, these concerns will not be mitigated by rejecting this proposal. Stakers are already demanding block reward sharing, and we’re witnessing rapid growth in off-protocol solutions. Examples include:\nJito compelling validators in the jitoSOL stake pool to share rewards through their tip router.\nMarinade demanding block rewards via their stake auction market.\nRetail stakers increasingly choosing LSTs that offer reward sharing.\nThis trend will inevitably continue, becoming increasingly complex and fragmented. The core protocol needs to address this clear network demand explicitly. If we don’t implement a standardized, transparent solution within the protocol, third parties will define how block rewards are shared—often at the expense of validators, as these entities typically extract their own fees.\nVoting “no” won’t stop reward sharing; it will simply cede control to external parties. Implementing this mechanism within the protocol standardizes the process, enhances transparency, reduces trust requirements, and improves operational security by allowing validators to direct rewards to secure accounts (e.g., multisigs or cold wallets).\nA “no” vote is effectively a vote for third-party intermediaries to control reward distribution, potentially extracting additional fees from validators and stakers alike.\nIt’s essential we proactively shape this inevitable outcome rather than leaving it to external actors.\n\n[pico.sol]: I am going to vote “no” at this time.\nI already distribute block rewards through sanctum LST, and also distribute block rewards to native stakers through my own program as well.\nIf validators create their own program, they do not have to make extra payments to third parties.\nAlso, they can flexibly distribute more block rewards to long-term stakers, and less to short-term stakers, for example.The program is not too difficult to implement.\nFor small validators, the SFDP’s delegation rule is practically enforced; Jito MEV reward was initially allowed at 100%. But, due to the adverse effects of Marinade’s SAM, etc.??, 10% was set as the maximum. The same will no doubt happen with block rewards. And other stake pools will also set block reward % criteria.\nIf SIMD-123 is applied, validators will become mere commodities. We can only differentiate ourselves by simply setting the less number to a percentage.\nValidators should be able to differentiate ourselves by our own ingenuity; SMD-123 takes away the room for ingenuity. It eliminates diversity and drives us into a fruitless race to 0%.\nIt would bring temptation to vicious side deals such as sandwiches. Add to that a reduction in the number of validators, and security concerns increase.\nHowever, I will make the final vote after listening to the intentions of my stakers.\n\n[laine]: Hey Pico,\none thing to consider is that you’re already doing this out of protocol, but it makes it difficult for your stakers to see and understand the APY they’re earning.\nAt the same time, what you describe using a custom program, relies on trust and manual transfers from your identity account (even if you automate them with a script, you have to first receive the lamports into the identity account), while with this proposal you could specify your custom program as the collector of your block rewards and still use a custom program.\n\n[pico.sol]: Michael, I was not aware that we could use a custom program. Thanks.\nYour two points are good ones. From a technical standpoint I think they are great. I also sympathize with the idea of accelerating innovation.\nIf this proposal passes I may use it.\nHowever, I am still against it because of the increased competition and security risks that remain, and when combined with SIMD-228, the negative impact on small validators is too big. I don’t want to see small validators committing mass suicide towards 0% and disappearing. I don’t want to see users exposed to more sandwich attacks.\nI oppose this because I am proud to be a part of this ecosystem and I really like it and want to protect this Solana community.\n\n[fberreth]: The playing field for validators must be even. Today, large validators have way too much income through block rewards because voting costs are the same no matter how much you stake. This simply should not be. If a validator votes on behalf of 1mil SOL it should pay more for voting then a validator voting for 1 staked SOL. This would level the playing field somewhat because right now with the price of SOL voting costs are the majority of costs for running a validator (the HW costs are probably around 20% of that). Small validators really only have block rewards to cover their costs (no one staked on a validator that keeps X % for themselves) and since they are not leader often don’t receive many block rewards. One could simply say - based on leader schedule - your voting cost is magnified X times based on how often you are leader (someone smart figure out the details :)). You get the ideal - level the playing field. This proposal will just commercialize the opposite. While its true some validators give out block rewards, it is the large validator that does that (as they swim in SOL literally) and can price everyone else out. Do not vote for this. I don’t care that they have to do extra work to give out block rewards, this should only ever be implemented when the playing field is even.\n\n",
            "comment_count": 19,
            "original_poster": "Ben.Hawkins",
            "activity": "2025-03-14T00:25:12.002Z"
        },
        {
            "id": 3294,
            "title": "Proposal For Introducing a Programmatic, Market-Based Emission Mechanism Based on Staking Participation Rate",
            "url": "https://forum.solana.com/t/proposal-for-introducing-a-programmatic-market-based-emission-mechanism-based-on-staking-participation-rate/3294",
            "created_at": "2025-02-25T00:03:16.507Z",
            "posts_count": 64,
            "views": 3943,
            "reply_count": 42,
            "last_posted_at": "2025-03-14T00:24:13.815Z",
            "category_id": 11,
            "category_name": "Governance",
            "description": "This proposal is updated in Epoch 750 to change the full rollout of emissions formula from 10 epochs to 50 epochs based on community feedback. Based on rough consensus achieved on Discord, there won’t be any change to the discussion period and voting will continue as per the original schedule.\nAuthors: Tushar Jain, Vishal Kankani, Max Resnick\nBackground\nAs Solana matures, stakers increasingly earn SOL through mechanisms like MEV. This income stream reduces the network’s historical exclusive reliance on token emissions to attract stake and security. According to Blockworks (https://solana.blockworksresearch.com/), in Q4 2024 MEV, as measured by Jito Tips, was approximately $430M (2.1M SOL),representing massive quarter-over-quarter growth. In Q3 Jito Tips were approximately $86M (562k SOL), Q2 was approximately $117M (747k SOL), and Q1 was approximately $42M (300k SOL).\nGiven the level of economic activity the network has achieved and the subsequent revenue earned by stakers from MEV, now is a good time to revisit the network’s emission mechanism and evolve it from a fixed-schedule mechanism to a programmatic, market-driven mechanism.\nThe purpose of token emissions in Proof of Stake (PoS) networks is to attract stakers and validators to secure the network. Therefore, the most efficient amount of token issuance is the lowest rate possible necessary to secure the network.\nSolana’s current emission mechanism is a fixed, time-based formula that was activated on epoch 150, a year after genesis on February 10, 2021. The mechanism is not aware of network activity, nor does it incorporate that to determine the emission rate. Simply put, it’s “dumb emissions.” Given Solana’s thriving economic activity, it makes sense to evolve the network’s monetary policy with “smart emissions.”\nThere are two major implications of Smart Emissions:\n1. Smart Emissions dynamically incentivizes participation when stake drops to secure the network.\n2. Smart Emissions minimize SOL issuance to the Minimum Necessary Amount (MNA) to secure the network.\nThis is good for the Solana network and network stakers for four reasons:\nHigh inflation can lead to more centralized ownership. To illustrate the point, imagine a network with an exceedingly high inflation rate of 10,000%. People who do not stake are diluted and lose ~99% of their network ownership every year to stakers. The higher the inflation rate, the more network ownership is concentrated in stakers’ hands after compounding for years.\nReducing inflation spurs SOL usage in DeFi, which is ultimately good for the applications and stimulates new protocol development. Additionally, a high staking rate can be viewed as unhealthy for new DeFi protocols, since it means the implied hurdle rate is the inflation cost. Lowering the “risk free” inflation rate creates stimulative conditions and allows new protocols to grow.\nIf Smart Emissions function as designed, they will systematically reduce selling pressure as long as staking participation remains adequate. The inevitable side effect and primary downside to high token inflation is increased selling pressure. This is because some stakers in different jurisdictions have taken the interpretation that staking creates ordinary income, and therefore they must sell a portion of their staking rewards to pay taxes. This selling is a significant detriment to the network and does not benefit the network in any way. At today’s approximate 4.5% annualized inflation, at a $120 billion fully diluted valuation, new emissions amount to ~USD 5.5 billion per year.\nIn markets, sometimes perception is as important as reality. While SOL inflation is technically not cost to the network, others think it is, and that belief overall has a negative impact on the network. Inflation causes long-term, continual downward price pressure that negatively distorts the market’s price signal and hinders fair price comparison. To use an analogy from traditional financial markets, PoS inflation is equivalent to a publicly listed company doing a small share split every two days.\nHistorically, issuance curves have remained static due to Bitcoin’s immutability ethos—a “Bitcoin Hangover” so to speak. While immutability suits Bitcoin’s mission to become digital gold, it doesn’t map to Solana’s mission to synchronize the world’s state at light speed.\nIn summary, the current Solana emissions schedule is suboptimal given the current level of activity and fees on the network because it emits more SOL than is necessary to secure the network. An issuance curve set by diktat is not the right long-term approach for Solana. Markets are the best mechanism in the world to determine prices, and therefore, they should be used to determine Solana’s emissions.\nFor the sake of clarity, this proposal for SIMD-0228 is independent of other SIMDs under discussion currently. It can be executed standalone.\nTesting\nBased on our discussions with Anza and with core engineers the community call, there is an understanding that this is a straightforward technical update and poses no technical risks. Upon approval of the proposal, we will collaborate with Anza, the Solana Foundation, and the Jump/Firedancer teams on implementation.\nProposal\nWe propose updating the emissions rate formula to more accurately capture the market dynamics.\nList of variables:\nFraction of total supply staked: (s)\nIssuance Rate (i)\nValidator returns: v(s) = i/s + MEV\nr is the current emissions curve that automatically goes down every epoch at an annualized rate of 15% every year until it reaches 1.5% where it stops changing.\nThe suggested new formula and curve is:\nnew formula1044×436 38.3 KB\nnew issuance rate1044×614 37.3 KB\nWhen s &gt; .5 the curve corresponds to just the first term: r(1 - sqrt(s)). This was the curve in the previous version of the SIMD. Based on community feedback, we have added the cmax(1-sqrt(2s),0) term to make the curve more aggressive when a smaller fraction of the network is staked. c is chosen such that the curve starts becoming more aggressive at s =.5, when half of the supply is staked, and surpasses the current static emission schedule of r when s = 1/3.\nThe derivation of c is provided in the appendix.\nThis yields a vote reward rate for validators with good performance of:\nstaking yield1044×166 11.1 KB\nstaking returns over time1376×812 62.1 KB\nTo ensure that the transition from the old static issuance schedule to this new schedule is smooth, we will interpolate between the old issuance rate and the new issuance rate over 50 epochs using the formula:\nnew phase-in formula1040×160 12.6 KB\nwhere “alpha” is a parameter that controls the speed of the transition, taking the values 1/50, 2/50, …, 49/50, 1, over the first 50 epochs before settling to the new issuance rate at = 1.\nissuance rate during rollout1920×1130 73.4 KB\nAlternatives Considered\nWe considered a few alternatives and decided to settle upon the above curve.\nAlternative Design 1: Pick another fixed curve. We rejected it because replacing one arbitrary curve with another arbitrary curve makes little sense.\nAlternative Design 2: Fix Target Staking Yield inclusive of emissions and MEV payments. We rejected this approach because it incentivizes MEV payments to move out of sight of the tracking mechanism, thereby rendering the design completely ineffective, and impossible to implement.\nAlternative Design 3: A controller function that increases or decreases inflation proportional to the magnitude of the difference between the actual staking participation rate and the target rate (for example, 50%). While this approach would have allowed for a more dynamic response to fluctuations in staking participation, it risks putting emissions back to current highs even if staking participation rate organically went below the target rate (for example, 50%)\nAlternative Design 4: We considered only the first term of the proposed formula above. But that left the specific edge case of existing emissions curve unaddressed. That edge case was if at a future date staking rate continues to drop, and the terminal inflation hits 1.5%, then how would the network be secure. Hence, we added the second term to let the inflation increase beyond what is implied by the current curve in such an edge case scenario.\nAfter considering all these options, we believe the proposed static curve is the most appropriate solution to address our inflation concerns as it is market-based, programmatic, and a function of a market variable.\nChanges in the spirit of this proposal\nShould any changes be necessary to ensure a safe and functioning implementation, such changes will be permitted without further governance requirements so long as the spirit of the proposal is maintained.\nVoting Process\nThe voting process will proceed as follows:\nDiscussion period: Validators are encouraged to participate in discussions to address any concerns.\nStake weight collection period: Stake weights will be captured and published for voting. Validators will have the opportunity to verify these weights.\nVote token distribution will require validators to utilize the adapted Jito Merkle Distributor tool (available at GitHub - laine-sa/solgov-distributor: A merkle-based token distributor for the Solana network that allows distributing a combination of unlocked and linearly unlocked tokens.) to claim the vote tokens corresponding to their stake weights.\nThree token destination accounts will be created for voting choices: Yes, No, and Abstain.\nValidators will have a designated period to vote by sending their tokens to the respective addresses.\nAfter the voting period, if the sum of Yes votes is equal to or greater than 2/3 of the total sum of Yes + No votes, the proposal will pass.\nThe proposal has a quorum threshold of 33%, abstentions count towards the quorum.\nAll announcements regarding this process will be made in the Governance category of the Solana Developer Forums.\nStake weights and a tally script will be available at solgov-distributor/votes at master · laine-sa/solgov-distributor · GitHub\nTimeline\nEpoch 747 - 751: Discussion period\nEpoch 752: Stake weights captured and published, discussion/confirmation of stake weights\nEpochs 753 - 755: Voting tokens available to claim, voting completes at the end of epoch 755\nDiscussion\nActive participation in discussions about this proposal is crucial. Discussions may also take place on the Solana Developer Forums or on Discord Governance channel. It’s encouraged to consolidate discussions to ensure broad participation and minimize redundancy.\nReferences\nSIMD-0228: solana-improvement-documents/proposals/0228-market-based-emission-mechanism.md at patch-1 · tjain-mcc/solana-improvement-documents · GitHub",
            "comments": "[Leapfrog]: This proposal will likely have disastrous effects on the confidence of Solana when needed most.\nIf inflation were to increase when investment confidence is low ie investors were destaking and selling this will magnify the panic. A volatile asset is no place for long term large investors no matter how great the staking reward … if anything the fundamental idea here should be exactly opposite of what has been proposed.\nAs a believer in the future of solana I’d try encourage stability of the asset price and steady market cap growth over staking rewards. There already are additional incentives to staking in solana like the transactional activity (Jito) rewards, so no need to add selling pressure and flight risk by escalating inflation in the moment stability and confidence are required most…\n\n[IceFrosTv2]: The more coins in staking, the more stable and higher the token price and the lower the inflation, and therefore the lower the APY that delegates receive.\nAt the same time, this does not affect competition among validators, giving an advantage to some over others, as the APY will be about the same for all.\nAs for doubts about capitalization and token price at critical moments, I don’t see a problem with that. Those investors who want to sell their coins in a moment of panic will do so regardless of the inflation rate at the time. But it may stop many other people from selling their coins in favor of staking. After all, if a person believes in the future of blockchain and thinks it will be a success, they are more likely to choose to make money from staking than to sell in the hope that they might be able to buy cheaper.\n\n[Leapfrog]: Your assumption is that when inflation goes up people stake more. I would argue that when the inflation goes up as there is a lot of destaking - those who have yet to stake would rather just sell…\n\n[IceFrosTv2]: To each their own. I’m just speaking from my point of view. If I see 5% APY, I’m more likely to sell to avoid locking up funds. If I see 10-20% APY, I’m more likely to put it in staking, given that I can release the funds and sell within two or three days.\nBesides, there’s nothing stopping me from opening a hedge position. For example, even with an APY of 20%, I can throw coins into staking and open a short with the first leverage. This way I don’t care about the price fluctuations in the market, but I also take my percentage from farming\n\n[Leapfrog]: So I think the main point that we are missing is that as the system currently stands - if 300M sol staked went down to 150M sol staked (ie half) Then the inflation reward (say 5%) would suddenly be distributed to half the stake and so double the staking APY… so as it stands the reward already goes up with less stake.\nThe second point is that inflation rewards are often miss interpreted as earnings where as what is actually happening is a devaluing of the currency. So taking your scenario where you see 5% APY lets call it 5% inflation then you see 20%APY ie 10% inflation (as the rest is due to the inflation being distributed to less stake as pointed out above)… all that means is that every sol is decreasing in value by 10% per annum rather than 5% per annum (assuming stable market cap). So your new rewards are actually just there to keep what you had previously. The rest of the sol holders are losing money twice as fast.\n\n[bullmoosesystems]: I’ve been back and forth here, but ultimately I’m against this, mostly because I don’t think there’s a big impact, and the uncertainty created from changing the monetary policy to something that is floating is damaging for institutional investor confidence.\nAdditionally, it feels somewhat arbitrary here. Why is 50% stake the “pivot point”? Why not 66% or 75%? The issuance/inflation rate itself doesn’t impact price at all, it’s the 2nd order effect of stakers selling those inflation rewards. On the other hand, higher inflation rates might encourage buyers to step into the market.\nRegarding the tax question, those users (in many jurisdictions) can simply hold an LST to avoid the tax issue. I don’t think we should be optimizing for something that is a moving target and very different globally.\nThe market already naturally addresses the aim of Smart Emissions today:\nWhen activity is low during bear markets, stake % naturally increases. Go look at nearly every major PoS asset during any bear market and you’ll see that. When the bull market arrives, stake % naturally declines because there are more opportunities to do things with your SOL, so the “risk free” inflation rate from staking isn’t as attractive any more.\nWe have no clue what the “Minimum Necessary Amount” of stake is to secure the network. This proposal targets 50%, but that feels very arbitrary.\nUltimately, I think there’s significant value in having stable monetary policy. Once we change monetary policy once, it becomes more likely that we will change it again. Institutional investors do not like the uncertainty. Worse yet, if we change monetary policy to a floating target (albeit algo determined), then it becomes even more difficult for institutional investors to model returns and thus build valuation models. That uncertainty leads to increased discount rates and lower valuations in these valuation models.\nAlthough the initial, fixed monetary policy may not be “perfect”, there isn’t really a perfect monetary policy out there and there’s a real cost to the change. If we change from one thing that isn’t “perfect” to another thing that isn’t “perfect”, but incur the change cost, is it really worth it?\n\n[kankanivishal]: bullmoosesystems:\nnge monetary policy to a floating target (albeit algo determined), then it becomes even more difficult for institutional investors to model returns and thus build valuation models. That uncertainty leads to increased discount rates and lower valuations in these valuation models.\nAlthough the initial, fixed monetary policy may not be “perfect”, there isn’t really a perfect monetary policy out there and there’s a real cost to the change. If we change from one thing that isn’t “perfect” to another thing that isn’t “perfect”, but incur the change c\nHi @bullmoosesystems Thanks for your response. We have updated the emissions formula to a static curve. It does not target 50% staking rate anymore. The updated formula was arrived after consulting many validators and community members in the space. It is updated in SIMD and also is above. Please let us know your thoughts on the new formula.\n\n[0x_zak]: As a Staker, I am completely for this,\nBut I also understand that my faith is in the validators approving or rejecting this very important proposal. Given that the validators voted for SIMD 96, I hope this comes to pass as well, or I def. would start to get concerned about the optics and incentive alignment between stakers and validators, and yes I know I can just delegate to another validator who is in favour.\nbut just a general sentiment.\nIt is very important to not overpay for security, I really hope every validator thinks about that and the overall health of our blockchain.\nThanks\n\n[kankanivishal]: Main objective of this proposal is to reduce unnecessary emissions, not increase. So long as the staking rate is above 33%, SIMD-0228 proposed emissions are lower than today’s curve. At current staking participation rate of ~65%, they are ~80% lower. This increases investor confidence.\nOnly when staking rate starts dropping below 33% for whatever reasons, we have introduced the failsafe of incentivizing stakers. This feedback was directly sourced from community of validators looking out for network risks.\n\n[Leapfrog]: When staking drops below 33% the incentive is already there for the rewards naturally increase as I’ve pointed out above. No need to increase inflation at this point… or any point for that matter.\n\n[kankanivishal]: it is not for just today. say 5-6 years out, if Solana inflation is at 1.5% and stake is at 33%, and activity is mild - in that edge case, it is a boost to stakers/validators to stay onboard.\n\n[Xen]: Those unnecessary emissions are not that unnecessary for validators, as they must pay vote fees in SOL regardless of the network’s total stake. New or smaller validators that need to take a commission to supplement their vote fees are greatly affected.\nI think this will be disastrous for smaller validators and will raise the barrier to entry to a point where it excludes potential new entrants based on stricter criteria, reducing innovation and decentralization. The barrier to entry is already high, and the validator count is already low (in my opinion, of course). We could argue all day about decentralization, but I don’t think the current curve is bad or arbitrary it’s the curve Solana is succeeding on and the one the market has accepted.\nYou do not lose to inflation if you stake. And now, there are LSTs that remove the downsides of native staking, which, let’s be honest, are very few. Epochs are getting shorter and shorter as the network becomes faster. I am no math or market expert but I think this has negative effects on the validator ecosystem.\n\n[businessmngr]: SIMD-0228’s dynamic emission model could unintentionally push small validators out of the network by raising the break-even threshold. With only Foundation delegation and stake pools delegation, and limited liquidity from the general public, many small validators may struggle to remain profitable as rewards decrease when staking participation exceeds the target. Unlike larger validators, small ones also lack significant independent delegations, making it even harder to attract stake and sustain operations. This will likely lead to further stake centralization, as larger validators can absorb reward reductions more easily, while smaller ones will be forced to shut down.\nThe primary goal of SIMD-0228 is to decrease inflation and adjust the amount of SOL staked to market equilibrium. With SIMD-0123 (sharing block rewards at protocol level), validators will race to zero by competing for the best APY, potentially directing almost all block rewards to stakers. This will concentrate liquidity around those offering the highest APY, making it difficult for smaller validators to cover costs and remain competitive, similar to the current challenge of LSTs competing with JupSOL in terms of APY. We likely won’t see a significant reduction in staking, as the race to zero rewards will keep APY high.\nAlso, I think LSTs will lose their edge over pure staking somehow. Right now, only LSTs can distribute premium yield by sharing back block rewards. If this is equalised, the APY will be the same as native staking, and most people will choose native staking since it is more secure and avoids the additional layer of LST risk.\n\n[123test]: Will this result in fewer Validators? Probably yes.\n\n[solanacompass]: One thing I feel problematic about the inflation proposal is this:\nStakers delegate to us to act in their interests. This means ‘securing the network’ (presumably this includes voting for things that may make it better) but also ‘earning us yield’.\nIt’s not really clear how much they are influenced by one concern over the other, but you could argue the high staking rate (~70% of SOL) and low defi usage (~11% of stake is liquid) implies they like the status quo and are not that interested in defi\nDepending on where a validator stands on this SIMD, they may or may not feel it is a vote to improve the network for all users. But a vote in favor is still actively voting to decrease their own staker’s yield. If this is a staker’s main motivation in staking, a validator is voting against their stakers’ own interests.\n\n[c2yptic]: yes. mid-small level validators ( in terms of stake ) will have extreme difficulties operating if this change comes in to place and will likely close shop\n\n[diman]: a copypasta from SIMD-0228: Market-Based Emission Mechanism by tjain-mcc · Pull Request #228 · solana-foundation/solana-improvement-documents · GitHub\nMotivation is very weak. There are questions about many statements.\nI see other issues that concern me more, such as the percentage inflation rate.\nYou start the motivation by mentioning MEV payouts, which are not part of the protocol at all. Validators themselves are customers/clients in this case, meaning they depend on a commercial project rather than the protocol, or they can act independently (i.e., validators extract MEV themselves, and it is distributed to stakers without intermediaries), or there is real competition among MEV proxy providers in the market. For me, the way MEV currently works on Solana is already a huge problem. So making protocol changes based on the MEV issue seems like the wrong move to me. The MEV issue needs to be solved first.\nAnd so on. Brian mentioned several points above and did not receive answers to them. I won’t repeat.\nAlso, for example, I see a completely opposite conclusion from the four points in the proposal.\nTo me, it is obvious that reducing the APY of native staking will lead to an outflow of small stakers from native staking. This means that the share of large stakers in active staking will increase. And active native staking controls the PoS network. So the network control by large stakers will grow.\nTechnically, finding the balance point involves delegating and undelegating. But the problem is that fees in liquid pools and skipping one epoch in native staking make this process less active. Due to restaking (not to be confused with the currently popular marketing term), up to a month of rewards can be lost. All of this combined will cause small stakes to shift under the control of liquid pools with additional yield.\nSo, I see why this is bad for the Solana network.\nMy conclusion: perhaps this proposal is ahead of its time.\n\n[bullmoosesystems]: Ah I see now. Thank you.\nRegardless, that was only a minor point. I’m mostly not in favor because I don’t like the idea of drastically changing the monetary policy for an unclear set of tradeoffs/advantages/disadvantages and changing it to something that itself is constantly changing. I don’t feel like we have a major issue or imminent threat to address and there are likely to be unforeseen negative consequences to changing the policy.\n\n[MaxSherwoodH2O]: Firstly I want to thank the authors for this well-constructed proposal.\nHowever, I disagree with this proposal.\nWhile it’s true that many validators are arguably earning “too much” at the moment, it’s also true that many of us operated at break-even or at a loss during the years of 2022 and 2023. Block rewards and MEV were non-existent, and we depended on commission on voting rewards to survive. When the bull market started, many of us moved to a 0% voting commission fee and a 0% MEV commission fee, earning just the block rewards. But block rewards vary extremely from epoch to epoch depending on volatility in the market day-to-day. MEV is the same. Once the bull market is over, I expect block rewards and MEV to significantly decrease, requiring many validators to raise commissions and once again depend on voting /inflation in order to stay profitable.\nSolana is one of the few networks with a large, permissionless set of validators. However, the distribution of stake is very long-tail.\nimage1448×623 78.8 KB\nIf you imagine that the break-even point (how much stake does a validator need to break even) depends on the current MEV, block rewards, and inflation / voting rewards, it’s not hard to image a scenerio where this proposal passes, reducing inflation by 80% as stated previously, then a bear market reduces MEV and block rewards by 80% or more, and suddenly we go from 1500+ profitable validators to 150. For me, one of Solana’s strengths is it’s decentralization, (despite what the nay-sayers would have you believe) and it would be hugely bearish for the validator set to reduce by 80% or more in the name of saving 3 or 4% per year in inflation.\nAdditionally, I also agree with the previous points that a “dumb” curve is actually much easier to understand and model, whereas a “smart” curve is more complicated and unpredictable.\nI do understand the reasoning behind this proposal, but I think it would be exceptionally poor timing to pass this during the height of the bull market. I am fully supportive of revisiting this in a year’s time when we can see evidence of persistent MEV and block rewards over more than a few quarters.\n\n",
            "comment_count": 19,
            "original_poster": "kankanivishal",
            "activity": "2025-03-14T00:24:13.815Z"
        },
        {
            "id": 1179,
            "title": "Proposal for Enabling the Timely Vote Credits Mechanism on Solana Mainnet",
            "url": "https://forum.solana.com/t/proposal-for-enabling-the-timely-vote-credits-mechanism-on-solana-mainnet/1179",
            "created_at": "2024-03-13T20:28:00.882Z",
            "posts_count": 25,
            "views": 4440,
            "reply_count": 3,
            "last_posted_at": "2025-01-16T17:42:24.989Z",
            "category_id": 11,
            "category_name": "Governance",
            "description": "Proposal for Enabling the Timely Vote Credits Mechanism on Solana Mainnet\nBackground\nValidators have learned that they can maximize vote credits earned by reducing the likelihood of voting on a dead fork by delaying votes long enough to see which fork is most likely to win (or has already won). This has led to varying strategies employed by a host of validators that induce different degrees of delay (“vote lag”) in voting. This delay in voting results in slower confirmations and finalizations of blocks which directly impacts the speed at which Solana processes transactions. A solution to this problem was devised with the Timely Vote Credits (TVC) feature, which proposes that votes be awarded a variable number of credits, with faster votes receiving more credit than slower votes. This provides an economic incentive to vote quickly, which counterbalances the advantage to waiting to survey forks before voting.\nThe Timely Vote Credits feature was proposed in the Solana Improvement Document number 33 (SIMD-033). It is fully implemented by two features:\n7axKe5BTYBDD87ftzWbk5DfzWMGyRvqmWTduuo22Yaqy (“replace Lockout with LandedVote (including vote latency) in vote state #31264”) which upgrades vote accounts to a new structure which has room to record the latency of votes in preparation for using those latencies to determine vote credits. This feature has been enabled on testnet since epoch 586 (22 epochs at the time of this writing), and enabled on mainnet since epoch 585.\ntvcF6b1TRz353zKuhBjinZkKzjmihXmBAHJdjNYw1sQ (“use timeliness of votes in determining credits to award”) which enables the vote program to begin recording the latencies of votes and then use this latency when awarding vote credits, such that votes with latency 1 or 2 earn 16 credits, with each subsequent slot of latency earning 1 fewer credit, down to a minimum of 1 credit awarded for any vote.\nThe second feature is only available in the 1.18 Solana Labs/Agave branch and so can only be activated on mainnet-beta after this release is present on a sufficient quorum of stake, as per the normal feature enabling rules.\nTesting\nThe following testing has been performed to ensure the safety and correctness of the implementation:\nUnit tests were added to the Solana Labs/Anza node implementation that demonstrate that the feature works as expected in a wide variety of cases, including awarding the expected number of vote credits:\ntest_timely_credits\ntest_retroactive_voting_timely_credits\ntest_process_new_vote_state_replaced_root_vote_credits\nA test cluster was created to test the effects of TVC in operating nodes. A write-up of the results of this test has been created\nProposal\nIt is proposed that the Timely Vote Credits (TVC) feature as described by Solana Improvement Document 33 (SIMD-0033) shall be enabled on the Solana mainnet-beta cluster according to the following steps:\nFeature tvcF6b1TRz353zKuhBjinZkKzjmihXmBAHJdjNYw1sQ shall be enabled on the Solana testnet cluster for a period of no less than 8 epochs, with any issues that are discovered fixed in a way that does not violate the purpose of the feature as described in the SIMD.\nAfter this point, TVC will have been fully enabled on testnet for no fewer than 8 epochs.\nFeature tvcF6b1TRz353zKuhBjinZkKzjmihXmBAHJdjNYw1sQ shall be enabled on the Solana mainnet cluster.\nAfter this point, TVC will be fully enabled on mainnet-beta.\nVoting Process\nThe vote will be conducted as follows:\nA discussion period will commence, during which all validators should participate to ensure that all concerns are addressed.\nStake weight collection period will then commence. The stake weights that will be used for voting will be captured and published, and all validators will have an opportunity to verify these weights.\nVoting tokens will be distributed to the validators according to the stake weights gathered in step 2. Each lamport of stake will receive one vote token. Thus a validator who had 125,000 SOL stake, which is 125,000,000,000,000 lamports, will receive 125,000,000,000,000 vote tokens. It is expected that in practice, vote tokens will be referred to after begin divided by 10^9, so a validator will have 513.71625 “vote tokens” if they have 513.71625 SOL stake.\nThree token destination accounts that correspond to three voting choices will be created: a Yes vote account, a No vote account, and an Abstain vote account.\nValidators will have the remainder of the vote token distribution epoch plus 3 additional epochs in which to vote by sending vote tokens to these addresses (there is nothing preventing a validator from sending some of its tokens to more than one address, although this can only serve to reduce the effectiveness of their vote should they choose to do so).\nAfter the voting period is complete, if the sum of the Yes votes is equal to or greater than 2/3 of the total sum of Yes + No votes, then the proposal has passed.\nAll of the announcements about this process, including the announcement of the vote tally account addresses, will occur in both the Governance category of the Solana Developer Forums and the vote-timely-vote-credits channel of the Solana Tech Discord.\nTimeline (each step starts at the beginning of the first indicated epoch and completes at the end of the last indicated epoch):\nEpoch 588 - 594: Discussion period\nEpoch 595: stake weights captured and published, discussion/confirmation of stake weights\nEpoch 596: voting token distribution, vote addresses created, voting begins\nEpochs 596 - 599: voting continues and completes\nEpoch 600: voting is finished and the resulting tally determines the outcome\nDiscussion\nIt is imperative that validators participate in discussion about this proposal. The best place is on the Solana Tech Discord’s vote-timely-vote-credits channel, where active discussion has occurred already. In addition, discussion may occur on the Solana Developer Forums and also ad-hoc in places like Twitter, Reddit, etc. All discussion wherever it happens is encouraged, but it’s even better if the discussion is kept relatively concentrated in one place so that most participants can see most of the discussion without redundancy. For this reason, the preferred discussion forum is the vote-timely-vote-credits channel on the Solana Tech Discord, and any discussion that starts outside of this forum should ideally be directed to continue in the Discord.\nReferences\nSIMD-033: https://github.com/solana-foundation/solana-improvement-documents/blob/main/proposals/0033-timely-vote-credits.md\nvote-timely-vote-credits Solana Tech Discord channel: https://discord.gg/solana (and then select the channel)",
            "comments": "[laine]: This is a great approach to fixing a loophole in the rewards mechanism on Solana and has gone through a lot of scrutiny showing it works as intended and is safe to enable. I support this feature.\n\n[dev_null]: I participated in the dedicated cluster for testing these features. To my knowledge, the feature is safe and works as intended.\nI also agree with the spirit of this proposal: patching the loophole that is being exploited to gain extra credits by failing to participate in consensus (and simply echoing whatever consensus is reached by the rest of the cluster).\n\n[kernelpanic]: Spectrum Staking fully supports TVC feature and the voting process.\n\n[Knox]: Juicy Stake fully supports this feature. Let’s incentivize network performance and not APY\n\n[nasmithan]: Aurora Validator will vote yes for timely vote credits. Fast confirmations, low fees and high throughput are key to ensuring the success of Solana. Thank you for helping get a fix for this loophole Zantetsu.\n\n[eej]: We at Stronghold Validator fully support this feature\n\n[max.kaplan]: Edgevana is supportive of TVC and will be voting yes.\n\n[LJ-StakeCity]: Great initiative! Stake City will support.\n\n[ripatel-jump]: The Firedancer software includes support for Timely Vote Credits.\nThe implementation is not yet public.\n\n[cfl0ws]: Chainflow appreciates the work @zantetsu and others have done to bring this to a point where it can be voted on. We’re also glad to see that testing was done prior to the proposal being posted.\nIt seems to us that fully activating Timely Vote Credits on Mainnet is a net positive for the Solana community and for this reason we support it. The proposed timeline feels sufficient as well, i.e. long enough to generate an appropriate level of discussion, while not causing an unnecessary delay.\nWe also note that the voting mechanism suggested is the same one used for the first signaling vote. And finally we’re glad to see this vote further aligning the in-development governance process with the more established SIMD process.\n\n[Brian]: Jito Labs is fully supportive of this proposal and will be voting yes once live. Thanks to Zan and others for pushing it for so long.\nBrian\n\n[italoacasas]: The SolDev validator will vote yes once live as well. Thanks to everyone involved.\n\n[mariaeverstake]: Thanks for this important proposal. Everstake will support TVC feature.\n\n[1000X]: Great job man, 1000X.sh fully supports this proposal. LFG\n\n[laine]: The token distribution based on stake weights has been captured. Please review the voting procedure and validate the distribution file here: GitHub - laine-sa/tvc-gov\n\n[Edouard_stakin.com]: Great work, thank you for this proposal.\nWe are happy to support it at Stakin.com and have voted Yes.\nWe believe Timely Vote Credits will make Solana more efficient and contribute to building a fairer validator ecosystem.\n\n[Tissle_T-STAKE]: Great work! Thanks for all the effort it took to make this proposal. T-STAKE Systems fully supports this proposal. Unfortunatly I cannot vote yet as I just started my mainnet validator, just wanted to show my support\n\n[ayk777]: Very good​:+1:\n\n[laine]: The voting period has ended in the last slot of epoch 599 (slot 259199999) with the following results:\n YES | 51.7500% | 191499201395655382\n NO | 0.8400% | 3108403106138704\n ABSTAIN | 0.7000% | 2596234841182772\n CAST | 53.2900% | 197203839342976858\n SUPPLY | 100.0000% | 370034545735897184\nThe required threshold for this proposal to pass was 66.66% of all YES + NO votes. The final tally is 98.4% YES, therefore the proposal to approve and enable Timely Vote Credits has passed the governance vote.\nEnabling of the TVC feature gates is subject to final timing by Anza engineers based on network adoption of minimum supported versions and overall network health.\n\n",
            "comment_count": 19,
            "original_poster": "zantetsu",
            "activity": "2025-01-16T17:42:24.989Z"
        },
        {
            "id": 2869,
            "title": "Timely Vote Credits -- now even more expensive to run a Solana Validator",
            "url": "https://forum.solana.com/t/timely-vote-credits-now-even-more-expensive-to-run-a-solana-validator/2869",
            "created_at": "2025-01-07T07:17:21.588Z",
            "posts_count": 4,
            "views": 147,
            "reply_count": 0,
            "last_posted_at": "2025-01-16T07:15:17.955Z",
            "category_id": 11,
            "category_name": "Governance",
            "description": "All the big validators are using private networks like bloXroute… which costs $1250 USD per month, so now it costs $800 + $1250 per month to run a non-lame validator… the small validators cannot afford that, so all this proposal has done is put a MOAT around the big players… previously high performing but smaller validators are now WAY DOWN on the leaderboard, big players with their Private Lanes are all at the top…\nWHY DID YOU DELETE MY LAST POST you dishonest fkrs.",
            "comments": "[zantetsu]: Wrong in every way, but OK. Rant on.\n\n[solanabigguy12]: You are right…there is no way for smaller boys to enter. But it is what it is.\n\n[MCF]: This is basically correct.\n\n",
            "comment_count": 3,
            "original_poster": "123test",
            "activity": "2025-01-16T07:15:17.955Z"
        },
        {
            "id": 1456,
            "title": "Proposal for Enabling the Reward Full Priority Fee to Validator on Solana Mainnet-beta",
            "url": "https://forum.solana.com/t/proposal-for-enabling-the-reward-full-priority-fee-to-validator-on-solana-mainnet-beta/1456",
            "created_at": "2024-05-08T21:23:34.504Z",
            "posts_count": 77,
            "views": 10592,
            "reply_count": 40,
            "last_posted_at": "2024-12-25T09:45:42.638Z",
            "category_id": 11,
            "category_name": "Governance",
            "description": "Background\nThe current model of collecting priority fees, with 50% being burnt and 50% rewarded, does not fully align with validator incentives and inadvertently encourages side deals. For example, consider a scenario where a transaction submitter wishes to prioritize their transaction. Under the existing model, they might opt to directly pay the block producer a 75% priority fee to ensure their transaction is processed promptly, rather than paying 100% priority fee where the block producer only receives 50%. This creates an imbalance where the incentives of validators are not adequately aligned with the network’s overall health and security.\nTo rectify this issue, it’s proposed to adjust the priority fee structure to reward validators with 100% of the fees collected. This ensures that validators are appropriately incentivized to prioritize network security and efficiency, rather than being incentivized to engage in potentially detrimental side deals.\nThis proposal, outlined in Solana Improvement Document number 96 (SIMD-0096), has been fully implemented with the feature:\nFeature Gate 3opE3EzAKnUftUDURkzMgwpNgimBAypW1mNDYH4x4Zg7 : Reward full priority fee to validators #34731: This feature ensures that 100% of the priority fee is awarded to the validator.\nThe implementation of this proposal requires the use of a feature gate. While there will be no change to the payment structure for transaction submitters, the software incorporating this proposal will allocate a larger portion of fees to the validator compared to previous versions. Thus, a feature gate is essential to ensure a smooth transition for all validators to the new functionality at the epoch boundary, thereby maintaining consensus.\nThis feature is currently available in the Master of Agave repo (minimum version 2.0) and can only be activated on mainnet-beta after the release of a sufficient quorum of stake, following normal feature enabling rules. Additionally, it’s recommended to activate this feature after the implementation of Feature BtVN7YjDzNE6Dk7kTT7YTDgMNUZTNgiSJgsdzAeTg2jF to address potential rounding errors.\nTesting\nThe following tests have been conducted to ensure the functionality of the proposed feature:\nUnit tests added to the Agave repo demonstrate the feature’s functionality in various scenarios.\ntest_total_fee_rounding\ntest_filter_program_errors_and_collect_fee_details\ntest_check_execution_status_and_charge_fee\ntest_distribute_transaction_fee_details_normal\ntest_distribute_transaction_fee_details_zero\ntest_distribute_transaction_fee_details_burn_all\ntest_distribute_transaction_fee_details_overflow_failure\nTesting performed on a local test cluster with the feature enabled, observed slots advance and bank capitalization change.\nProposal\nIt is proposed that the Reward Full Priority Fee to Validator feature, as described in Solana Improvement Document 96 (SIMD-0096), be enabled on the Solana Mainnet-beta cluster according to the following steps:\nEnsure Feature BtVN7YjDzNE6Dk7kTT7YTDgMNUZTNgiSJgsdzAeTg2jF is enabled to address potential rounding errors.\nEnable Feature 3opE3EzAKnUftUDURkzMgwpNgimBAypW1mNDYH4x4Zg7 on the Solana Mainnet-beta cluster.\nAfter these steps, validators will receive 100% of the priority fee as part of their reward.\nVoting Process\nThe voting process will proceed as follows:\nDiscussion period: Validators are encouraged to participate in discussions to address any concerns.\nStake weight collection period: Stake weights will be captured and published for voting. Validators will have the opportunity to verify these weights.\nVote token distribution will require validators to utilize the Jito Merkle Distributor tool (available at https://github.com/jito-foundation/distributor) to claim the vote tokens corresponding to their stake weights.\nThree token destination accounts will be created for voting choices: Yes, No, and Abstain.\nValidators will have a designated period to vote by sending their tokens to the respective addresses.\nAfter the voting period, if the sum of Yes votes is equal to or greater than 2/3 of the total sum of Yes + No votes, the proposal will pass.\nAll announcements regarding this process will be made in the Governance category of the Solana Developer Forums.\nTimeline\nEpoch 612 - 615: Discussion period\nEpoch 616: Stake weights captured and published, discussion/confirmation of stake weights\nEpoch 617: Voting token distribution, vote addresses created, voting begins\nEpochs 618 - 620: Voting continues and completes\nEpoch 621: Voting is finished, and the resulting tally determines the outcome\nDiscussion\nActive participation in discussions about this proposal is crucial. Discussions may also take place on the Solana Developer Forums or on Discord Governance channel. It’s encouraged to consolidate discussions to ensure broad participation and minimize redundancy.\nReferences\nSIMD-0096: https://github.com/solana-foundation/solana-improvement-documents/blob/main/proposals/0096-reward-collected-priority-fee-in-entirety.md\nFeature Gate: Reward full priority fee to validators #34731: https://github.com/solana-labs/solana/issues/34731",
            "comments": "[laine]: Thanks for putting this proposal together Tao!\nA technical question which maybe should’ve been asked on the SIMD: There is an expectation (at least by some) that this would lead to lower priority fees as the market finds a new equilibrium. This made me wonder if the RPC methods to fetch recent priority fee samples needs to be (or is) adjusted with this change to reflect the changed burn rate?\nOverall I believe this change makes sense, though I’d prefer to have more evidence that side deals are happening and are a problem. At the same time the burn on the base fee should in theory protect the original purpose of the burn.\nThere have been some comments in the past relating to this proposal with reference to economics and reduced burn causing a rise in net emissions on the network, while this might be the case I’m not convinced that this is materially significant relative to the inflationary rewards.\nEDIT: On reflection the burn argument doesn’t make sense, as burning prio fees is just a bonus, if prio fees are not burned or prio fees don’t exist at all makes no difference to emission.\n\n[Ben.Hawkins]: though I’d prefer to have more evidence that side deals are happening\nJito bundles are sort of like side deals on txn inclusion. Not all bundles are MEV specific and some use cases use Jito bundles with a small tip on non MEV txns for better inclusion.\nI don’t see this as a major issue but it is evidence that people are already doing “side deals” in a streamlined way.\n\n[laine]: Agreed, though arguably Jito bundles provide far more utility to users, validators and stakers than priority fees currently can, given they provide for prioritization, rewards to stakers and fee collection by validators. Priority fees currently have no automated, elegant or enforcable mechanism of distribution to stakers, so stakers ultimately should prefer the use of jito bundles.\nAbove to be seen in context of SIMD-0123 SIMD-0123: Block Fee Distribution by jstarry · Pull Request #123 · solana-foundation/solana-improvement-documents · GitHub\n\n[buffalu]: I support this SIMD.\n\n[EONpool]: I will vote ‘For’. This will help small validators stay afloat, especially if a block reward sharing system is implemented.\n\n[tao-stones]: laine:\nif the RPC methods to fetch recent priority fee samples needs to be (or is) adjusted with this change to reflect the changed burn rate?\nRPC methods deal with what submitters pay (via set_compute_unit_price), not how much block producers were rewarded. So the change of burn rate should not impact RPC methods.\n\n[Madhatt3r]: How does this proposal benefit stakers? Seems like this proposal helps validators which is great but doesn’t the 50 percent burn help stakers?\n\n[0xMert]: I strongly support this SIMD.\n\n[tao-stones]: Madhatt3r:\nHow does this proposal benefit stakers?\nFair question. Could see the side deals incentivized by 50% burn don’t benefit stakers either.\n\n[0xMert]: It’s more transparent, visible, and allows stakers to hold validators accountable since their stake plays a material role in these rewards. With side deals, they could be doing this and stakers would have no idea. Also means validators have more revenue they make which might incentivize them to lower commission without losing income.\nIt also leads nicely into this proposal: SIMD-0123: Block Fee Distribution by jstarry · Pull Request #123 · solana-foundation/solana-improvement-documents · GitHub\n\n[asymmetric-research]: After carefully reviewing SIMD-0096, we would like to express our support for this enhancement. We believe this proposal is a positive step towards improving network efficiency and aligning validator incentives with Solana’s long-term health and security.\n\n[Knox]: Juicy Stake will be voting For in favor of this proposal. This will potentially make it easier to achieve break even or profit for newer or smaller validators, which helps delegators gain confidence in the validator they stake with.\n\n[NFP]: Having just sat in on the tremendous call hosted by Tim and Ben (thank you!), I’d like to express my opinion as someone who is just now trying to break into the validator business.\nI think because priority fees are (also) earned relative to the amount of stake you have, this proposal will put us into a “rich get richer” kind of scenario, and it will make it harder for new validators to compete for stake in the long run (because larger validators can afford to offer kickbacks and other financial incentives with the increased revenue).\nI also don’t see how this precludes validators from making side deals.\nI do agree that this proposal helps aligns validator incentives, but I think maybe I’m hung up on the philosophical debate about which validators are being helped, and whether or not helping them is in the network’s best interest (in terms of decentralization).\nAppreciate the discussion; thank you to everybody who is participating!\n\n[strategichash]: I am for this proposal.\nHowever, there are a few things to consider.\na) if this goes through, we will stop seeing out of protocol payments - Good \nb) Smaller validators will make more - Good \nc) This does nothing for stakers - \nTo solve for c, we need to seriously look into the exact mechanism of fee sharing, maybe even make the two changes go live together.\n\n[jimmydapizza]: A is 100% false.\nThey talked about distribution offchain if they share the reward with stakers. You swapping one thing for exactly the same thing. The irony is ludacris. There is no on chain mechanism to distribute rewards. Therefor its all manipulation and scare tactics acting like its in every users best interest while they pocket this money for themselves and do their own backroom deals. These people can not be trusted. Who is buying into Validators voting to give themselves more money? What makes you believe you are getting any accurate information from them? It’s in their best interest to give themselves more money and thats why they all here talking about it behind their stakers backs and without their inputs.\n\n[jimmydapizza]: Not in favor of giving validators more sol so they can literally do the exact off chain things they say this prevents. One backroom deal vs another is not saving anyone.\n\n[strategichash]: Out of protocol does not mean “off-chain”. Today’s priority fee burn circumvention still happens onchain. They just send it as a different transaction to the block producer, on-chain. This proposal would just make those fee payers pay the same fee as part of the same transaction. So this is a good thing for the network. However, a fee sharing mechanism must be introduced so stakers can capture this fee. It’s also important to note that some validators already share all their fees to the stakers, so your particular concern along with many others in this thread are invalid.\nAll that said, my stance is simple - The proposal should go through but the feature should only be activated along with the staker fee share mechanism.\n\n[dev_null]: I agree with the mitigation of side deals, but I think the economic impact of such a change needs to be carefully thought through.\n\n[mariaeverstake]: Everstake fully supports this proposal. By implementing this, we enhance the incentives for validators, thereby reinforcing the network’s security and stability.\n\n",
            "comment_count": 19,
            "original_poster": "tao-stones",
            "activity": "2024-12-25T09:45:42.638Z"
        },
        {
            "id": 485,
            "title": "Who votes - three proposals",
            "url": "https://forum.solana.com/t/who-votes-three-proposals/485",
            "created_at": "2023-08-31T13:09:15.044Z",
            "posts_count": 24,
            "views": 1860,
            "reply_count": 7,
            "last_posted_at": "2024-07-23T10:19:28.924Z",
            "category_id": 11,
            "category_name": "Governance",
            "description": "Who votes – three options\nOne of the key decision-making points in implementing governance is deciding who governs.\nWith this in mind, discussions on Discord have been distilled into three possible scenarios:\nValidators vote, with votes weighted by stake\nValidators and token-holders (stake accounts)\n** In this scenario a token-holder could override their validator’s vote for their share of the validator’s stake-weight\nValidators, token-holders and other stakeholders\n** Other stakeholders might include RPC operators, developers etc.\nThis post is intended to permit users to provide their viewpoints in replies - please try to summarize all your points for and against certain options in a single post, avoiding multiple replies to address points made by others, so that we have a concise but complete record of everyone’s view points.\nIf possible begin your reply with your preferred option.",
            "comments": "[laine]: My opinion: Validators vote\nReasons: Validators are custodians of the blockchain. They are entrusted with stake to decide on what constitutes the correct view of the chain, they are tasked with staying up to date with technical developments, are generally an active community of a large but manageable size.\nValidators should engage with governance to represent their view points and express assent or dissent to developments. These expressions form part of the larger validator persona that should be taken into consideration by stakers when deciding who to entrust their stake with.\nGovernance works best with engaged and knowledgable participants, and ideally has a high rate of participation. A system whereby token-holders vote would likely lead to lower levels of engagement, necessitating a lower quorum and thus overall lover level of security.\nIn permitting other stakeholders to vote there is no objective measure of how to weight their votes - this presents further problems.\nUltimately the chain does what validators (or at least those representing a supermajority of stake) choose to do, for governance to be effective it must be enforceable, which is only possible when those enforcing the chain state are the ones voting.\n\n[0xNallok]: Thanks Laine for getting this started.\nI believe that validators should vote, and that their vote should be representative of the broader ecosystem, thus enriching and surfacing the validators perspective from a simple primitive of vote (eg. Yes, No).\nBy exposing the vote details through categorical representation I believe it can incentivize and motivate stake in a much more dynamic and supportive way that a validator simply cannot.\nThese layers which exist on top of the chain drive immense value to the base and therefore may be better positioned to direct users to stake with validators which support their perspectives as well.\nGovernance works when it works for those it governs. Creating an echelon of sophisticated infrastructure managers doesn’t necessarily mean that the depth of knowledge will suffice for all issues where the vote is cast. I believe there may be issues which require the yielding or the validators may not want to cast their vote, therefore having a more dynamic system may be called for.\nA system of on-chain governance should be strong when engagement is low as well as adaptable for a future where engagement and activity could require the casting of billions of votes (of which I do not have an answer yet, but exploring actively).\nWhat I would like to avoid is centralization factors (such as LSTs) driving validators to vote for X or lose stake, and by leveraging other representation (not with a vote, but to be represented by a vote) could weigh these factors better.\n\n[MCF]: My opinion: Validators, token-holders and other stakeholders\nReasons: 1) whatever the governance process is used for initially, I’m pretty sure, it will eventually be used for much more; including funding proposals. 2) validators only really know about validating, I don’t think they have the insight to steer the ecosystem 3) who “owns” the network? is it not the token holders? if only validators vote, we have incentive misalignment.\nWhen I think “other stake holders”, I’m mainly concerned with dev teams – attracting more dev teams to our chain, is the current issue, we are fighting other chains for those precious projects – so why not give them a seat at the table? I will admit, I don’t know an easy way to implement that directly.\nIn Cosmos chains the validator votes, but the token holder can override. The part all Cosmos chains have screwed up, is the interface, it’s very not-obvious for token holders to do it. But in theory I like it.\nI also like the idea of transferable votes that Cardano has. In this scheme the voting tokens can be transferred, so you can give your vote to an expert. For example, if the vote was important to the developmentability of the ecosystem, I could give my votes to a team like Jito, who know more about this than me.\nSummary: I believe token holders need to vote because token holders are the financial owners of the network and therefore will care the most about the network. Validators are typically just handling other peoples tokens not their own bags; yes they have an interest but it is limited. If we had transferable voting, token holders could “give” their vote to whoever they thought knew best for that particular vote.\nThese are just my thoughts, I am relaxed about whatever the community decide - however, I do feel that the discussion so far (telegram and discord) has been almost exclusively validators.\n\n[cfl0ws]: For those looking to orient themselves with the discussions that have taken place to-date, you can find the details of the three different proposals here.\nProposal A - Validators Only\nProposal B - Validators and Stake Accounts\nProposal C - Validators, Stake Accounts and Other Stakeholders\n\n[cfl0ws]: My current opinion is that, consistent with Chainflow’s values, the governance process should be inclusive as possible. Recognizing the practical limitations of this open-ended statement, my thinking is that we start with Proposal B and look to move toward Proposal C as the governance process continues to evolve.\n\n[metaproph3t]: Really really cool that you guys are working on this. Here’s my two cents, as a Solana application dev:\nI think it makes most sense for validators and token-holders to govern the Solana protocol.\nAs pointed out by @laine, using other stakeholders is subject to sybil problem.\nIMO, we want to avoid as much as possible the ‘shadow governance’ where a small group of core devs end up making the decisions.\nI don’t see any downside to having token-holders in addition to validators. 99% of the time, users wouldn’t care about a vote, and the validators would be de facto in charge, but having the recourse of overriding the validators could prevent validators from exploiting their position for their benefit (and to the public’s detriment), such as passing a proposal that enforces 50% validator staking fees at the proposal level.\n\n[laine]: please join the discord discussion too (Solana Staking Alliance) - I’d like to comment on your post without distracting from the convo here too much, i.e. on Discord\n(in short I believe the best approach to the token-holder voting is to allow them to redelegate to a differently voting validator)\n\n[joebuild]: Validators and token-holders (stake accounts)\n^ is the clear choice IMO\nAgree with statements above that 99% this will default to validators, but there is no reason not to include stake accounts if they would like to cast their vote.\nAlso agree that “other stakeholders” would be problematic because they’re difficult to define and weight.\n\n[Freedom]: I think “governance” is highly overrated when it comes to blockchains, most projects that have any kind of governance end up becoming obsolete, and eventually replaced by newer projects with better technology.\nIf anything serves as an example, our democracies are a big mess, where citizens are stolen from in the form of inflation in order to finance wars in other parts of the world.\nAnd I honestly think Solana is so early in its development of what it could be that I would rather keep things running as they’ve been running.\nHere are a couple of my main arguments against creating bureaucracy on blockchains.\nEmpirical evidence:\nMost of the well-run organizations don’t have any form of governance mechanism.\nTesla, Amazon, Facebook, Microsoft.\nThese organizations are empirically the ones that have created the most wealth in this planet, what do they all have in common?\nNo bureaucratic governance, nor voting democracy, they are simply put run by one guy with a good vision for the most part.\nOn the other without doubt the worst run institutions, measured by the amount of resources that they use in unproductive activities, by running deficits, are precisely those institutions in which every decision has to be made through a governance mechanism, Governments.\nNow to be clear, there are well run governments and badly run companies with single founders, but something is very clear, the best run government in the world is not even close as well run as tesla in the creation of wealth for its “citizens” or “stakeholders”. and the worst run companies is not even close to the level of wealth destruction made by badly run governments (Venezuela, Argentina, etc.).\nBlockchains by their nature don’t require a governing mechanism since everyone participating in them is doing so voluntarily, the reason we need governance when it comes to countries is because for the most part when we are born we are automatically part of a group, and are naturally forced to live within that group and in order to do that one has to follow a set of rules, for peaceful living, and the cost of leaving the governing institution is really high, one has to change almost everything in order to leave, for the most part, learn a new language, sell all property, travel, learn new culture, buy new property, etc.\nThe price for leaving is so high that it makes sense that everyone has a saying on what set of rules we should be living our lives in order to have a peaceful living, that is not the case in blockchains, the price I have to pay if I don’t like what the blockchain is doing is virtually inexistent, I can with just a couple of clicks leave the “community” as easily as I arrived.\nFor this reason having a governing mechanism is just going to end up becoming a dead weight in the development of this technology.\nBlockchains are open source.\nNone of the fundamental components of blockchains are private to the public, anyone with enough knowledge of these systems can just as easily fork, or start from scratch with the already developed technology if they don’t agree with the way the tech is being developed.\nSolana already has a governance mechanism, and it’s the best for the task IMHO.\nWhen Solana labs, jito, or anyone releases a newer version of Solana, all the validators have to upgrade their nodes, and therefore the validators and stakers vote with their actions whether they agree or disagree with with such update, Core developers vote with their actions when they decide on which version to develop (Labs, jito, firedancer), validators vote on what version of the software to run, and stakers vote based on the validator that they delegate, therefore everyone has an opportunity to vote with their actions.\nAnyone in the world can decide if they want to change the protocol by forking the open source code and working on those changes, and anyone can decide if they agree or disagree with them by using their stake as vote. so effectively is the best form of “governance” IMO, it’s effectively a free market of “governance”.\nTezos.\nTezos was one of the most promising projects in crypto back in 2017-2019, and one of their main pillars was this idea of upgradeability and governance, because of what had happened with bitcoin, bitcoin cash, eth and eth classic.\nIt was top 15 for a long time, but the net result of all these governance mechanisms was a slowdown of their technology development in favor of bureaucracy, now after 7 years of development it has a market cap even lower than what they had back then in 2016-2017.\nAnd as far as I researched this project back then, they really had good talent, they took top governance mechanisms from swiss institutions.\nHere’s a snippet of 6 minutes of Andrej Karpathy, he was one of the leads engineers at tesla AI division talking about how a well run organization looks like, it’s nowhere close to governance or bureaucracy.\nhttps://youtu.be/cdiD-9MMpb0?t=5665\n“Show me the incentive and I’ll show you the outcome” - Charlie Munger.\nI Think we naively assume that everybody has the project future in mind when voting, but as countless times evidence has shown for the most part people don’t think beyond their personal interest, and this may not be with bad intentions but it’s usually the case, I myself have seen multiple times, validators within the super minority keep promoting stakers to stake in their validators, since it’s economically beneficial to them, they keep doing it, at the cost of the end goal of decentralization, this is the reason I don’t trust people to put the protocol before their self interest, and IMO, the best way to vote is with your actions.\nThe whole point of blockchains at the end of the day is adoption, and users, and users also can vote without the need of a governance mechanism, as a user I am as free to use sol, or eth, or bitcoin, and what I decide to use is effectively my vote, and I can vote whenever I want by moving my resources to the protocol that best aligns with my beliefs.\nIn my most sincere opinion governance although an idea with great intentions it seems to me that it only adds bureaucracy, and nothing more, because anyone involved in blockchains can already vote with their actions, and as far as evidence I’ve been able to wrap my head around, free markets are the best form of governance.\n\n[cfl0ws]: Thanks for your input. I’m glad to see a developer perspective here. While it may be tangential to this discussion, I feel that the more infrastructure operators and developers are in sync, the stronger Solana will become.\nRegarding your point, is it fair to say that you support an option where -\nValidators and token holders vote, in which\nThe validator votes with the tokenholder’s stake weight\nAnd the tokenholder can override their validator’s vote by voting for themselves\n\n[metaproph3t]: I’ve actually been redelegate-pilled by Laine in the Discord discussions. The main problem with the ability to override a validator’s votes is that it encourages governance apathy from validators.\nIf I as a validator know that any of my stakers that disagree with what I put forth, they will simply override me, I don’t have a strong incentive to either research proposals or explain my opinions.\nOn the other hand, if people cannot simply override me and they must move their stake elsewhere, then I am incentivized to research proposals, vote on what I think my stakers will agree with, and explain my votes. Otherwise, those who disagree may decide to park their stake elsewhere.\nThere has also been some back and forth on whether we need redelegation. IMO, it’s not a huge sticking point, but I lean towards redelegation because without redelgation, someone who disagrees with their validator on a vote will probably keep their stake parked since at that point it’s a sunk cost. But yeah, there’s probably some implementation cost here, and I’m not in a position to evaluate whether that cost is worth it.\n\n[simpdigit]: Validators vote. Users who want to vote should make an effort to become validators or move their stake based on what Validators propose whenever something needs to be decided and voted.\n\n[dev_null]: Validators vote\nIf stakeholders aren’t happy with the validator’s vote, they should redelegate their stake to a validator that supports their position. This seems like a logical extension of the way the network already functions.\n\n[laine]: Hi All,\nI have created an advisory vote to sample validators’ stake-weighted opinions: VOTE! First Governance Advisory Vote by Validators\n\n[denysk]: Currently, there is no option that allows validators to vote with a system where one validator equals one vote. All provided options favor validators with larger stakes, who are primarily funds, investors, and whales, and who may be influenced by other funds and whales. Under the present system, one validator with a 2M stake would have the equivalent voting power of 36 validators on foundation stake.\nTo achieve true democracy, we should implement a system where one validator equates to one vote, irrespective of stake.\nWho responded first to the chain restart? Who were the validators attending to restarts in the middle of the night? Predominantly, it was the smaller validators (in terms of count, not stake) who responded, while the entire chain awaited the others to wake up. It is unjust governance to provide unequal voting rights, especially if votes are weighted by stake.\nMy 5c.\n\n[laine]: As mentioned on Discord this introduces sybil risk, even if setting an arbitrary min stake amount.\nIn your example it also means that the foundation basically controls governance, as foundation validators in absolute numbers account for &gt;80% of all validators, and thus would account for over 80% of all votes in a governance system using this method. All it takes is for the foundation to say “We will no longer stake with validators who vote for this proposal” …\n\n[denysk]: The stake-weighted approach will favour a super minority of validators, many of whom are funds, exchanges, and private validators with 100% fees, among others. How can this be considered fair governance? It is imperative to find a quintessential balance that satisfies all parties involved.\n\n[zantetsu]: To achieve true democracy, we should implement a system where one validator equates to one vote, irrespective of stake.\nThis is a Proof of Stake network, not a Proof of Humans network. Implicitly influence in the network is acquired with stake, so stake weight is the only logical way to apportion voting influence.\nWho responded first to the chain restart? Who were the validators attending to restarts in the middle of the night? Predominantly, it was the smaller validators (in terms of count, not stake) who responded, while the entire chain awaited the others to wake up. It is unjust governance to provide unequal voting rights, especially if votes are weighted by stake.\nThis is an uninformed opinion to be honest. The loudest participants in restarts are generally a fairly “core” set of validators, along a spectrum of stake-weights, some large and some small. It is true that the largest “institutional” validators are generally the least participatory and only of average at best responsiveness when it comes to actually implementing the agreed-to path of action. But that is also true of most small validators, who have little incentive to be active participants and mostly just follow instructions at whatever rate is most convenient for them.\n\n[Brian]: Jito has now voted option 1. Rationale is validators are best positioned to evaluate these decisions. Option 2 is elegant but more complex to implement and enshrines a specific model. Think option 1 works more cleanly as a first step.\nValidators should explore their own model for communicating with stakers and including that feedback in their decision. Option 1 keeps things as simple as possible to start with room to evolve as tooling and governance matures\n\n",
            "comment_count": 19,
            "original_poster": "laine",
            "activity": "2024-07-23T10:19:28.924Z"
        },
        {
            "id": 1209,
            "title": "Open Source Contributors - A promising alternative to idea based funding",
            "url": "https://forum.solana.com/t/open-source-contributors-a-promising-alternative-to-idea-based-funding/1209",
            "created_at": "2024-03-18T11:32:08.967Z",
            "posts_count": 3,
            "views": 480,
            "reply_count": 1,
            "last_posted_at": "2024-03-25T16:15:52.982Z",
            "category_id": 11,
            "category_name": "Governance",
            "description": "We’ve recently finished a proposal outlining the opportunity to experiment with an open source contributor funding process. This suggested funding process could become a highly effective and scalable approach for Web3 ecosystems to more consistently generate impact.\nThe opportunity to experiment with a contributor funding approach is better highlighted by understanding some of the problems that exist with idea based funding:\nFor contributors, the proposal submission process can require a large amount of effort and time upfront for contributors to participate and then also handle the ongoing burden of proposal writing and budgeting complexities to get involved in an ecosystem. The structure and incentives of this funding process result in a reduced amount of contribution flexibility and income stability that can deter contributors from participating or limit their ability to easily generate high impact.\nFor voters, it is often highly complex to compare and select ideas effectively. Many voters lack the sufficient context, skills and experience required to be well informed and effectively participate in these decisions. The selection process can be highly time consuming and complex for the voters. Voters can rarely express their exact preferences with their voting decisions and also do not have enough accountability or incentives to be expected to spend a meaningful amount of time on voting to make more optimal and well informed decisions.\nFor Web3 ecosystems, ideas are often treated as ephemeral yes or no funding decisions rather than being a collaborative process that looks to discuss and explore different solution approaches. In larger funding processes it can also become easier for innovative ideas to be ignored due to being less well understood or known. The allocation of assets can also be more inefficient in situations where the allocated funds are not actively being used to generate contribution outcomes, this can increase the percentage of deadweight assets that are not being fully utilised at a given point in time.\nTo understand these problems in more detail you can review our analysis on the current funding landscape - Current funding landscape | Contributors\nThe good news is that most of these problems can be either greatly reduced or fully resolved! An open source contributor funding process can help with resolving these problems and also could become a highly reliable and effective process for maintaining and improving Web3 ecosystems over the long term.\nweb3-open-source-contributor-funding2020×1070 238 KB\nOur proposal outlines the suggestion of experimenting with directly funding a small number of open source developers that would help with developing any open source initiatives - this could include improving any existing pieces of software used in the ecosystem or creating entirely new tools and libraries.\nThe advantages and long term opportunities for adopting a contributor funding approach are numerous. We’ve covered the advantages and opportunities of this suggested funding process in more detail in our proposal - Open source contributor funding | Contributors\nExperimentation in Solana\nCurrently a request for proposal based idea funding process is being used in the Solana ecosystem. One way that a open source contributor funding process could be experimented with could be to enable developers to indicate their interest in working in the ecosystem and which areas they are interested in helping with. This could mean just removing the requirement of submitting a company and website information as many individual contributors might not have an existing project or company they are working on.\nThe immediate opportunity with using a contributor focused funding approach would be that it would make it easier for software developers to indicate their interest in working on these request for proposal areas of focus within the Solana ecosystem. The Solana Foundation could then identify if there are any promising candidates that could be suitable for contributing towards any relevant initiative in the ecosystem. Contributor funding proposals could end up bringing in impactful talent that otherwise might not have got involved due to the more time consuming upfront idea proposal process. The idea funding process can make it more difficult for these individuals to just express their interest in working in the ecosystem. A contributor focussed process could provide a more collaborative path for identifying promising talent and matching them with ongoing work that could create impact in the ecosystem.\nExperiment facilitation\nIf any ecosystem was interested in experimenting with this suggested funding process but doesn’t want to handle the process fully themselves I am eager to collaborate with them and help wherever I can with setting up and running any of these experiments that are focussed on contributor funding.\nIn either event, contributor funding experiments across many ecosystems could be highly beneficial for the wider industry. The outcomes from these experiments can be analysed to better compare the strengths and weaknesses of this suggested approach against the widely adopted idea based funding approach that we see being more widely used across Web3 ecosystems today. Analysing and documenting these experiment outcomes is something that we intend to work on at the Web3 Association!\nCommunity discussion\nGeneral thoughts &amp; feedback\nThere’s likely going to be many opportunities and problems that could be better explored and addressed. If you have any immediate thoughts and feedback please share anything below in a comment!\nOrganising a wider discussion\nIf there are already weekly or monthly organised discussion events that happen internally or publicly I’d be delighted to join one to listen to peoples different perspective about this funding process suggestion to better understand any of the different viewpoints around this approach. Alternatively I could also help to facilitate a one off discussion instead. Please share below if you would be interested in having a dedicated discussion in the near future about this suggested experiment.\nDirect communication\nIf anyone would prefer to chat about this funding process suggestion with me directly then please feel free to reach out to me on Discord - lovegrovegeorge or Telegram - georgelovegrove. Otherwise just throw any questions in the comments below!",
            "comments": "[cfl0ws]: Where did you post the proposal and through what process? I’d be curious to see how something like this might fit into the currently under-development governance process.\n\n[lovegrovegeorge]: Hey @cfl0ws, this proposal is applicable to any ecosystem in the Web3 industry so it has been shared in many ecosystems. The proposal isn’t requesting funding and instead is outlining the reasons why a contributor funding model has a lot of opportunity to become a highly effective funding process over the currently most adopted idea funding process.\nPurpose of this proposal was to explore the WHY this funding process is important and why it resolves the many issues that exist with idea based funding. This proposal doesn’t go into too much detail about the HOW as this is dependent on each ecosystem and what tools and processes they have already setup. My key suggestion is to keep it simple with experimenting with this suggestion and start small and build from there. I’ve reached out to all ecosystem teams to see if they want to collaborate on experimenting with this approach and my intentions are to support any ecosystem experiment I can. Any learnings and results I can then share across the industry. Speeding up Web3 adoption through improving treasuries is my focus\n\n",
            "comment_count": 2,
            "original_poster": "lovegrovegeorge",
            "activity": "2024-03-25T16:15:52.982Z"
        },
        {
            "id": 1011,
            "title": "Aligning the \"What\" of Governance with the Existing SIMD Process",
            "url": "https://forum.solana.com/t/aligning-the-what-of-governance-with-the-existing-simd-process/1011",
            "created_at": "2024-01-30T23:18:45.265Z",
            "posts_count": 1,
            "views": 862,
            "reply_count": 0,
            "last_posted_at": "2024-01-30T23:18:45.323Z",
            "category_id": 11,
            "category_name": "Governance",
            "description": "Background and Context\nA group consisting mainly of Solana validators has been working to develop a governance process for the Solana network. The effort kicked off in July 2023 and is organzed by defining -\nWhy a governance process is needed\nWho should vote\nWhat should be voted on\nHow the voting should be done\nRecent efforts culminated in a non-binding “signaling” vote related to “Who” should vote in late 2023. Governance was also a track at the inaugural Block Zero, held just before Breakpoint 2023.\nSince that vote, activity related to this effort has mostly paused. The conversations that have ensued since then, in Discord chats and validator calls, relate to the “What should be voted on question”.\nWhat should be voted on?\nA couple proposals have shaped-up during this earlier effort to define the scope of the governance process. Their details are found here.\nAnother approach has been suggested by various participants as well. That approach is to align the governance process with the Solana Improvement Documents (SIMD) process.\nAligning Governance with the SIMD Process\nThe SIMD process is reasonably well-formed and active among the Solana core developer and contributor community. The idea behind aligning the governance process with the SIMD process is that doing so will effectively define the governance process scope.\nGovernance process participants would vote on the SIMDs as they surface through the SIMD process. The assumption is that the parameters of the SIMD process are well understood (at least among existing participants) and that this understanding provides an implicit filter for the process, hence defining its scope.\nAligning the governance process to this scope implies that governance process participants agree in principle to this scope definition, at least for now. However, one way the governance process could further limit scope is to pick and choose which SIMDs to vote on. The risk of doing this is that a loop is formed that brings the group back to its current position of having to further define scope, i.e. to determine which SIMDs are voted on and which aren’t.\nThe most straightforward way to use the SIMD process to define scope is to hold a vote on each SIMD as it surfaces. Governance paricipants could also work through the SIMD backlog to resurface issues the participants feel could be particularly beneficial to Solana, e.g. Timely Vote Credits, which is now a year old.\nConclusion\nAligning the governance process with the SIMD process would allow a broader set of voices to introduce noticeable feedback into the Solana development process, in a consistent and meaningful way, while minimizing disruption on the current development process.\nThis voting process can likely help synchronize the development process among the various client teams as well, if not right away, in the not too distant future. Doing so would also help the governance development process gain momentum in a meaningful way, by providing an answer to the “What” discussion and establishing a regular voting cadence among the governance process participants.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "cfl0ws",
            "activity": "2024-01-30T23:18:45.323Z"
        },
        {
            "id": 597,
            "title": "VOTE! First Governance Advisory Vote by Validators",
            "url": "https://forum.solana.com/t/vote-first-governance-advisory-vote-by-validators/597",
            "created_at": "2023-10-10T10:50:21.118Z",
            "posts_count": 5,
            "views": 2273,
            "reply_count": 0,
            "last_posted_at": "2023-10-20T12:17:22.896Z",
            "category_id": 11,
            "category_name": "Governance",
            "description": "Dear Community,\nfollowing discussions in community and validator calls, on this forum and on Discord, I am now initiating a first advisory vote by validators on a key question of governance:\n“Who should vote in a Solana Governance DAO”\n(Further discussion here and here as well as on Discord here)\nPlease feel free to share how you voted and why in this thread\nOption 1: Validators vote, votes weighted by stake\nOption 2: Validators vote &amp; Delegators can override, if a delegator votes it overrides that portion of the validator’s stake\nOption 3: Validators, Delegators &amp; Other Stakeholders (identity and weighting to be determined)\nVoting takes place through tokens deposited to each validator’s identity account. The token has the address BX4ZwH36vMf8T8A4GQ44SSKb8puhXkn7ymJyGi1Ky2GJ and the supply is equivalent to the total active stake in epoch 515. Each validator identity account receives the number of tokens representing their amount of active stake.\nThe mint and distribution is done using the SPL Feature Proposal program.\nVOTING INSTRUCTIONS\n(You will need the spl-token cli tool to vote, it is included in the solana-cli install if you use the installer or install with cargo install spl-token-cli)\nTo cast your vote you must transfer your tokens to one of four public keys to vote for one of the three options or to abstain:\nOption 1: opt1AMNK6g6UxG4N3sFUtfwmM5dKXbJamKCkcr91jai\nOption 2: opt2i1BogY7cwfp23hTz92iQcZyqeX1m4fNZcEcMCSL\nOption 3: opt3T4MkCfJLFHcScTaNZpWSMDGZqNUZwLGCTHFEPzK\nAbstain: ABShtgkUD7UMB7SF9Ur9oLU4XxudnCezjSF8sqqae97c\nTo vote first verify your token balance matches your active stake (in epoch 515) (replace validator-keypair.json with your identity keypair):\nspl-token accounts --owner ~/validator-keypair.json BX4ZwH36vMf8T8A4GQ44SSKb8puhXkn7ymJyGi1Ky2GJ -v\nThen to cast your vote (replace OPTION_PUBKEY with the option pubkeys listed above):\nspl-token transfer --owner ~/validator-keypair.json BX4ZwH36vMf8T8A4GQ44SSKb8puhXkn7ymJyGi1Ky2GJ ALL &lt;OPTION_PUBKEY&gt;\nVOTING PERIOD\nWe will look at the balances of these accounts on 20 October 2023 at 12:00 GMT to determine the result of the vote, please vote by then, this is in 10 days from now.\nTALLYING VOTES\nThis is an advisory vote, it does not form a binding decision and as such there is no particular threshold that should be met for any action to be taken. At the end time mentioned above the results will be tallied by myself (and anyone else may do so themselves of course) and published in this thread and on sg.laine.one.\nThe total supply of the voting token equivalent to active stake is 389,207,145.95. Therefore a simple majority would be 194,603,572.975.\nTo check votes cast for a particular option:\nspl-token balance --owner &lt;OPTION_PUBKEY&gt; BX4ZwH36vMf8T8A4GQ44SSKb8puhXkn7ymJyGi1Ky2GJ\nADDITIONAL READING\nSummary of discussion on “Who votes” Who - Solana Governance Think Tank\nVoting token and supply info: Solscan\nCSV File for token distribution by stake: Who - Solana Governance Think Tank\nFeature Proposal Program Docs: Feature Proposal Program | Solana Program Library Docs\n(we are using this program for its ability to automatically mint a token with the exact active stake and create a distribution to all validators based on stake weight, not to activate an actual on chain feature)",
            "comments": "[cfl0ws]: Chainflow’s Vote - Option 3, Validators, Delegators &amp; Other Stakeholders\nInclusivity is one of Chainflow’s four core values. From our perspective, Option 3 is the most inclusive of the three options. This is why we chose it.\nThe choice represents our well-intentioned and best effort to make a decision to move the Solana governance process forward. We recognize it’s based on incomplete information (as most, if not all decisions are) and made within an evolving process.\nWe recognize that Option 3 is also probably the most complicated of the three. We feel that these complications can be figured out and handled over time.\nSolana governance is in a very early developmental stage. It feels important to keep the process as open and inclusive as possible at this stage.\nThat said, we are open to supporting a more restrictive option, such as Option 2, in the future, if that trade-off is required to take an incremental step forward. We would weigh this decision against a number of factors if and when that time arrives.\nOur experience, however, having participated in governance on many chains over the years, has demonstrated that blockchain decision-making is generally led by engineers. As such, the tendency is to move toward efficiency and simplification. So for us to support Option 2, we would need to see a clear commitment to re-opening the voting process to a more inclusive set of stakeholders in the future.\n\n[zantetsu]: I will be voting for the Validators Vote option.\nMy reasons are twofold:\nValidator only voting is the most practical and efficient to implement voting system. A nascent governance system is likely to die outright if saddled with the burden of implementing a complex or impractical voting system. Validator only voting is simple, has been demonstrated already in practice, and has existing infrastructure to support it. In future, after time has passed and a validator only voting system has been used enough times that issues surrounding voting are better understood, other voting systems could be proposed as upgrades to the process. But I think that a staged approach where we take practical, implementable steps first, and then expand to more complex systems if they then appear advantageous later, is the right way.\nValidators will always have an opportunity to engage their stakers and implement their own governance mechanisms within the body of their own stake. So some validators could implement a pre-governance vote method that allows their stakers to provide input, in a whole variety of ways, each validator choosing whatever works best for their stakers. Some may allow stakers complete control over how the validator votes; others may implement a validator choice with staker override method. Still others might have informal methods based on discussion forum feedback with the validator acting as final arbiter on the decision. This plurality of voting methods will allow validators to compete for stake by providing the best voting experience, and validators can individually move faster on finding better voting methods than the overall ecosystem as a whole making monolithic choices can. In addition, this plurality will allow stakers to choose the voting method that they like best by staking with a validator that provides that method. In short, there is no need for a mandatory ecosystem-wide staker-inclusive voting policy when individual validators can allow whatever policy they want to for their own stakers.\n\n[MCF]: MCF - OPT2 - Validators with Delegator override.\nOPT3 - has merit, but would be too complex to implement in reasonable timeframes.\nOPT1 - takes power away from the token holders, so that’s a NO from us.\nhttps://twitter.com/MCFvalidator/status/1711830346751267244\n\n[laine]: THE RESULTS ARE IN\nWith over 170 participants and around 14.3% of stake voting, over 70% has voted for option 1 with option 2 coming in second at 24%.\ngovvote752×452 14.9 KB\nThe absolute number of votes cast (representative of SOL staked):\nValidators by stake-weight 39557413.31\nValidators &amp; Delegators by stake-weight 13574516.82\nValidators, Delegators &amp; Others 2552960.05\nAbstain 28300.55898\n\n",
            "comment_count": 4,
            "original_poster": "laine",
            "activity": "2023-10-20T12:17:22.896Z"
        },
        {
            "id": 486,
            "title": "What does Governance encompass?",
            "url": "https://forum.solana.com/t/what-does-governance-encompass/486",
            "created_at": "2023-08-31T13:29:41.296Z",
            "posts_count": 3,
            "views": 658,
            "reply_count": 0,
            "last_posted_at": "2023-09-05T14:33:14.723Z",
            "category_id": 11,
            "category_name": "Governance",
            "description": "Governance is a broad term, and for it to be effective on Solana we must be clear on the scope of any governance system that we implement.\nLooking at other chains and ecosystems, we see instances where very minute details are determined by governance, for instance.\nA proposal for an initial scope for governance on Solana could be:\nChanges to the Stake Program, Stake Pool Program, SPL Token programs (legacy and Token2022), Vote Program, etc\nChanges to sysvars or other on-chain parameters (e.g. slot duration, account and block CU limits, base fees)\nMajor* changes impacting the consensus protocol\nProposals for major changes impacting the economics (including inflation, credits/rewards calculations, functionality or restrictions on stake accounts or vote accounts (also covered by the first point))\nImplementation of any sort of slashing mechanism\nChanges to governance itself\nAppointing committees to delegate certain responsibilities too (such as determining the acceptance/denial of a parrticular scope of proposals/SIMDs)\nAppointing a committee/multisig to hold feature activation keypairs\n*Major here would indicate a new feature or functional change, but not a bug or minimal performance fix. A performance fix that requires a large amount of code changes, might be part of governance if this is determined through social consensus/discussion.\nWhat would not be covered by governance?\nGrants (Governance has no treasury at this stage, Foundation grants are theirs to decide)\nWhether or not to activate a feature (other than perhaps through delegation to a committee that holds such keys and makes such determination in accordance with social consensus amongst core contributors as is currently the case)\nNaming or terminology of things\nWhether or not to adopt a particular version of a client\nPlease provide your comments on these lists, whether you agree or disagree with particular points (with reasons) as well as any additional items you think should be on either list.",
            "comments": "[MCF]: laine:\nProposals for major changes impacting the economics (including inflation, credits/rewards calculations, functionality or restrictions on stake accounts or vote accounts (also covered by the first point))\nit would be good to be able to move the inflation rate relative to the fiat token price, that way the community can steer toward the required number of validators and keep that number constant.\n\n[cfl0ws]: For those looking to orient themselves to the discussion to-date, these two proposals have been defined to-date. While they are not mutually exclusive, they represent two slightly different ways of organizing similar information.\n\n",
            "comment_count": 2,
            "original_poster": "laine",
            "activity": "2023-09-05T14:33:14.723Z"
        }
    ],
    "sRFC": [
        {
            "id": 16,
            "title": "Solana Request for Comments Info READ FIRST",
            "url": "https://forum.solana.com/t/solana-request-for-comments-info-read-first/16",
            "created_at": "2023-02-24T05:12:00.382Z",
            "posts_count": 2,
            "views": 1019,
            "reply_count": 0,
            "last_posted_at": "2023-02-24T05:12:41.804Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "What is a SRFC?\nSolana Request for Comments is a proposal for an application standard on Solana. The proposal should document the rationale for the standard and provide enough documentation to understand the implementation\nAn application standard can be something like the following:\nWallet standard api requirements\nMetadata retrieval flow\nIdentity standard\nA SRFC does not apply to SIMDs or core protocol proposals, but it can propose one if the standard requires a SIMD.\nHow do I submit a SRFC?\nTo get requests for comments on your new application standard on Solana, submit a topic under this category. You can include links out to further documentation, implementations, or PRs.\nHow will I know if my SRFC gets accepted?\nIn the current state there is no “acceptance” of SRFCs. They’re requests for comments on potential application standards and it is up to the author to help drive consensus on the standard.",
            "comments": "[jacobcreech]: \n\n",
            "comment_count": 1,
            "original_poster": "jacobcreech",
            "activity": "2023-02-24T05:12:41.804Z"
        },
        {
            "id": 3155,
            "title": "sRFC-35 - Address/Domain Association Specification",
            "url": "https://forum.solana.com/t/srfc-35-address-domain-association-specification/3155",
            "created_at": "2025-02-06T17:07:10.088Z",
            "posts_count": 18,
            "views": 362,
            "reply_count": 13,
            "last_posted_at": "2025-03-06T01:14:12.452Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "sRFC-35 - Address/Domain Association Specification\nSummary\nSolana addresses can be publicly linked to domain names using DNS TXT records or a well-known file. This allows for applications and services (like wallets, exchanges, and infrastructure providers) to easily validate if a company is associated (or denounce associations) with a token, program, or other Solana address that may be using their trademarks.\nBackground\nAs more companies and brands interact with permissionless blockchains (like Solana), having simple mechanisms for them to publicly associate (or denounce association) with onchain assets will enable verifiable trust for these groups and the broader Solana ecosystem.\nUtilizing DNS TXT records or a well-known file on a website, domain administrators can associate blockchain addresses for tokens, programs (smart contracts), or other Solana addresses. This allows other companies and products to validate the original company’s connection to said Solana address.\nExample use cases:\nA brand issues a Solana token with their website listed in the token’s metadata. They add a DNS record to store the token’s Mint address on their DNS records. Popular token list providers, like Jupiter and CoinGecko, can fetch, parse, and validate the token in question was created by the brand. Enabling their verification processes to be more efficient while also enabling the token issuer to get easier access to liquidity at launch.\nA company deploys a Solana program (smart contract) onchain. The company adds the DNS record associating the program address to their website. The program’s security.txt information includes the company’s contact information and website for security researchers to responsibly report vulnerabilities.\nNote: This practice of domain verification through DNS records is already a common practice in the broader tech industry. For example, connecting your website to the Google Cloud Console or verifying domain ownership for various AWS products.\nSpecification\nTo associate a Solana address with a domain name, the domain administrator should create a DNS TXT record on their registrar or a solana.txt file on their website’s .well-known directory to store the “association records” using any of the applicable well-known tags listed below.\nAssociation through DNS TXT records\n(most recommended)\nDomain administrators can add one or many DNS TXT records, each storing a single “association record”.\nNote: DNS TXT records have a few limitations that are not expected to be an issue for this specification, but worth noting in the event they are. If either of these are a concern, see solana.txt below for another option:\nTXT records are limited to 255 characters, per the DNS spec. A typical “association record” per this spec should normally be a max of &lt;80 characters.\nSome domain registrars and platforms, like Google, recommend no more than 49 TXT records due to this being the max supported by domains themselves.\nAssociation through the solana.txt file\nDue to company policy or technical limitations, it is possible that some administrators are unable to create additional DNS TXT records for their domain. In such cases, address association can be accomplished by creating a solana.txt file and storing it in the .well-known directory (see RFC-8615).\nFor example:\nDomain: example.com\nLocation of the association file: example.com/.well-known/solana.txt\nThe solana.txt file should only store one “association record” per line to allow efficient parsing by record validators.\nNote: It is highly recommended that administrators allow frontend applications to access their solana.txt by setting correct CORS headers. This will help ensure anyone that would like to can validate the address/domain association.\nAssociation records\nA single entry in the DNS TXT or line in the solana.txt file is called an “association record”.\nEach entry should store the UTF-8 string of a space separated list of tag-value pairs (using the well-known tags supported below).\nEach association record should contain a single “address tag” and a single Solana address as a base58 encoded string, followed by any additional qualifier tag-value pairs (deny, allow, network, etc).\nIf a domain or website wants to deny all association with all Solana addresses (tokens, programs, etc), they can create add a single “deny all” association record of solana-address=denyall. This special record takes precedence over all other association records.\nWell-known tags\nEach association record should contain a single “address tag” that best describes the purpose of the Solana address\nsolana-mint-address - associates a token’s Mint address to the domain.\nsolana-program-address - associates a program address to the domain.\nsolana-address - associates any other Solana address with a domain (such as a verified signer, program deployer, or other authority).\nIn addition to the blockchain specific tags above, the following generic tags (qualifiers) are also supported to add amplifying information about the associated relationship:\ndeny (optional) - Explicitly deny the association of the address with the domain.\nallow (optional) - Explicitly allow the association of the address with the domain.\nnetwork (optional) - Define the Solana or SVM network the address is used on. This value can be either a Solana network moniker (mainnet, devnet, testnet) or the genesis hash of a SVM network, as a base58 string. If not set, defaults to Solana mainnet.\nQualifier notes:\nThe value of a qualifier should be the case-insensitive string or numeric equivalent for a boolean value (i.e. allow=true and allow=1 set “allow” to the boolean true)\nRecords should not contain both allow and deny\nWhen no allow or deny is set, defaults to allow=true signifying the domain is in fact associated with the address.\nSo an association record would resemble the following examples:\n# token on mainnet\nsolana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v\n# token on devnet\nsolana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v allow=1 network=devnet\n# generic address\nsolana-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v deny=1 network=devnet\nsolana-mint-address\nThis association record’s “address tag”-value pair should be used to store a token’s Mint address as a base58 string:\nTo allow association:\nsolana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v\nTo deny association:\nsolana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v deny=1\nExample: To associate the USDC token with a domain, create an association record with a value of:\nsolana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v\nWhich is also the same as:\nsolana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v allow=true\nsolana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v allow=1\nsolana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v network=mainnet allow=1\nTo deny association of a token’s mint:\nsolana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v deny=1\nWhich is also the same as:\nsolana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v deny=true\nsolana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v network=mainnet deny=true\nsolana-program-address\nThis association record’s “address tag”-value pair should be used to store a program’s address as a base58 string:\nTo allow association:\nsolana-program-address=SMPLecH534NA9acpos4G6x7uf3LWbCAwZQE9e8ZekMu\nTo deny association:\nsolana-program-address=SMPLecH534NA9acpos4G6x7uf3LWbCAwZQE9e8ZekMu deny=1\nsolana-address\nThis association record’s “address tag”-value pair should be used to store any address as a base58 string. This is considered a fallback address tag and should only be used when the other, more explicit tags do not fit the address’ use case.\nTo allow association:\nsolana-address=nicktrLHhYzLmoVbuZQzHUTicd2sfP571orwo9jfc8c\nTo deny association:\nsolana-address=nicktrLHhYzLmoVbuZQzHUTicd2sfP571orwo9jfc8c deny=1\nLinking a domain within onchain information\nIf there is a domain referenced in onchain-linked data, including the contents of referenced offchain JSON files, an association check should be performed to verify a connection to the brand/company’s linked domain.\nWhen wallets, token lists, block explorers, and other applications are presenting information about onchain data (i.e. url for a token’s image, primary media file, or url for its additional offchain metadata file), they should also present the address/domain association status when available.\nThese “association validators” can poll the DNS records (or solana.txt file) for the domain in question and validate if the domain and onchain information point to each other. If they do and are in the “allow” state, the ecosystem can consider these associated. Otherwise, not-associated and handled as such.\nListed below are some of the most commonly used standards where domains are referenced in onchain-linked data. As future metadata standards develop and are adopted, they should also consider and adopt similar pattern. And therefore be supported by this specification.\nMetaplex’s Fungible and Non-fungible asset standards - contain support for the image, external_url, and animation_url fields\nSPL Token metadata interface - contain support for the a uri field that links to offchain metadata file, normally as a JSON format. This JSON file commonly follows one of Metaplex’s standards.\nsecurity.txt - contain the project_url field and other domain related fields\nSecurity considerations\nPermissioned access. Modifying DNS records or uploading the solana.txt file require permissioned access to a domain and its resources. If an attacker was able to gain permissioned access, they could modify the association records. Due to this not being an attack vector being specific to this specification, it is consider out of scope. Domain administrators should ensue they have strong security practices to prevent this.\nHomograph attacks. With the use of domain names to validate if an addresses is associated, attackers will likely attempt to perform these letter substitution attacks on domains/users.\nSince the address/domain association checks will be performed by code and automated systems, so it is not a concern there.\nHowever, when the domain names are presented to users they could become fooled by the letter substitutions. As such, applications should be diligent in helping to minimize/prevent fake domains by performing mitigation steps. See examples here.\nDNS cache poisoning. Attackers could attempt to poison the DNS cache for servers and users attempting to get the association records from a domain. If an attacker is successful in this type of attack, association validators could be fooled into validation the attacker-inserted association. Resulting in applications and users falsely thinking the attacker’s address is truly associated with the domain.\nDNS cache poisoning can be prevented by domain administrators enabling and using DNSSEC. It is strongly recommend that administrators enable DNSSEC and that automated validation processes check for DNSSEC enabled domains.",
            "comments": "[joshmo_dev]: This is awesome! Voicing my support as this initiative will not only significantly help companies who are on-chain to be verified, but also prevent companies who are purely off-chain from becoming the target of nuisance crypto scams.\n\n[bjoerndotsol]: Love the idea. If we go with the solana.txt file we could even think of including the actions.json for blinks in there\n\n[wedtm]: I think this would need some additional record akin to DS records that contains a signature from the authority (wallet / update).\nWithout this, a nefarious website could link any program/address to their domain.\nIf client implementations utilize sRFC-35 for sensitive or verification purposes this could lead to loss of integrity in the security sense and make for a bad time.\n\n[blockiosaurus]: You will lose 90% of companies as soon as they have to learn to deploy a Solana program.\nI agree with @wedtm that DNS doesn’t really seem like the best place for this.\nYou really want some sort of on-chain attestation, most of which can be built with existing tools like Civic KYC, Core Autograph plugins, Bonfida domains, Solana ID, etc. I think it makes the most sense to create a spec with existing partners instead of building a new tool.\n\n[blockiosaurus]: Edit to 1, maybe I misread and the program deploy only relates to associated program IDs.\nAddendum, this is already possible with mints using the Verified Creators array on Token Metadata. A DAO or KYC wallet can sign to verify themselves as a creator for the token/NFT mint.\n\n[nickfrosty]: The other DNS record types are interesting \nI’m not sure I agree with your point about nefarious websites linking to real tokens. Since the token would also have to link back to the website.\nBoth have to point at each other for the association to be valid. Same for programs.\nFor a generic address, this might apply though.\n\n[nickfrosty]: How does a TM verified creator help here? It ensures the Singer was capable of singing. But it in no way ensure the signer is also actually in control of any of the domains listed in the metadata fields\n\n[nickfrosty]: Program deploys applied to the metadata already stored in the programs info (like security txt)\nIf a company does deploy a custom program, they lost their domain in the security txt metadata, then update their domain to effectively acknowledge that they own the program.\nIf they don’t want to deploy a program, they don’t have to\n\n[blockiosaurus]: Verified creator is a two way handshake. The token authority must add the creator to the array (e.g. Metaplex DAO wallet gets added to MPLX) and the added creator must also sign (Metaplex DAO wallet signs the metadata). Bidirectional attestation.\n\n[beeman]: I really like this idea, it seems like the only real way for a Web2 company to verify an on-chain identity.\nUsing a Solana-based protocol for verification doesn’t actually solve the problem since there’s no guarantee the company is who they claim to be. It just shifts the issue somewhere else.\nIt’s also the only way a company can actually deny being associated with anything on-chain. Asking them to sign a Solana transaction to prove they aren’t on Solana doesn’t seem to make much sense.\n\n[wedtm]: Attack scenario:\nI can vanity a token address that matches the first and last N characters of a popular token to avoid the common pattern of middle-out truncation.\nlink it to a scam site, and implement sRFC-35 with only the suggested.\nstart a common spam campaign with a 1-2% success rate across 100k impressions\nThis doesn’t even begin to go down the rabbit hole of orgs that do wildcard entries to places like vercel, allowing anyone to create a new vercel site with scamsite.legitcompany.com.\nWe could probably role-play a few more scenarios, but the real benefit of the signature is that it allows cryptographic validation by machines. This would enable something like the green lock / address bar for extended validation TLS certificates.\nTHIS would incrase user experience massively as wallets and clients could use this bi-directional verification as a first line of defense in filters.\nIf the DNS signature is from the token deployer, that’s a cryptographic prove that the same entity controls access to both.\n\n[nickfrosty]: This still only connects addresses to other addresses. It doesn’t solve the same issue that this sRFC addresses. It does nothing in the way of attesting offchain info (like a domain name) is validated or not. Its just two addresses connecting together\n\n[nickfrosty]: Re: your attack scenario\nThe existing ecosystem trust assumptions still exist. The only benefit of sRFC-35 is that if multiple tokens are created and point to the same domain, you can tell which is the one true token. This spec does not technically improve anywhere else (yet). Even with your signature idea, the same attack scenario still exists.\nThey are issuing their scam token and association on their scam site. They could add the signature into their record. Signature does not prevent this, it just validates that the scam token minter updated the DNS entry.\nTherefore, what is the actual benefit of putting a signature in the association record? I can only think of it negating scenarios where people try to associate with a token they did not issue (which is still useful). So I’m totally game to add something into the spec here for it. Do you have a suggestion on how it should be added?\nMy initial thought is adding the signature into the association record so it could be verified by others. The message to be signed should a standard format including the domain itself and the address and maybe a timestamp of some sort?\nFor tokens, token issuer signing makes sense but you would also have to go back in chain history for that (since mint authority could have changed or been removed). Mint authority could be used but what if it is removed? Same for upgrade authority.\nFor programs, upgrade authority could be used to sign but it can also be removed. So what then?\nFor generic addresses, they could sign their own message. So I guess that works\n\n[wedtm]: Signature does not prevent this, it just validates that the scam token minter updated the DNS entry.\nThe signature allows easy denouncement. Any app/dapp can automatically invalidate any token that claims to be from a domain that:\nHas a valid signature in it’s DNS, and;\ndoesn’t have a signature for the token in question.\nI do think that tokens are the most problematic like you’ve pointed out since any link could be renounced later.\nPotentially the spec could specify that only the most recent authority is considered valid unless it’s completely renounced and then the last is considered authoritative?\nThat doesn’t really solve for the opposite where a domain might eventually change ownership as well.\n\n[seeker]: I have published an IETF Internet Draft for mapping that we hope will become an internet standard.\n \n \n IETF Datatracker\n \n \n \nDNS to Web3 Wallet Mapping\n \nThis document proposes a implementation standard for mapping wallets to domain names using the new WALLET RRType, allowing for TXT record fallback while the WALLET RRType propagates through DNS providers. The goal is to provide a secure and scalable...\n \n \n \n \n \n \nWould primarily use the WALLET RRType and DNSSec to secure the addresses.\nWould like to see if we can unify this into an RFC to everyones benefit.\n\n[nickfrosty]: Just read through the IETF draft you proposed. It looks like we have a lot of similar ideas on this idea.\nAfter a first read, your proposal only allows associating base chain coins (e.g. BTC, SOL, etc) to a domain, but does not really allow more fine grain association of tokens created within the chain’s themself. This would be fine for the chains that do not really support additional tokens being created by users of the network (like bitcoin), but for smart contract chains like Solana and Ethereum, your proposal does not really support people/companies associating other tokens to their domain. Example: Circle’s USDC on Solana and/or Ethereum or Paypal’s PYUSD on Solana\nIn addition to wallet addresses that can hold the base chain token’s, addresses can be used for other purposes aside from holding tokens. Solana programs also have a base58 encoded address similar to a regular “wallet address”. Same for Ethereum smart contract addresses.\nHow do you propose unifying our proposals? Including to ensure we can cover other types of address that are not just the base layer coin?\n\n[nickfrosty]: @seeker accidentally didn’t reply, so tagging you here\n\n",
            "comment_count": 17,
            "original_poster": "nickfrosty",
            "activity": "2025-03-06T01:14:12.452Z"
        },
        {
            "id": 2026,
            "title": "sRFC 33 - Sign message in Actions/blinks",
            "url": "https://forum.solana.com/t/srfc-33-sign-message-in-actions-blinks/2026",
            "created_at": "2024-08-22T19:12:35.253Z",
            "posts_count": 13,
            "views": 438,
            "reply_count": 8,
            "last_posted_at": "2025-02-07T14:46:05.725Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "sRFC 33 - Sign message in Actions/blinks\nTLDR\nAdd the ability for Actions and blinks to ask users to sign a plaintext message. This is a common and no-cost way to validate a user actually controls a given wallet address.\nBackground\nCurrently, all actions ultimately require the user to sign and send a transaction to be confirmed on chain. So there is always transaction fee for users and therefore always a realized cost. Even in action chaining, the user must sign a transaction before proceeding to the next action. Each time paying a transaction fee.\nA very common flow within dApps is asking the user to sign a plaintext message to validate they do in fact control a specific wallet address. This is great because it is completely free for users. There is no transaction fee paid.\nAdding the ability for actions/blinks to request users to sign message will unlock new use cases for blinks, including:\nreducing costs for users\nblinks could authenticate a user’s wallet address natively within the blink\nallow blinks to craft customized blink metadata based on the wallet interacting with it\nProposal\nNote: specific implementation details to be worked out in PRs.\nadd the ability for an action to request a user sign a transaction OR plaintext message via updating LinkedAction to support both\nActionPostResponse should handle different “action response types”, one being for transaction signature requests and another being message signature request\nmessaged being signed should have a consistent structure and ensures their can be a security mechanism to ensure messages originated from its own action api\nRequesting the user sign a message\nLinkedAction be updated to support multiple types. Something similar to this:\nexport type LinkedActionType = \"transaction\" | \"sign-message\";\nexport interface LinkedAction {\n type?: LinkedActionType;\n href: string;\n label: string;\n parameters?: Array&lt;TypedActionParameter&gt;;\n}\nWhen the LinkedActionType is a sign message type:\nthe blink client will make a POST request to the href with the user’s account address (just as transaction focused actions do now)\nthe api server will respond with the message for the user to sign (see message sign request)\nafter the user signs the message, the blink client should make a POST request to\nResponse payload for a sign message request\nWhen an action api server is requesting a user sign a plaintext message, vice a transaction, the server should respond with an updated ActionPostResponse described below:\nNote: Sign message requires action chaining. Therefore when an action api sends a sign message request to the client, the api server must also include a PostNextActionLink to inform action-clients where to send the signature of the user signed message.\nexport type ActionPostResponse = TransactionResponse | SignMessageResponse;\nexport interface TransactionResponse {\n type?: \"transaction\"\n transaction: string;\n message?: string;\n links?: {\n next: NextActionLink;\n };\n}\nexport interface SignMessageResponse {\n type: \"sign-message\"\n data: SignMessageData; // see \"Structured sign message data\"\n state?: string;\n message?: string;\n links: {\n next: PostNextActionLink;\n };\n}\nThe state should be a utf-8 string of a MAC created by the action api server using a secret stored on that server. Action clients should pass this value back to the api server in the PostNextActionLink request. This enables api servers to cryptographically verify that the initial sign message request came from their server by generating a HMAC on their server. It also make it so they are not required to maintain server state of which messages their api requested users sign.\nAfter the user signs the provided message, the blink client will make a POST request to the included next action (i.e. perform action chaining) with a payload as follows:\naccount (required) - the user’s wallet address that signed the message\ndata (required) - the structured data that the user was requested to sign. See Structured sign message data\nsignature (required) - the signature created by the account singing the data (as a base58 encoded string)\nstate (optional) - the same state value the action api initial provided, relayed back from the client.\nAn example of the updated NextActionPostRequest looks like this:\nexport interface NextActionPostRequest extends ActionPostRequest {\n /** signature produced from the previous action (either a transaction id or message signature) */\n signature: string;\n /** */\n data: SignMessageData; // see \"Structured sign message data\"\n /** */\n state?: string\n}\nNote: since the data already supports a key-value object, no change to that type should be required.\nAfter receiving the NextActionPostRequest, the action api should perform all required validation checks on the signature, data, and state to satisfy their business constraints.\nThe action api can now return the metadata for the next action, and the user can continue within the blink experience.\nStructured sign message data\nFor better user experience and improved security, the plaintext message a user will be prompted to sign should be structure with a few required fields:\ndomain (required) - domain requesting the user to sign the message\naddress (required) - base58 string of the Solana address requested to sign this message\nstatement (required) - human readable string message to the user. it should not contain new line characters (i.e. \\n)\nnonce (required) - a random alpha-numeric string at least 8 characters. this value is generated by the action api, should only be used once, and is used to prevent replay attacks\nissuedAt (required) - ISO 8601 datetime string. This represents the time at which the sign request was issued by the action api.\nchainId (optional) - Chain id compatible with CAIPs, defaults to Solana mainnet-beta in clients. If not provided, the blink client should not include chain id in the message to be signed by the user.\ntype SignMessageData = {\n domain: string;\n address: string;\n statement: string;\n nonce: string;\n issuedAt: string;\n chainId?: string;\n}\nNote: this structured data is similar to the Sign In With Solana spec, but without the additional rarely used fields. Therefore structure of this data is compatible with SIWS.\nchainId is consistent with sRFC 31: Compatibility of Blinks and Actions.\nissuedAt should be validated by the action api during their signature verification process in order to perform any desired expiration checks (i.e. was this sign request issued in the last 10 minutes? if not, my api will reject it)\nAt a minimum, the required fields in the structured message should be presented to the user at or before they are prompted to sign said message.",
            "comments": "[tsmbl]: @nickfrosty, this is great\nGeneral question - is there a reason for enforcing consistent structure and specifically use SIWS-like structure for every signed message in context of Blinks?\nMy understanding is that SIWS is great for initial authentication, but it is not strictly needed for every single message signing operation in solana. There can be application specific message signing operations, that are not SIWS compliant and this is fine. Imo, same applies to Blinks.\nFrom the SIWS docs:\nSIWS aims to standardize message formats in order to improve authentication UX and security, replacing the traditionally clunky connect + signMessage flow with a one-click signIn method.\nSIWS shifts the responsibility of message construction from dapps to the wallet.\nI like idea of SIWS, but I would propose to allow more freedom and let developers decide on final message structure by supporting arbitrary message signing, instead of enforcing SIWS-like SignMessageData structure. The approach below also has less coupling by relying on composition\nexport interface SignMessageResponse {\n type: \"sign-message\"\n data: string // can still be SIWS-like message if it's needed by use-case\n state?: string;\n message?: string;\n links: {\n next: PostNextActionLink;\n };\n}\nThis way we still can support both options:\na) use SIWS via composition, we even can provide utility functions for this\nb) use arbitrary business specific messages\n nickfrosty:\nThe state should be a utf-8 string of a MAC created by the action api server using a secret stored on that server. Action clients should pass this value back to the api server in the PostNextActionLink request. This enables api servers to cryptographically verify that the initial sign message request came from their server by generating a HMAC on their server.\nLike the idea of having optional message signature, can serve as an extra security level if use-case needs it. Do you have an idea or example of what attack can be prevented by verifying that the initial sign message request came from their server?\n\n[nickfrosty]: I like idea of SIWS, but I would propose to allow more freedom and let developers decide on final message structure by supporting arbitrary message signing, instead of enforcing SIWS-like SignMessageData structure. The approach below also has less coupling by relying on composition\nDevelopers still have the ability to ask for any message to be signed via the statement field in my proposed SignMessageData. This is the plain text message that the user will see when signing. So they get the same composability and business logic specific messages put into the statement field.\nEnforcing a structure like this allows wallets to present all of this info the the user so the user can be sure that the data is coming from a place that they expect. Since blinks can (eventually) be on any domain, having this data being provided to the wallet can help boost user confidence and present a better UX in the wallet signing modal itself.\nThe fact that it is compatible with SIWS is sort of just a nice side effect and convenience.\nDo you have an idea or example of what attack can be prevented by verifying that the initial sign message request came from their server?\nSince anyone and any bot can send a signed message to the api server, including an HMAC allows the api server to validate the initial request came from their server to begin with (and do other logic checks like expiration checks, etc).\n\n[nze]: hey everyone, thanks for moving sign message forward.\nI agree with @tsmbl that SIWS seems to be too specific for a feature called “sign message”.\nI would vote to keep sign message generic as it’s done in the wallets, and create a separate sRFC for SIWS support in actions/blinks if needed. Basically as a rule of thumb I’d propose to keep the same level of abstraction which is used for wallets and for wallet standard and not to overthink for developers, I believe a lot of devs might want to use a good old low level sign message for their needs\n\n[tsmbl]: nickfrosty:\nSince anyone and any bot can send a signed message to the api server, including an HMAC allows the api server to validate the initial request came from their server\nThanks for details! I understand the general idea, but couldn’t imagine a use-case, when this is useful.\nRegarding bots, in my understanding, any bot can first make a request to API to generate the message to be signed, so what difference does having HMAC make?\nDo you have an example use-case in mind, that demonstrates in which context this extra validation is needed? I guess this feature is inspired by some user request or specific idea of Blink, so curious to learn more about the specifics.\n\n[nickfrosty]: the state can help it so the server does not need to store the state of the message-to-be-signed, since it can be verified via it being an HMAC string (if they chose to use it).\nalso to note, since the state is an arbitrary string that should not be modified by the client, they can also pass any other data if they want. HMAC was more of just an example of something to pass which can be used for extra verification/validation if the api provider wants to use it\n\n[rexap]: One example use case for sign message is if I want to integrate non-blockchain actions that are still done by the same user/wallet. Take for example doing email verification for a user that is signing up for a platform via blink. Why should I pay or force the user to pay some even be it small tx fee just to move on to the next action in a multi-action blink?\nOr as another example if I’m playing a game via blink not every action/move the user makes is a tx especially for games that are not fully on-chain.\nIn both cases, SIWS is too specific bc I don’t want to force a user to sign in every time they take an action that doesn’t require a tx.\n\n[aliquotchris]: Hey all, chiming in here to get this initiative unblocked. Had some IRL chats including with Jon. Plan based on those conversations is as follows:\nWe’re going to get started on an implementation here alongside a couple design partners, specifically DRiP.\nWe’ll start with Nick F.'s proposal on a more opinionated implementation as is proposed here in the original post. While this isn’t strictly sign message in the most general sense, &amp; I am concerned about that from an educational &amp; specification perspective, I understand the desire to enforce a stricter message structure for safety &amp; security.\nAs we proceed with the implementation, we’ll report back if we learn that maintaining the original, generic implementation of sign message is what teams want.\nLet’s get some great experiences shipped.\n\n[aliquotchris]: This is good feedback rexap. Let’s connect on Telegram and discuss what you’re building. My username is @ chrisoss if you want to send me a message.\n\n[aliquotchris]: @nickfrosty if the above sounds good, can you update the actions spec types to reflect this? Then we’ll have what we need to get started.\n\n[nickfrosty]: there is another proposal here (SRFC #32) that covers “optional transactions” in actions/blinks.\neffectively, it proposes the ability for actions to be only making a POST request and never asking the user to sign anything. I think this exact proposal is what you would be asking for here: “non-blockchain actions”?\n\n[Qalbehabib]: Could you provide an example repo or code snippet to demonstrate the implementation of SignMessageResponse, structured message validation, and NextActionPostRequest? This would greatly help in understanding and implementing the feature.\n\n[nickfrosty]: This is already live in the @solana/actions package: solana-actions/packages/solana-actions/src/signMessageData.ts at main · solana-developers/solana-actions · GitHub\n\n",
            "comment_count": 12,
            "original_poster": "nickfrosty",
            "activity": "2025-02-07T14:46:05.725Z"
        },
        {
            "id": 2876,
            "title": "sRFC 34 - Standardized Relayer API",
            "url": "https://forum.solana.com/t/srfc-34-standardized-relayer-api/2876",
            "created_at": "2025-01-07T21:30:08.728Z",
            "posts_count": 1,
            "views": 520,
            "reply_count": 0,
            "last_posted_at": "2025-01-07T21:30:08.786Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "sRFC 34 - Standardized Relayer API\nSummary\nThis RFC proposes a standard API that teams can adopt/implement to enable on-chain activity without end users holding SOL (gas abstraction/gasless transactions). Creating a network of relayers would enable end users, or companies, to choose their preferred relayer, or provide sponsored transactions with certain partnerships. Creating an up to date relayer also allows teams to build faster as they would not have to build their own. The ultimate aim is to create a marketplace of permissionless relayers with differentiated features that improve user experience.\nMotivation\nMany companies currently create their own version of a relayer (or Paymaster in EVM speak) to support their own business and use case. Historically, teams tend to fork Octane and re-work it to their needs. This, however, is less than ideal as it has not been updated in three years, was not audited, and is not production ready. Additionally, relayers have proven themselves to be a required primitive in many scenarios which would benefit from composability and a permissionless standard. Solana thrives on well established primitives rather than copy pasting code for each use case.\nUse Cases\nThis relayer is designed to be flexible to support the major flows teams encounter. This means that it can provide customers a way to transfer SPL tokens, or execute an arbitrary transaction without needing to own any SOL. The relayer may choose to sponsor transaction fees for a transfer completely or request a fee to be received in the SPL token that is transferred. Here are the main use cases I am targeting:\nPayments: Transferring tokens without needing SOL.\nSmart wallet activity: As a PDA cannot be a fee payer, it requires a relayer or another party to pay transaction fees. Having a standardized set of relayers which can support these transactions increases their proliferation and usage.\nEmbedded wallets: Embedded wallets are often used as a way to abstract the complexities of wallets away from end users. As an example, in game actions that utilize an embedded wallet can use a relayer to execute and sponsor game activity.\nProposal\nImportant: This spec is purposefully not prescriptive on the implementation details of the relayer and instead is purely focused on utilizing a common API interface. Each relayer provider can differentiate on features and capabilities. As an example, it may be worthwhile for one relayer to add lighthouse guards to transactions but not useful for another. Additionally I expect a number of “add-on” standardized features to be developed and provided by different providers such as Jito bundling.\nAn Rust based implementation named “Kora” is in progress by Solomon here.\nBelow is an overview of the proposal. To see the full implementation proposal, please refer to this doc and provide direct comments where desired.\nRelayer RPC HTTP Methods Overview\nMethod Name\nDescription\nRequired Parameters\nResult\nfeeEstimate\nEstimate the fee for an arbitrary transaction using a specified token.\ntransaction, fee_token\nestimated_fee, conversion_rate\ntransactionTransfer\nCreate a transfer transaction for a specified token, sender, and recipient. The token supplied will be assumed to be the token to also be used for fees. Returns a partially signed transaction.\namount, token, source, destination\ntransaction, fee_in_spl, token, fee_in_lamports, valid_until_blockheight\ntransactionPrepare\nPrepare a transaction by adding relayer-specific instructions. Returns a partially signed transaction.\ntransaction, fee_token\ntransaction, fee_in_spl, fee_token, fee_in_lamports, valid_until_blockheight\ntransactionSign\nSign a prepared transaction without submitting it to the blockchain.\ntransaction\ntransaction, signature\ntransactionSignAndSend\nSign and submit a transaction to the blockchain.\ntransaction\ntransaction, signature\ntransactionSend\nSubmit a fully signed transaction to the blockchain.\ntransaction\nsignature\ngetSupportedTokens\nRetrieve a list of tokens supported by the relayer for fee payments.\n(none)\ntokens (list of token metadata)\nKey Terminology\ntransaction: Base64-encoded serialized Solana transaction. This could be a signed or unsigned transaction.\nsignature: Unique “transaction hash” that can be used to look up transaction status on-chain.\nsource: Source wallet address. The relayer is responsible for deriving and the TA.\ndestination: Destination wallet address. The relayer is responsible for deriving and creating the TA if necessary.\nfee_token: Token mint address for the fee payment.\nfee_in_spl: Fee amount the end user will pay to the relayer to process the transaction in spl tokens in the smallest unit of the spl token (no decimals)\nfee_in_lamports: Fee amount in Lamports the Paymaster estimates it will pay for the transaction.\nvalid_until_block_height: Expiration block height for time-sensitive operations.\ntokens: Array of supported token metadata (e.g., symbol, mint, decimals).\nfeatures: Array of features enabled by the relayer (e.g., bundle support, sponsorship).\nNote: I am in no way married to the name “relayer”. I just don’t really want to use paymaster.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "ilan_g",
            "activity": "2025-01-07T21:30:08.786Z"
        },
        {
            "id": 2539,
            "title": "Directly supporting blinks in wallets (aside from external sites/x)",
            "url": "https://forum.solana.com/t/directly-supporting-blinks-in-wallets-aside-from-external-sites-x/2539",
            "created_at": "2024-11-30T17:20:21.607Z",
            "posts_count": 1,
            "views": 94,
            "reply_count": 0,
            "last_posted_at": "2024-11-30T17:20:21.654Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Adding blinks support for QR scanning wallets\nSummary\nBrowser wallets like Phantom support blinks by reading a URL and rendering data received back. For example, displaying options for the user to pick from, entering an amount, etc… before submitting the transaction.\nWhat would be good, is if this same flow and UX was initiated by wallets scanning a blink url (outside the context of any existing site or social media like X).\nFlow:\nmobile wallet scans a blink URL\nthe URL returns data such as options for the user to select\nthe wallet renders the options (same as it does in X for blink) and allows the user to input some data and/or make selections\nthe user then submits the transaction\nSimple Use Case:\nUser-selected payment method. The simple scenario that we’d like to build goes like this:\nmerchant displays a blink as a QR code\nmobile app scans the the code and submits the initial GET request\nthe action provider returns a series of payment options (USDC, USDT, BONK, etc…)\nthe wallet renders the options for the user to select from\nuser/wallet selects an option and submits the returned transaction\nThis would greatly improve the flow and UX, as right now the merchant has to select the payment currency before generating the QR code to scan.\nOne ideal improvement here would be the ability to submit the wallet pubkey in the initial GET request so that the action provider could respond back with actual available options instead of blindly returning selections that may or may not be available to the user. Pretty obvious though, and I know it’s already been mentioned by a lot of folks.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "silostack",
            "activity": "2024-11-30T17:20:21.654Z"
        },
        {
            "id": 2072,
            "title": "Multiple txs in actions/blinks",
            "url": "https://forum.solana.com/t/multiple-txs-in-actions-blinks/2072",
            "created_at": "2024-09-09T23:00:05.617Z",
            "posts_count": 5,
            "views": 228,
            "reply_count": 2,
            "last_posted_at": "2024-10-21T20:11:01.823Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Multiple txs in actions/blinks\nSummary\nCurrently Action POST response only allows for the returning of a single transaction (tx) as a string.\n/**\n * Response body payload returned from the Action POST Request\n */\nexport interface ActionPostResponse&lt;T extends ActionType = ActionType&gt; {\n /** base64 encoded serialized transaction */\n transaction: string;\n /** describes the nature of the transaction */\n message?: string;\n links?: {\n /**\n * The next action in a successive chain of actions to be obtained after\n * the previous was successful.\n */\n next: NextActionLink;\n };\n}\nThere are many cases where more complex on-chain actions with programs need to be taken due to the limitation of Solana account and tx sizes. In such cases forcing the user to separately sign multiple txs when wallets already support the signing of multiple txs at once (ie. phantom with signAndSendAllTransactions) simply results in worse UX. Oftentimes this means making a decision between relaying/proxying txs for better UX or creating long more transparent action chains with worse UX.\nImplementation:\nExpanding the transaction field to an array of txs would solve this problem and allow for users to still have great UX while retaining the transparency of executing more complex on-chain actions with multiple txs directly. For example something like:\n/**\n * Response body payload returned from the Action POST Request\n */\nexport interface ActionPostResponse&lt;T extends ActionType = ActionType&gt; {\n /** array of base64 encoded serialized transaction(s) */\n transactions: [];\n /** describes the nature of the transaction */\n message?: string;\n links?: {\n /**\n * The next action in a successive chain of actions to be obtained after\n * the previous was successful.\n */\n next: NextActionLink;\n };\n}",
            "comments": "[Damien]: I think for most cases it makes sense to batch multiple instructions into one transaction and return it instead of returning an array of multiple transactions.\nIt’s possible I’m wrong, however as far as I know the only benefit of signing and sending multiple transactions as once is to go around the max size limit and account read/write limit of a single transaction on Solana. On the other hand, multiple transactions mean the user pays more in fees and the transactions are not atomic, meaning that some could fail and others may succeed, which are all cases that would cause harm to the UX instead of improving it.\nFor these reasons I see that specifically for Actions and Blinks transaction batching makes more sense instead of returning multiple transactions to be signed and sent.\n\n[rexap]: Yes the tx size limit is the primary issue. Most complex on-chain actions cannot be done all in one tx bc you cannot pack all of the required ixs into one tx.\nCurrently for most non blink use cases standard practice for failed txs is to simply retry them. I would imagine something similar could be implemented in blinks use cases.\nAm curious what you mean by “Actions and Blinks transaction batching”? Do you mean action chaining?\n\n[mat]: This would be very beneficial for us at Access Protocol as well as we could implement a blink for reward claiming.\nWe are batching as many operations into a single transaction as possible but even with this approach there are multiple transactions to be signed due to the transaction size limit.\nI think that this is a good idea as the most prominent wallets have an implementation of signTransactions method\n\n[rexap]: @nickfrosty @tsmbl @aliquotchris @0xaryan would love your feedback on this\n\n",
            "comment_count": 4,
            "original_poster": "rexap",
            "activity": "2024-10-21T20:11:01.823Z"
        },
        {
            "id": 2267,
            "title": "sRFC 33: Media Types of Blink",
            "url": "https://forum.solana.com/t/srfc-33-media-types-of-blink/2267",
            "created_at": "2024-10-17T13:29:58.603Z",
            "posts_count": 4,
            "views": 111,
            "reply_count": 2,
            "last_posted_at": "2024-10-21T16:01:41.206Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Summary\nCurrently, the media types, support for the icon field are svg , png, webp, which limits the design space for the developer to show content.\nThe goal is to introduce default media types supported by most of the latest browsers, which will widen the type of content that blinks can show starting with video/audio and more types of images.\nMedia Types\n1. Image Types\nJPEG (image/jpeg)\nGIF (image/gif)\nBMP (image/bmp)\n2. Audio Types\nMP3 (audio/mpeg)\nWAV (audio/wav)\nOGG (audio/ogg)\nAAC (audio/aac)\nWebM Audio (audio/webm)\n3. Video Types\nMP4 (video/mp4)\nWebM (video/webm)\nOGG Video (video/ogg)\nMPEG (video/mpeg)\nImplementation\nCurrently the ActionGetResponse looks like below\n/**\n * Response body payload returned from the initial Action GET Request\n *\n * note: `type` is optional for backwards compatibility\n */\nexport interface ActionGetResponse extends Omit&lt;Action, \"type\"&gt; {\n type?: \"action\";\n}\n/**\n * A single Solana Action\n */\nexport interface Action&lt;T extends ActionType = \"action\"&gt; {\n /** type of Action to present to the user */\n type: T;\n /** image url that represents the source of the action request */\n icon: string;\n /** describes the source of the action request */\n title: string;\n /** brief summary of the action to be performed */\n description: string;\n /** button text rendered to the user */\n label: string;\n /** UI state for the button being rendered to the user */\n disabled?: boolean;\n links?: {\n /** list of related Actions a user could perform */\n actions: LinkedAction[];\n };\n /** non-fatal error message to be displayed to the user */\n error?: ActionError;\n}\nIf we modify the Action to include 2 additional fields, media and mediaType\n/** \n* Media Types for media in the action\n*/\nexport type MediaType = \"video/mp4\" | \"video/webm\" | \"video/ogg\" | \"video/mpeg\" | \"image/jpeg\" | \"image/png\" | \"image/gif\" | \"image/svg+xml\" | \"image/webp\" | \"image/bmp\" | \"image/x-icon\" | \"audio/mpeg\" | \"audio/wav\" | \"audio/ogg\" | \"audio/aac\" | \"audio/webm\"\n/**\n * A single Solana Action\n */\nexport interface Action&lt;T extends ActionType = \"action\"&gt; {\n /** type of Action to present to the user */\n type: T;\n /** image url that represents the source of the action request, and\n will be used as a fallback image incase\n media URL is un-reachable/un-renderable */\n icon: string;\n /** media URL that represents the content to be shown upon request**/\n media : string;\n /** represents the type of media URL provided */\n mediaType : MediaType;\n /** describes the source of the action request */\n title: string;\n /** brief summary of the action to be performed */\n description: string;\n /** button text rendered to the user */\n label: string;\n /** UI state for the button being rendered to the user */\n disabled?: boolean;\n links?: {\n /** list of related Actions a user could perform */\n actions: LinkedAction[];\n };\n /** non-fatal error message to be displayed to the user */\n error?: ActionError;\n}\nThen this would allow the blink developer to include more media types, icon field would still be present to maintain backward compatibility + used as a fallback if the media URL is un-reachable / un-renderable.",
            "comments": "[rexap]: Very much agree the current media types can be limiting.\nI do think perhaps not restricting to just one file though could be helpful and more forward compatible as spec evolves. Perhaps formatting the media field more like the optional links field would allow for this while also keeping the spec compact:\nmedia?: {\n /** list of media items to display */\n items: MediaItem[];\n };\nwhere media items could be:\ninterface MediaItem {\n url: string;\n type: MediaType;\n}\nOf course this brings up the question of how to render multiple media files. An initial thought is providing an optional display field could define how to show the media files with the default being simply displaying the first item.\nmedia?: {\n items: MediaItem[];\n display?: \"default\" | \"carousel\" | \"gallery\" | \"list\";\n };\n\n[0xaryan]: What use-case do you think for array of Media, that would make it a gallery like layout, which imo would not be required by majority of use case\n\n[rexap]: This comes primarily from e-commerce use case where want to have multiple images of a product without forcing a user to create complex product images or having to custom generate one image from a set of images. I also think social, gaming, and RWA blinks would find this useful too.\n\n",
            "comment_count": 3,
            "original_poster": "0xaryan",
            "activity": "2024-10-21T16:01:41.206Z"
        },
        {
            "id": 1971,
            "title": "sRFC 32: Optional Transactions in Action Chaining",
            "url": "https://forum.solana.com/t/srfc-32-optional-transactions-in-action-chaining/1971",
            "created_at": "2024-08-12T17:12:23.587Z",
            "posts_count": 19,
            "views": 493,
            "reply_count": 11,
            "last_posted_at": "2024-09-25T16:08:33.145Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "sRFC 32: Optional Transactions in Action Chaining\nSummary\nTransactions in Action Chaining are non-optional right now, therefore for each action in the chain the user need to sign completely differnet transactions which works fine for a limited use-cases but for majority of the user-cases and also to create a better UX, the user should only sign the transaction once whenever possible, that would be mostly the last action in the chain.\nSame way as any form works, taking information from the user and only showing the submit button at the last. This could open up some interesting design spaces, where a game is getting played on the blink and when game ends, the winner can claim the price, or generating NFT’s with help any AI directly from the blink and deploying the collection, these are just some examples to support.\nImplementation:\nCurrently when the client makes the POST request, then ActionPostResponse is sent with following structure.\n/**\n * Response body payload returned from the Action POST Request\n */\nexport interface ActionPostResponse&lt;T extends ActionType = ActionType&gt; {\n /** base64 encoded serialized transaction */\n transaction: string;\n /** describes the nature of the transaction */\n message?: string;\n links?: {\n /**\n * The next action in a successive chain of actions to be obtained after\n * the previous was successful.\n */\n next: NextActionLink;\n };\n}\nIf, we modify the ActionPostResponse to keep the transaction as an optional field\n/**\n * Response body payload returned from the Action POST Request\n */\nexport interface ActionPostResponse&lt;T extends ActionType = ActionType&gt; {\n /** base64 encoded serialized transaction */\n transaction?: string;\n /** if transaction is present, describes the nature of the transaction */\n message?: string;\n links?: {\n /**\n * The next action in a successive chain of actions to be obtained after\n * the previous was successful.\n */\n next: NextActionLink;\n };\n}\nThe client can now skip the pop-up to user to sign the transaction and depending upon the NextActionLink render the blink.\nexport type NextActionLink = PostNextActionLink | InlineNextActionLink;\n/** @see {NextActionPostRequest} */\nexport interface PostNextActionLink {\n /** Indicates the type of the link. */\n type: \"post\";\n /** Relative or same origin URL to which the POST request should be made. */\n href: string;\n}\n/**\n * Represents an inline next action embedded within the current context.\n */\nexport interface InlineNextActionLink {\n /** Indicates the type of the link. */\n type: \"inline\";\n /** The next action to be performed */\n action: NextAction;\n}\nIf the NextActionLink is of type InlineNextActionLink then the client can directly render the metadata and no callback would be required.\nIf the NextActionLink is of type PostNextActionLink then the action server can return metadata depending on the previous inputs by the user address\nIt should be upto the aciton server to server a InlineNextActionLink or PostNextActionLink type of NextAction to make a better UX for the specific use-case.\nThis way, the action server can also track informations across a series of actions happening on the client. The idea of JWT also crosses the mind, but imo having each action linked to an address and on client only the given address can sign the transaction removes a lot of concerns.",
            "comments": "[tsmbl]: I like the idea, allowing user actions without requiring direct wallet interaction can significantly improve the UX for certain use cases, opening up new design and application possibilities.\nAs we explore this further, adding more action types makes sense, especially with potential future needs like signing messages or navigating to external resources. To support this, I suggest refining the implementation to be more strict and explicit, below is a rough idea of how the interfaces could look with respect to my comment\n// Can be extended to \"tx\" | \"post\" | \"sign-message\" | \"external-link\" in the future\nexport type LinkedActionType = \"tx\" | \"post\" \nexport interface LinkedAction {\n type?: LinkedActionType; // Can be used to tailor UX/UI depending on action to be performed\n href: string;\n label: string;\n parameters?: Array&lt;TypedActionParameter&gt;;\n}\nAs an idea we can also correspondingly define multiple response types, it can also simplify the flow a bit\nexport type ActionPostResponse = TxResponse | PostResponse;\nexport interface TxResponse {\n type?: \"tx\"\n transaction: string;\n message?: string;\n links?: {\n next: NextActionLink;\n };\n}\nexport interface PostResponse extends Action {}\nAnyway, I think we can discuss the final interfaces during the PR in actions-spec.\n\n[0xaryan]: Agreed, on having more action types, just saw a sRFC requesting for external-link so the implementation will make more sense in the long term.\n// Can be extended to \"tx\" | \"post\" | \"sign-message\" | \"external-link\" in the future\nexport type LinkedActionType = \"tx\" | \"post\nI like the idea of defining multiple response types, it will enable more explicit types. I’ll make a PR in action-specs\n\n[scriptscrypt]: Instead of optionally passing / framing the transaction, If we could pass the tx base64 back to the server.\ntxbacktoActionServer974×162 69.4 KB\nMeaning, Instead of exposing the tx directly to the client, we can pass the tx to the chained action,\nSo that once the chain of actions are completed, user can only sign the transaction once before the last step,\nThis also ensures all the actions have a transaction, and can be found onchain.\nThe chained actions should check for Previous / Incoming Transactions (from the params / body), append that to the current transaction, then construct a transaction, before it reaches the completed action.\nThe code for merging the tx might look like this :\nimport { Transaction, Message } from '@solana/web3.js';\nfunction mergeTransactions(tx1Base64: string, tx2Base64: string): string {\n // Decode base64 transactions\n const tx1Buffer = Buffer.from(tx1Base64, 'base64');\n const tx2Buffer = Buffer.from(tx2Base64, 'base64');\n // Deserialize transactions\n const tx1 = Transaction.from(tx1Buffer);\n const tx2 = Transaction.from(tx2Buffer);\n // Create a new transaction and combine instructions\n const mergedTx = new Transaction();\n mergedTx.add(...tx1.instructions, ...tx2.instructions);\n // Set the feePayer and recentBlockhash from one of the original transactions\n mergedTx.feePayer = tx1.feePayer;\n mergedTx.recentBlockhash = tx1.recentBlockhash;\n // Encode the merged transaction back to base64\n const mergedBuffer = mergedTx.serialize();\n const mergedBase64 = mergedBuffer.toString('base64');\n return mergedBase64;\n}\n\n[nickfrosty]: I really like this idea and concept @0xaryan: not all actions must result in a transaction. The current “a transaction is required” stemed from the general design for blinks/actions of “actions return transaction”, but there is no reason we need to continue to shoehorn into that design space anymore (I think)\nI also prefer @tsmbl’s recommendation of clarifying the types to make them more extensible. Multiple “response types” will already be needed for the Sign Message functionality, so this is also inline there too.\nI think proposed PostResponse simply extending Action is interesting since I think it would also enable actions to never prompt for a transaction or other signable event. The api server could continue to return an “action” metadata until eventually return type=completed, then the user would never be prompted to sign anything. I don’t see this as something we need to prohibit, but I do think it may require more thought on implications.\n@tsmbl what is the benefit of adding a type into LinkedAction? What “tailor UX/UI” do you foresee?\nShould the ActionPostResponse have an single type? My thought is that on the blink/client side, performing the checks of what to do with the POST response, you would have to now check for all type variants for the post ActionPostResponse’s type and the ActionType\n\n[nickfrosty]: this would then result in the transaction able to be altered on the client side before it is sent back to the server. and that should seriously be avoided!\nwith @0xaryan general idea here, and the existence of action chaining now, you can already build a blink with multiple actions that can build a complex transaction like you are proposing doing with merging the instructions together. except without all the security risk of transaction being alterable on the client side.\nit looks like the missing piece for your described flow is what Arun is proposing: not all actions require the user to sign a transaction, effectively just allow a form submit. so you ultimately built one single transaction for the user to sign, based on all the inputs and action chaining steps you collected\n\n[0xaryan]: I have made a PR which can clarify the whole flow.\nIt took me time to wrap my head around the whole flow, but it finally made complete sense with Alexey’s idea.\nIMO, having type in the LinkedAction can help the blink client look for only specific data fields when the action endpoint returns the data.\nfor example,\nin the case of type post, the blink client would know to not look for transaction in the response\nin the case of type sign-message, the blink client would only look for data: SignMessageData\nIn a general sense, to create more explicit types on both the blink client and action sides.\n nickfrosty:\nI don’t see this as something we need to prohibit, but I do think it may require more thought on implications.\nI think, enabling this would simply make the blinks, a metadata standard to the URLs which anyone can use to take any form of data from the user. The ability to sign transaction/messages is just the cherry on top.\n\n[nickfrosty]: I saw the PR and I am skimming it now \nhaving type in the PostResponse makes sense to me, just not in LinkedAction for this. it seems irrelevant to be in LinkedAction.\nfor example, the initial GET request to display the first action to a user:\nthe LinkedActions are displayed as form elements the user can interact with\nno matter if there is a type representing “transaction” or “post” requests, the client will still make a POST request to the href endpoint. this POST request will be made either if type=post or type=transaction\nthe RESPONSE of this POST request should have a type noting what data is being returned for the user to interact with:\nif type == “transaction” =&gt; asking the user to sign the transaction (the current flow of all actions)\nif type == “sign-message” =&gt; ask the user to sign a message\nif type == “post” =&gt; nothing for the user to sign, we just wanted to save the user input fields in our database\nnow we process the links.next action\nthis same flow would be the same concept for chained actions too, not just the initial GET request\nI don’t see any need to add a type into LinkedAction, just the PostResponse to determine how the action-client should handle the response\n\n[0xaryan]: ah, got it. makes sense. i’ll update the PR for the same.\n\n[tsmbl]: nickfrosty:\n@tsmbl what is the benefit of adding a type into LinkedAction? What “tailor UX/UI” do you foresee?\nI don’t see any need to add a type into LinkedAction, just the PostResponse to determine how the action-client should handle the response\nThe only potential benefit I see is customization of the button press behaviour &amp; button UI. One example can be External Linking, for this case we can change button to visually display external link icon somewhere on button and change client behaviour to open a new tab, instead of making a POST request.\nI’ve noticed that you also proposing to have type in LinkedAction in Sign Message proposal. Is it a bit outdated based on what was discussed in scope of this sRFC, or you have different considerations for having it in LinkedAction in your proposal?\n@nickfrosty, the flow you’ve described above looks right to me if we don’t consider External Linking.\n\n[nickfrosty]: The only potential benefit I see is customization of the button press behaviour &amp; button UI. One example can be External Linking , for this case we can change button to visually display external link icon somewhere on button and change client behaviour to open a new tab, instead of making a POST request.\nI only think it is needed in the POST response. I don’t think a type on LinkedActions really matters, at least right now. Right now, no matter what, the linked action will always make a POST request to the href. Adding a type field in the POST response is what would help determine how the response is handled (i.e. is this a transaction, sign message request, external link, etc). The client would handle this then.\nI’ve noticed that you also proposing to have type in LinkedAction in Sign Message proposal. Is it a bit outdated based on what was discussed in scope of this sRFC, or you have different considerations for having it in LinkedAction in your proposal?\nGood catch, yes that is outdated then. I do not think LinkedActions should have a type at all with any of the current sRFCs or any specific features I foresee in the future.\nI’m happy to have my mind changed, I just do not currently see a need for it.\n@nickfrosty, the flow you’ve described above looks right to me if we don’t consider External Linking .\nwhat special consideration do you see for “external linking” as an action?\nmy thought was:\nif type == “external-link” =&gt; nothing for the user to sign, no post request, just display a button/link to open the provided href as a full link in a new tab (likely requiring this be an absolute url and displaying the domain to the user below the button/link)\naside from good UX design from designers (like you mentioned in the other sRFC), do you foresee something different?\nSide notes:\nI generally thing “external links” should NOT be allowed in the initial GET metadata, ONLY in follow on chained actions. If external links were supported, then people might simply put a external link and no other action. This feels like an anti-pattern for what actions are.\nI feel fairly strongly that external linking should be considered a terminal action, like the current completed state. After the user has completed all interactions with the blink, the provider can provide an external link to be display to the user. I think this because external links inherently take the user away from the blink they are interacting with.\nso this is contributing to me thinking there is no need to add a type field into LinkedAction\n\n[tsmbl]: nickfrosty:\nI do not think LinkedActions should have a type at all with any of the current sRFCs or any specific features I foresee in the future.\nwhat special consideration do you see for “external linking” as an action?\nmy thought was:\nif type == “external-link” =&gt; nothing for the user to sign, no post request, just display a button/link to open the provided href as a full link in a new tab (likely requiring this be an absolute url and displaying the domain to the user below the button/link)\nMy consideration was based on the External Linking proposal and previous experience with other tools, that clearly indicate the action type on button in certain cases. I was assuming that External Linking is proposed to be implemented by adding type to LinkedAction, allowing to process external link without making extra POST call. I also didn’t notice that you’re hesitant about the External Linking proposal.\n nickfrosty:\nI only think it is needed in the POST response. I don’t think a type on LinkedActions really matters, at least right now. Right now, no matter what, the linked action will always make a POST request to the href.\nI fully agree that having type only in POST response is sufficient to deterministically cover the general flow. That said, it looks healthy thing to have type in LinkedAction in scope of External Linking for the reasons below\nImplementing extra POST endpoint &amp; making extra network call to implement external link feels a bit redundant to me\nVisually displaying external link on button can slightly improve the UX\nPlease reflect your thoughts in Blinks CTA: External Linking, we should probably discuss it there, lol.\n nickfrosty:\nI generally thing “external links” should NOT be allowed in the initial GET metadata, ONLY in follow on chained actions. If external links were supported, then people might simply put a external link and no other action. This feels like an anti-pattern for what actions are.\nI feel fairly strongly that external linking should be considered a terminal action, like the current completed state.\nMostly agree, however there still might be cases when “external links” useful in any GET metadata, including the initial one. For instance developers might need to have external link to the text document that doesn’t fit the blink description, e.g. governance proposal text or ToS. So, imo what you are describing sounds like a common sense and best practice to me, but it should not be a strong constraint in specification.\n\n[nickfrosty]: my post here in that proposal was meant to funnel the conversation into this one, since they are very closely related (at least for implementation)\nI also didn’t notice that you’re hesitant about the External Linking proposal.\nNot hesitant, I’m generally for it. Just trying to work out the finer details for it. I think the spec supporting external linking in some way is very useful.\nImplementing extra POST endpoint &amp; making extra network call to implement external link feels a bit redundant to me\nwhy would there be an extra POST endpoint?\nMostly agree, however there still might be cases when “external links” useful in any GET metadata, including the initial one. For instance developers might need to have external link to the text document that doesn’t fit the blink description, e.g. governance proposal text or ToS. So, imo what you are describing sounds like a common sense and best practice to me, but it should not be a strong constraint in specification.\nI think a url like this for a governance proposal doc could and should be linked in the description (with the blink client making it clickable), not as a primary action button with the rest of the actions. Unless it is a terminal/completed action of some sort.\nHaving an “action button” that is an external link that fits into the form ui feels wrong and out of place to me. It takes the user away from the blink vice interacting directly with it.\nThe only time it feels right is for completed states: instead of a generic “completed button” the blink would render an “external link” in place of the completed button.\n\n[tsmbl]: nickfrosty:\nwhy would there be an extra POST endpoint?\nExternal linking sRFC proposes to add type attribute to LinkedAction, author proposes that it will be used by client to determine whether to open the link or making POST network calls.\nAt the same time, based on your comments\n nickfrosty:\nRight now, no matter what, the linked action will always make a POST request to the href .\nI do not think LinkedActions should have a type at all with any of the current sRFCs or any specific features I foresee in the future.\nAdding a type field in the POST response is what would help determine how the response is handled (i.e. is this a transaction, sign message request, external link, etc)\nFollowing this, if I understand correctly, your proposal is to keep LinkedAction untouched. Meaning, client will make POST request that will return a response of external-link type. This means that as an action developer I need to implement extra endpoint, rather than just indicating the type in LinkedAction.\n nickfrosty:\nI think a url like this for a governance proposal doc could and should be linked in the description (with the blink client making it clickable), not as a primary action button with the rest of the actions. Unless it is a terminal/completed action of some sort.\nIdeally you should be able to do both. Sometimes links are very long, buttons provide better UX in many cases - let’s consider telegram bots, where you’re free to decide how to encode link.\n nickfrosty:\nHaving an “action button” that is an external link that fits into the form ui feels wrong and out of place to me. It takes the user away from the blink vice interacting directly with it.\nThe only time it feels right is for completed states: instead of a generic “completed button” the blink would render an “external link” in place of the completed button.\nHm, not sure. Imo, blinks are not limited to form UI. Developers may do variety of use-cases and even in form UI you may need to include links to external resources. I would advocate to be more open and flexible closer to Frames / Telegram bots experience where developers are free to decide what buttons and where do they need, rather than adding constraints.\nAgain, I agree with your thoughts, but it feels more like a best practice &amp; common sense to me and should not be spec-level constraint.\n\n[nze]: Hey all, thanks for moving this forward. Sorry if I’m a bit late to the discussion.\nFrankly speaking I don’t see any disadvantages in adding type in the LinkedAction, looks very simple and clear solution which is commonly used in general in the programming. And knowing it in advance would allow for blinks to render different buttons differently, for example add a link icon for buttons that’s supposed to redirect user.\nAnd in terms of limitations it’s def better to not to limit external link button to terminal state only. I believe developers can decide on their own where and when they want to redirect their users. I would really feel bad for not allowing it, because it seems like a baseline for me.\nI would really recommend to look into telegram bots api i think it’s well designed and well tested on hundred thousand of use cases(which are not yet possible in blinks btw, but would really love to make blinks as power)\n \n \n core.telegram.org\n \n \n \nTelegram Bot API\n \nThe Bot API is an HTTP-based interface created for developers keen on building bots for Telegram.\nTo learn how to create…\n \n \n \n \n \n \nimage750×576 63.2 KB\n\n[0xaryan]: Hey @tsmbl / @nickfrosty, what can we do to move this sRFC forward form here?\n\n[nickfrosty]: sounds like I am the only one that had a concern or other desires from your proposal @0xaryan, so I can void those right now and I think we can move forward with saying this SRFC is “approved”\nto clarify what changes are being proposed after all the conversation above:\nmake the transaction optional in the type (blink clients need to handle this flow)\nadd a type to LinkedAction with the following to start:\ntransaction - if the linked action should include a transaction\npost - if the LinkedAction is effectively going to just make a post request, with nothing for the user to sign\nexternal-link - originally from this SRFC (we could add this in via the same implementation)\nsign message - in the future with the sign message SRFC\nupdate ActionPostResponse to add and handle the same type described above\nno restrictions on where these external links can be presented to the user\ndid I miss anything?\ncc @tsmbl @nze\n\n[tsmbl]: Thanks for the summary, @nickfrosty. This looks good to me, I believe we can work out the remaining details during implementation, if necessary\n\n[nickfrosty]: this spec update is now live: v2.3\n \n github.com/solana-developers/solana-actions\n \n \n \n \n \n \n \n \n [spec] v2.3 - Optional Transaction In Action Chaining and External Link Support\n \n \n solana-developers:main ← thearyanag:spec-optional-transaction\n \n \n \n opened 02:44PM - 22 Aug 24 UTC\n \n \n \n \n thearyanag\n \n \n \n \n +803\n -24\n \n \n \n \n \n # TLDR\nCurrently, Actions can be chained together in successive series, but e…ach action in the chain requires the user to sign a transaction thus creating a bad UX. \n1. A Post Request Type ( Optional Transaction ) has been added to avoid signing the transaction on each action.\n2. A External-Link type has been added, to direct the user to an external website.\nThis would enable developers to \n - providing better UX by avoiding individual pop-ups for each transaction, with the help of optional transactions.\n - support cases such as adding a `Learn More` or `Manage your Squads` kind of thing, with the help of an external link.\n - Later can be extended to support use cases like sign-message where each flow on the blink client end will be slightly different.\n## Rationale\nBlinks and actions currently require individual transactions for each action in the chain. If the user wants to move to the next action in the chain, they can't without signing a transaction. Having optional transactions would enable a better UX and open more design space for the developers. \nsRFCs :\n1. https://forum.solana.com/t/srfc-32-optional-transactions-in-action-chaining/1971/18\n4. https://forum.solana.com/t/blinks-cta-external-linking/2018/10\n \n \n \n \n \n \n \nblink client implementation will follow shortly, then wallet support after\n\n",
            "comment_count": 18,
            "original_poster": "0xaryan",
            "activity": "2024-09-25T16:08:33.145Z"
        },
        {
            "id": 2018,
            "title": "Blinks CTA: External Linking",
            "url": "https://forum.solana.com/t/blinks-cta-external-linking/2018",
            "created_at": "2024-08-20T20:45:26.628Z",
            "posts_count": 12,
            "views": 360,
            "reply_count": 6,
            "last_posted_at": "2024-09-25T16:08:21.689Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "CTA Links for Blinks\nLetting users define links to external sites\nWe are looking forward to add CTA before or after somebody sends a transaction. For example, someone could add a “Learn more” button that sends the user to some website, or “Manage your asset” after a minting transaction has been sent that redirects them to a website we host.\nRight now the way to external link the user, is to make them click on the image of the blink. Which is not a good UX for a CTA, and is limited to only link.\nImplementation: \nAdding a type to the LinkedAction, that denotes if the button should call an action or send the user to the defined website\nexport interface LinkedAction {\n /** URL endpoint for an action or the URL to redirect the user */\n href: string;\n /** button text rendered to the user */\n label: string;\n /** parameters used to accept user input within an action */\n parameters?: ActionParameter[];\n /** whether is an external|action link, defaults to action */\n type?: string;\n}",
            "comments": "[nickfrosty]: I think adding some sort of CTA “action type” is a good idea. There is a decent amount of overlap in this post: sRFC 32: Optional Transactions in Action Chaining for the implementation of something like this.\nWhat type of CTAs do you think would be useful? The only one I can really think of is a link that should open a new tab for the user.\nFor user safety, should there be any restrictions on what websites can be linked to? Or would be displaying the domain below the button be sufficient enough\n\n[c4b4d4]: Will think about which other CTAs could be useful. In the post you linked, @0xaryan mentions: post, sign-message, external-link and tx of course.\nAbout the security issue: I think it would be the same security issue than already trusting the action’s entity with the transaction building, so I believe there’s the same level of trust for these links? Unless they are user-generated.\nTaking that in consideration maybe we can have some type of “cors policy” in the host’s /actions.json defining which hosts are trusted.\n\n[0xaryan]: I believe, adding the domain below the button should be sufficient, there should be no restrictions on what sides it can be linked to.\nIf the website the user is getting redirected to, is a bad actor then Wallets ( blowfish ) can handle that IMO.\n\n[nickfrosty]: yeah, I agree. I think displaying domain should be sufficient. I just wanted to see what others thought\n\n[nickfrosty]: what would adding some cors policy type data to the actions json accomplish?\n\n[tsmbl]: The proposal makes sense to me, I’ve seen several requests and questions for supporting this.\n 0xaryan:\nI believe, adding the domain below the button should be sufficient\nThis can work, as an idea we can visually indicate external link by a special icon and/or may be even show a warning before opening external link. I personally think we should ask for recommendations from UX/UI experts during client implementation and polish this.\n c4b4d4:\nI think it would be the same security issue than already trusting the action’s entity with the transaction building, so I believe there’s the same level of trust for these links\nThis framing looks right to me\n\n[c4b4d4]: In cases that the links are user-generated. It would be a way for all to agree that an entity would only and only allow X domains.\nNFTs have an external url field where creators can put a website there, a Blink could add a button that links to this website. The server’s entity could filter malicious ones, but wanted to extend an idea to have a public list that anyone could read, regarding the security concern.\nDoes it make sense? Could also be too extra and not worth the maintenance of it on the long term, exhausting protocols with too much stuff is not cool.\n\n[Damien]: I would personally love to see this. There’s a lot of use cases where action buttons which link to external sites would be useful.\nFarcaster frames had this since the beginning and a lot of the frame actions were actually linking to external sites, most of them were ones which required some other external authorization/signup before executing a transaction, therefore it had to be done on the third-party website instead of directly through the Frame.\nOther use cases were already mentioned and also come in very handy, such as having a link to a page where users are presented with additional information (think “Learn more” button).\nThe implementation makes sense, I think for now it’s enough to have a LinkedAction which can either be one that triggers an endpoint call (action) or is just a link that opens a new tab in the browser.\n\n[nickfrosty]: to progress the proposal along, I want to vocalize that I am for this proposal.\nfor the specific implementation, I think it can and should be easily included into the same implementation effort as SRFC 32\neffectively making “external links” a different type of action. some actions will be transactions, some will be external links, etc.\nthe one caveat I think the spec update SHOULD include is that blink clients should at least display the domain of the link to the user somehow. eg: under the button or something. the design specifics can be up to each blink client\ncc @c4b4d4 @tsmbl @nze\n\n[tsmbl]: I support this proposal as well and agree that it can be easily integrated into the same implementation effort as SRFC 32. I think we are ready to move forward with the implementation, and any additional details can be addressed as they arise during the process.\n\n[nickfrosty]: this spec update is now live: v2.3\n \n github.com/solana-developers/solana-actions\n \n \n \n \n \n \n \n \n [spec] v2.3 - Optional Transaction In Action Chaining and External Link Support\n \n \n solana-developers:main ← thearyanag:spec-optional-transaction\n \n \n \n opened 02:44PM - 22 Aug 24 UTC\n \n \n \n \n thearyanag\n \n \n \n \n +803\n -24\n \n \n \n \n \n # TLDR\nCurrently, Actions can be chained together in successive series, but e…ach action in the chain requires the user to sign a transaction thus creating a bad UX. \n1. A Post Request Type ( Optional Transaction ) has been added to avoid signing the transaction on each action.\n2. A External-Link type has been added, to direct the user to an external website.\nThis would enable developers to \n - providing better UX by avoiding individual pop-ups for each transaction, with the help of optional transactions.\n - support cases such as adding a `Learn More` or `Manage your Squads` kind of thing, with the help of an external link.\n - Later can be extended to support use cases like sign-message where each flow on the blink client end will be slightly different.\n## Rationale\nBlinks and actions currently require individual transactions for each action in the chain. If the user wants to move to the next action in the chain, they can't without signing a transaction. Having optional transactions would enable a better UX and open more design space for the developers. \nsRFCs :\n1. https://forum.solana.com/t/srfc-32-optional-transactions-in-action-chaining/1971/18\n4. https://forum.solana.com/t/blinks-cta-external-linking/2018/10\n \n \n \n \n \n \n \nblink client implementation will follow shortly, then wallet support after\n\n",
            "comment_count": 11,
            "original_poster": "c4b4d4",
            "activity": "2024-09-25T16:08:21.689Z"
        },
        {
            "id": 2051,
            "title": "Blinks Language Localization",
            "url": "https://forum.solana.com/t/blinks-language-localization/2051",
            "created_at": "2024-09-03T00:41:49.003Z",
            "posts_count": 3,
            "views": 127,
            "reply_count": 0,
            "last_posted_at": "2024-09-11T18:41:01.069Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Blinks Language Localization\nAdding Language code to requests for localizing texts\nLocalizing texts to the user’s preferd language gives a better ux, plus it facilitates onboarding non-english speakers.\nSince the text of stuff is rendered on the server side, there are two ways we could have it done:\nThe client tells the server what languages does the user’s computer has\nThe server sends different JSON paths for every language it supports, and the client picks the one that matches the user’s browser setting\nAddressing possible implementations for each:\n1. Client tells the server the user’s languages\nAdding a query parameter to the URL with the user’s language code.\nhttps://host.com/blink/action?locale=en,es,fr\nWhich languages are available on JavaScript thru:\nnavigator.languages\n//[\"en-US\", \"en\", \"fr\"]\n//or fallback, if navigator.languages is not available in the users browser\nnavigator.language\n//\"en-US\"\n2. Server sends multiple JSONs\nFor this option, it could be done in multiple ways:\na) Passing an object on keys that can be localized, where the key of every entry is the language code and the value the localized text.\n{\n ...\n \"label\" : { \"es\": \"Coleccionar gratis\", \"en\": \"Collect for free\" },\n ...\n}\nb) Having the entire JSON be in different keys, easier to maintain but it would not make sense for ** ActionPostResponse** that also have the transaction data, since you would have it repeated in both localized JSONs.\n{\n \"en\":\n {\n ...\n \"label\" : \"Collect for free\",\n ...\n },\n \"es\":\n {\n ...\n \"label\" : \"Coleccionar gratis\",\n ...\n },\n}\nc) Having some sort of dictionary tags and a dictionary that has all the meanings of the tags, could be too much but adding it to the discussion.\n{\n ...\n \"label\" : \"collect_for_free\",\n ...\n}\nDictionary would look something like this:\n{\n \"en\" : {\"collect_for_free\":\"Collect for free\"}\n \"es\" : {\"collect_for_free\":\"Coleccionar gratis\"}\n}\nWhat are your thoughts? Solana is very global, we don’t want to have Blinks be in english-only all the time, do we? \nEdit:\nI think it should also apply to images, since in most cases we place text on images too. So letting define an URL or different URLs for images would be part of localizing it.",
            "comments": "[wagg]: Text Localization would be a great optional addition for developers to add to their GET and POST responses.\nBrowsers and mobile devices already do a decent job of providing the current session’s language preferences on request, and individual extensions by wallets and other integrators could allow a user to set a default language preference for blinks that would override the browser/mobile preference if it exists.\nHandling the default as English unless provided gives full backwards compatibility with the current solution, and providing the text in all languages (that dev has setup for this blink) handles scenarios where the user may want to toggle between languages on the UI without a full data refetch from server.\nMaking this a queryable parameter forces the spec to start reserving keywords or parts of the path, complications that would be good to avoid in the implementation spec. Instead I would advocate for returning all language definitions as defined by the developer, as an additional object in the fetchResponse or postResponse for each language localization. This would give frontends more control over how they choose which version to render by default and allow for smooth language handling in scenarios where preferred language isn’t defined by the developer.\nIt would be ideal to lean towards best practices created by communities such as the one around i18n or other internationalization frameworks, as they have already worked through the issues we would encounter. Examples of solved problems include text-within-images, variable text length in other languages causing wrapping issues, and extended character libraries causing broken text rendering if fallbacks aren’t supported. Ideally this upgrade is championed by someone who has already implemented localization at scale.\n\n[nickfrosty]: this is a great idea in general: enabling localization.\nI do think a simple approach could be used with headers.\nblink clients could set and send the AcceptLanguage header with the locale of the user\nthe action api can read/parse this header value and provide the appropriate localized strings\nif they dont care about localization, they can still just return action metadata as they do now\nthis would also simplify implementation and upkeep on part of blink clients since blink clients simply display the strings as they are returned. it becomes up to the action api to return the desired localized strings or image urls\nrelated: we could also support color modes of users too (light mode or dark mode) in a similar way using headers sent from the client\n\n",
            "comment_count": 2,
            "original_poster": "c4b4d4",
            "activity": "2024-09-11T18:41:01.069Z"
        },
        {
            "id": 1661,
            "title": "sRFC 23 – Field Authority Interface (for Token Metadata)",
            "url": "https://forum.solana.com/t/srfc-23-field-authority-interface-for-token-metadata/1661",
            "created_at": "2024-06-12T00:37:31.068Z",
            "posts_count": 5,
            "views": 515,
            "reply_count": 1,
            "last_posted_at": "2024-08-20T23:57:04.292Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "sRFC 23 – Field Authority Interface (for Token Metadata)\nSummary\nAn interface that works alongside the Token Metadata Interface (sRFC 00017) which provides field-based authority to additional public keys.\nProblem\nThe Token Metadata Interface outlines a basic standard which programs can implement for updating and retrieving fungible and non-fungible token metadata. The interface specifies a single update authority for all write operations. This works for basic tokens and NFTs.\nBut if you want to do something more advanced, such as allowing token holders to edit the token’s metadata, you have to either give them access to all fields or set the update authority to a PDA and carry out the logic in a custom program. The former doesn’t work because core fields could be edited like name and uri which would disrupt marketplaces and dapps. The latter “closes” the interface and takes us back to closed-program territory.\nSolution\nWe propose the “Field Authority Interface,\" a way to specify additional public keys as authorities on specific metadata fields. The Field Authority Interface works alongside the Token Metadata Interface; it lives in the same program and writes to the same metadata account. The update authority from the Token Metadata Interface can add and remove Field Authorities. Field Authorities can be System Program keys or PDAs that “plug in” and implement custom logic – but this time they don’t close the interface.\nfield-authority-interface-diagram1950×1489 140 KB\nImplementation / Live Example\nAt Garden Labs, we released a PFP collection called AI Aliens which implements the Field Authority Interface along with a Holder Metadata Plugin. We open sourced the code and provided a detailed write-up. The website and tweet thread focus on the holder metadata functionality, but it uses the Field Authority Interface under the hood. The implementation is fairly simple: adding a Field Authority creates a PDA with data, updating metadata via a Field Authority requires providing this PDA and a signature from the authority, and removing the Field Authority closes the PDA.\nLimitations\nThis implementation only allows for one Field Authority per field. It also uses PDAs which perhaps limits discoverability. We want to keep the implementation simple for now while the Token Metadata Interface gets adopted / understood.\nFurther Thoughts\nWith the Field Authority Interface, most additional metadata functionality should be covered. Custom logic can be implemented via external programs that plug in. That said, as the ecosystem matures, it may make sense to turn some of these plugins like Holder Metadata into interfaces themselves.",
            "comments": "[joncinque]: This is a really neat idea! Keeping the two interfaces in the same program simplifies a lot of the design.\nMy main idea for a multi-authority approach required the main authority to be set to a PDA on another program. This other program would handle the multiple authorities. The tradeoff is that you don’t need a bespoke program that implements two interfaces – instead, anyone can use the two deployed programs directly. On the flipside, the multiple levels of programs can be much more cumbersome to use unfortunately.\nAll in all, this is a great idea and a lovely use of program interfaces!\n\n[jacksondoherty]: Thank you!\nI originally considered the PDA approach but felt the cumbersome nature you’re talking about – for example, an NFT editing tool / website that uses the interface would present the wrong transactions. Maybe it could see that the authority is a PDA and know to use the PDAs program, but it would then have to know that program’s instructions and… we sort of end where we started, a closed Metaplex-like program.\nA lighter approach to what I’ve proposed might be to just add a second optional authority to the Token Metadata Interface that could be for PDAs, etc.\nAt the same time, I think one of the benefits of the field-based approach is that you can have multiple of these secondary programs “plug in.” I think this architecture lends itself better to libraries of plugins that do different types of auth-ing – holder-based, multi-sig, additional token-gating, etc. etc. It looks like Metaplex might have independently converged on this idea with their plugins and authority types in Core.\n\n[Gabynto]: The Field Authority Interface seems like a smart solution to expand metadata control without complicating the Token Metadata Interface. I would love to see how this can enable more flexibility and advanced use cases for NFTs and tokens.\n\n[jacksondoherty]: I’ve implemented a V2 which removes the PDAs and uses a TLV-based approach. A list of field authorities are now placed inside the same account as the metadata via a second TLV struct. This allows for much easier composability, no longer having to pass extra accounts (code).\nThis also matches the UpdateField parameters types and accounts with Token Metadata Interface’s. @joncinque perhaps the Token Metadata Interface’s spec could loosen what is meant by update_authority in the UpdateField function? With TLVs it seems we can actually handle a much broader set of functionality / make this a bit more powerful without having to change any code.\n\n",
            "comment_count": 4,
            "original_poster": "jacksondoherty",
            "activity": "2024-08-20T23:57:04.292Z"
        },
        {
            "id": 1734,
            "title": "sRFC 28: Blinks Chaining",
            "url": "https://forum.solana.com/t/srfc-28-blinks-chaining/1734",
            "created_at": "2024-06-27T13:11:53.528Z",
            "posts_count": 19,
            "views": 1173,
            "reply_count": 10,
            "last_posted_at": "2024-08-04T04:52:52.046Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Blinks as they exist now have a single depth to interactions. You fetch the entrypoint of the blink with a GET request, then use one of the button actions to make a POST request to the server to fetch a transaction with the user’s account info.\nThis single depth prevents good error handling, and can be easily expanded to allow for blink chaining by just reusing the ActionGetResult for the POST request (with an optional transaction field) and rerendering the blink.\nThis allows branching logic on the part of the blink, where actions can be shown specific to the user’s account, and even multiple transactions could be carried out. This also allows for better error messages and error handling, giving more info to the end user.\nSo what does this look like?\nCurrently ActionGetRequest looks like this:\nexport interface ActionGetResponse {\n /** url of some descriptive image for the action */\n icon: string;\n /** title of the action */\n title: string;\n /** brief description of the action */\n description: string;\n /** text to be rendered on the action button */\n label: string;\n /** optional state for disabling the action button(s) */\n disabled?: boolean;\n /** optional list of related Actions */\n links?: {\n actions: LinkedAction[];\n };\n /** optional (non-fatal) error message */\n error?: ActionError;\n}\nwe can expand it to add:\nexport interface ActionUnifiedResponse {\n /** url of some descriptive image for the action */\n icon: string;\n /** title of the action */\n title: string;\n /** brief description of the action */\n description: string;\n /** text to be rendered on the action button */\n label: string;\n /** optional state for disabling the action button(s) */\n disabled?: boolean;\n /** optional list of related Actions */\n links?: {\n actions: LinkedAction[];\n };\n /** optional (non-fatal) error message */\n error?: ActionError;\n /** optional b64 encoded transaction */\n transaction?: string\n}\nThis would get rid of the ActionPostResponse completely, and just use this unified response for all blinks.\nSo what would this look like?\nUser fetches the entrypoint to the blink with standard GET request and gets a list of actions\nUser clicks on an action, which POST requests with their account to the given action URL\nThis returns a new ActionUnifiedResponse with an optional transaction they can sign and rerender of potential links or errors the server had in case the account + path from previous request resulted in a transaction that didn’t work (like if that account was specifically timed out from interacting with that path).\nLoop until done\nNotes\nDoes chaining mean that servers have to hold state?\nNot necessarily, all state can be URL encoded as path params\nWhat are some other cool things this allows?\nRight now one of the biggest challenges is blink generation unique to user has to be done off platform :- if you want users to have their own blinks they have to go to telegram or discord or a website or something to generate a blink cause twitter api access is trash. With this you could give out new blinks to uses on twitter itself",
            "comments": "[pirosb3]: Hi @spacemandev this is a great idea, and definitely something I’m interested in.\nFor a secure authenticated session to take place, there needs to still be some form of transaction or signature. In the current implementation of Blinks only transactions are supported (looks like signatures are coming soon) - and it seems that once a transaction is sent, the blink reaches a final state. I would love to see this proposal also enable multiple subsequent states after a transaction takes place\n\n[spacemandev]: Actually, this implementation would support secure sessions. After the first transaction (which can be a memo ix or transfer of lamport from and to the user), you can give a JWT to the user encoded in the query parameters.\nThis allows you do to a multi state system with a secure session\n\n[nickfrosty]: I like this general idea of “blink chaining”. Support for chaining the Actions API responses in a sequential way like this could open some interesting designs and applications for actions/blinks.\nAside: I think technically it should be title “actions chaining” since the blink is just the renderer and not always required to be used. The actions are being chained and can be client agnostic.\nQuestions:\nHow many actions could be chained together? No limit or some limit?\nHaving no limit might be nice for some use cases, but I suspect it will lead to a lot of user drop of while submitting actions and signing transactions. Chaining alone might lead to some users getting confused when the UI gets updated on the subsequent chained action if the messaging in the previous aciton is unclear (I guess these are all general UI design best practices and not necessary actions specific though)\nHow would the action-aware client (like a blink) know to stop requesting for subsequent chained actions? Is it simply if the proposed UnifiedResponse has no links.actions declared?\nIn the current actions spec, if the GET response has no links.actions is not provided, then a single button is expected to be rendered in the client and the POST request be made to the same url as the GET request (aka backwards compatible with Solana Pay transaction requests). However, if links.actions does exist, then the POST request is made to the corresponding href value for the action a user submits via button click.\nWhat should happen if the action api returns a transaction and the rest of the metadata to support chaining an additional action? Should the user be promoted to sign the transaction before rendering the new chained actions? Should it be before?\nWhy remove the ActionPostResponse in favor of your proposed ActionUnifiedResponse that seems to only add the optional transaction field?\nEven in your proposal, it seems like after the very first GET response to collect the initial metadata and available actions, you are proposing to continue to make POST requests. Why not just add the optional transaction field to the ActionPostResponse interface? If this transaction field exists, it is the action api implicitly saying “I am performing action chaining”\nWith the idea of action chaining via your proposed “unified response” interface, should the error field become fatal? Halting the chain of events? Or should the only error drive fatal halting be when a proper http error coded response is returned from the action api?\n\n[nickfrosty]: Also, with a goal to maintains backwards compatability between action-aware clients that support action chaining and those that do not, do you have concerns or thoughts on how an action api should react if the client does not perform the chaining?\nWith another change to the spec for action-aware clients somehow declaring “what features they support” (which I dislike the idea of), there is no way for an action api to know if their request to chain multiple actions will actually be facilitated by the client UI\n\n[spacemandev]: How many actions could be chained together?\nWith the session based model on actions, it’d be up to the app developer to make sure they aren’t making their sessions so long that users drop off and have good handle on recovery\nHow would the action-aware client (like a blink) know to stop requesting for subsequent chained actions? Is it simply if the proposed UnifiedResponse has no links.actions declared?\nYes, you could still have ActionGet flow be the kick off flow to remain backwards compatibility and have no links.actions on that. You “end” the session when the user is done pressing buttons, not when there’s no links.actions left. I’m assuming the final branch of a flow would just return disabled is true to end the session\nWhat should happen if the action api returns a transaction and the rest of the metadata to support chaining an additional action? Should the user be promoted to sign the transaction before rendering the new chained actions? Should it be after?\nDefinitely before. As a follow on, it’s also important that there’s a way to fetch the txn signature from the client and then have it in the context of the following post request so the backend can confirm it\nWhy remove the ActionPostResponse in favor of your proposed ActionUnifiedResponse that seems to only add the optional transaction field?\nYou’re confusing ActionPostResponse with ActionGetResponse. ActionPostResponse in the current spec only has transaction and message fields. Specifically the reason you wouldn’t want to add transaction field to ActionGetResponse is because passive rendering of an action (such as through a blink on twitter) should not popup a client transaction. It’d be pretty bad UX if you were scrolling on twitter and your wallet kept popping up. Only after the first GET should there be an option of POST requests with transactions\nWith the idea of action chaining via your proposed “unified response” interface, should the error field become fatal? Halting the chain of events? Or should the only error drive fatal halting be when a proper http error coded response is returned from the action api?\nI don’t think the error field should become fatal. It can be used for a “try again” by users for example when the transaction cannot be confirmed or in case of other app logic.\nIn terms of backwards compatibility, this is a tough one. You could add a client version header on the initial GET request maybe?\n\n[nickfrosty]: Yes, you could still have ActionGet flow be the kick off flow to remain backwards compatibility and have no links.actions on that. You “end” the session when the user is done pressing buttons, not when there’s no links.actions left. I’m assuming the final branch of a flow would just return disabled is true to end the session\nMakes sense. I think using disabled to end the chaining session would be the easiest implementation for sure. For some reason, by brain does like it and it seems like not the best dev experience, but I cannot articulate why…\nDefinitely before. As a follow on, it’s also important that there’s a way to fetch the txn signature from the client and then have it in the context of the following post request so the backend can confirm it\nA few people have suggested wanting some sort of “callback” functionality to verify the signature on their server side. With action chaining, it makes sense to desire this too. But on the flip side, since the transaction id is being provided by the client, it can easily be spoofed.\nYou’re confusing ActionPostResponse with ActionGetResponse. ActionPostResponse in the current spec only has transaction and message fields. Specifically the reason you wouldn’t want to add transaction field to ActionGetResponse is because passive rendering of an action (such as through a blink on twitter) should not popup a client transaction. It’d be pretty bad UX if you were scrolling on twitter and your wallet kept popping up. Only after the first GET should there be an option of POST requests with transactions\nOhh you are right. I misthough / mistyped. I meant in the PostResponse to add the same fields as the GetResponse. Updating the interface to be this is what I meant to suggest:\n/**\n * Response body payload returned from the Action POST Request\n */\nexport interface ActionPostResponse extends ActionGetResponse {\n /** base64 encoded serialized transaction */\n transaction: string;\n /** describes the nature of the transaction */\n message?: string;\n}\nSpecifically the reason you wouldn’t want to add transaction field to ActionGetResponse is because passive rendering of an action (such as through a blink on twitter) should not popup a client transaction. It’d be pretty bad UX if you were scrolling on twitter and your wallet kept popping up.\nTotally agree lol. This is also why the initial GET request has no body payload since if it did, it would in theory send the user’s wallet address to every blink on the page. Bad experience and removes the user’s ability to interact or not interact with a specific blink.\nI don’t think the error field should become fatal. It can be used for a “try again” by users for example when the transaction cannot be confirmed or in case of other app logic.\nMakes sense, but the current non-fatal error does not follow this “try again” flow really.\n\n[0xaryan]: Currently the ActionPostResponse looks like\n/**\n * Response body payload returned from the Action POST Request\n */\nexport interface ActionPostResponse {\n /** base64 encoded serialized transaction */\n transaction: string;\n /** describes the nature of the transaction */\n message?: string;\nWhen the user signs the transaction, the action server has no option to verify/know if the user has signed the transaction, therefore the server needs to scan all the transactions for the given programId and check if the user has made the transaction or not. Scanning all the transactions is not a feasible approach, which can be replaced by a callback URL , the callback URL would accept a signature ( signed by the user ) and the account ( base58-encoded representation of the public key of the user ) in the body to link the transaction.\n/**\n * Response body payload returned from the Action POST Request\n */\nexport interface ActionPostResponse {\n /** base64 encoded serialized transaction */\n transaction: string;\n /** describes the nature of the transaction */\n message?: string;\n /** callback URL to be called after the transaction is confirmed */\n callback?: string;\n}\nAfter the user has signed the transaction, then the blink-client would use the callback URL field to send a HTTP OK JSON with the following payload\n/**\n* Response body payload returned from the Callback Post Request\n*/\nexport interface CallbackPostResponse {\n /** user signature */\n signature: string;\n /** base58-encoded representation of the public key of the user */\n account: string;\n}\n\n[nickfrosty]: with regards to @spacemandev’s proposal for action chaining, how does your callback idea fit into it?\ndev’s proposal suggests the post response would return the optimistic UI items that should be rendered after the previous transaction is successful, allowing 1 less network request and the user to immediately interact with the next action in the chain (as if it was a freshly rendered blink)\nfor your callback proposal, is the purpose to simply tell the action api server that the transaction was successful or to get the next action in the chain only after it was successful? or something else?\n\n[0xaryan]: For the callback proposal, the idea was simply to tell the action API server that the transaction was successful so that the API server can do any event linked to the user success tx.\nNow that I think of it,\nIn addition to the @spacemandev proposal of having the optimistic UI items in the post-response, the post-response should have 3 options:-\nsuccess optimistic ui - to be rendered after the previous transaction was successful\nfailed optimistic ui ( optional ) - to be rendered after the previous transaction failed. [ this would help in cases where the rendered blink is the last in the chain ]\ncallback URL - to enable a confirmation of the transaction to the action server, would remove the overhead on the action server to scan all the transactions related to that programId and also encourage the action developers to use third-party programs which otherwise would have a lot of on-chain transaction ( for example - a jupiter dca )\nThis can now open some interesting design space for the developers and creating better UX ( instead of having 5-6 input field in 1 rendered blink, it can be distributed among 2/3 steps )\n\n[Gabynto]: Wow, lots of good stuff here. I am learning a lot about blinks chaining\n\n[nickfrosty]: After chatting more with @spacemandev some on this, we are leaning towards this to both enable support for action chaining and provide the callback functionality. This will both prevent the new feature from being a breaking change and will play nice will other open spec change proposals and expected future ones (like message signing).\nNote: The interfaces listed below may be simplified version of their final implementations. To improve the type safety and DX, the specific names and types may be adjusted but will accomplish the same functionality.\nProposal\nUpdate the ActionPostResponse to allow passing the a url to discover the next action in the chain (via callback) or include the metadata for the next action in the action (without making a callback):\n/**\n * Response body payload returned from the Action POST Request\n */\nexport interface ActionPostResponse {\n /** base64 encoded serialized transaction */\n transaction: string;\n /** describes the nature of the transaction */\n message?: string;\n /** support action chaining */\n links?: {\n /**\n * - when `next` is type=`string` aka url =&gt; make a POST to this address with a payload of `NextActionPostRequest` to retrieve the next action\n * - note: this url is required to be same origin as the POST request returning this response\n * - when `next` is type=`ActionGetResponse` =&gt; after transaction is confirmed, render this data\n */\n next: string | ActionGetResponse;\n };\n}\nexport interface NextActionPostRequest extends ActionPostRequest {\n /** signature produced from the previous action (either a transaction id or message signature) */\n signature: string;\n}\nExplanation\nWhen any ActionPostResponse include the links.next attribute, an action chain is created/continued.\nThe links.next can either be a string url or the blink metadata (ActionGetResponse) that will trigger the chaining after the provided transaction is confirmed.\nIf the links.next is a string url, it is required to be from the same origin that creates the ActionPostResponse. After the transaction is confirmed, the blink-client should make a POST request to the provided links.next url with the user’s wallet address (account) and confirmed transaction id (signature) in the POST body as JSON.\nThink: on transaction confirmed =&gt; perform callback (with the transaction id) to get the next action in the chain\nIf the links.next is an object of ActionGetResponse, after the transaction is confirmed, this blink metadata should be rendered to the user (effectively making it appear as if a new blink) and no callback to the Actions API is made.\nThink: on transaction confirmed =&gt; render this blink metadata, I don’t need to confirm the transaction on my backend\nIf an Action provider needs to track any state between their chained actions, they must handle/validate that themselves. For example, using a query param to track the user’s current “step in the action chain”.\nWhen an Action provider wants to stop the action chain, they can return the final action as one with a disabled=true, which will stop the user from continuing via the blink UI.\nAn action chain can effectively be any length as long as the user continues to sign and confirm transactions and the action api continue to return signable transactions.\n\n[tsmbl]: @nickfrosty, this is great update! I like the idea of links.next, it looks consistent and explicit.\n nickfrosty:\nWhen an Action provider wants to stop the action chain, they can return the final action as one with a disabled=true , which will stop the user from continuing via the blink UI.\nThis will definitely work and allow to stop the chain. There is one case that came to my mind related to existing client behaviour. Currently blinks have a Completed state indicating the success of execution.\nimage1044×390 31.9 KB\nAs a developer I would like to preserve this behaviour and be able to clearly indicate the Completed state in the end of the chain. I believe setting disabled=true is not sufficient to cover this case.\nAs an idea, we could indicate Completed state explicitly, I see at least 2 options\na) Support 2 types in Chained POST Response to explicitly indicate terminal state, for example\nexport type NextActionResponse = Action | CompletedAction;\n/** A response indicating next action to be displayed to a user */\nexport interface Action extends ActionGetResponse {\n type: 'action';\n}\n/** A response indicating that terminal action state reached */\nexport interface CompletedAction {\n type: 'completed-action';\n}\nb) Somehow indicate Completed state in the GET Response, could be something like\ncompleted=true\nvariant='completed'\n nickfrosty:\nIf the links.next is an object of ActionGetResponse , after the transaction is confirmed, this blink metadata should be rendered to the user (effectively making it appear as if a new blink) and no callback to the Actions API is made.\nI think we can simplify a bit by keeping a single option for chaining - using the POST request. Rationale:\nChaining via POST request already covers all functional cases and provides broader functionality comparing to chaining via ActionGetResponse.\nHaving fewer options is better for end developer experience.\nIn my opinion the only potential benefit of chaining using ActionGetResponse is saving one network call, but the performance benefits are not clear to me in this case. So, not sure if ActionGetResponse case needs to be specially covered.\n\n[nickfrosty]: Currently blinks have a Completed state indicating the success of execution.\nThis is all in the blink-client though right? This same flow would be preserved if the final action in the chain simply does not return links.next, effectively just like now.\nIf we consider the current actions have a “action chain length of 1”, the “completed” state that the client displays shows after the transaction is confirmed and there is not links.next (since the spec does not exist yet). If there are more actions in the chain the user can continue to execute them, until when there is no longer any “next actions” set, then the blink-client can render this completed state.\nSupport 2 types in Chained POST Response to explicitly indicate terminal state,\nI do think having a way for action api’s to explicitly declare the final action in the chain is a good idea. Adding a type field to the GetResponse (which we should likely rename now lol) I think is the best way to declare this. Add in type with a default of action for backwards compatibility.\nYour proposed CompletedAction has a drawback of not allowing the action api to update the metadata at the very end of the chain which will be very useful (and I suspect will be very commonly used).\nI think we should make sure to support this case, and be able to smartly handle both cases of the final actions wanting to update the metadata AND final actions not wanting to update the metadata.\nIn my opinion the only potential benefit of chaining using ActionGetResponse is saving one network call, but the performance benefits are not clear to me in this case. So, not sure if ActionGetResponse case needs to be specially covered.\nSaving one network call is still useful imo. But the other benefit that I see is for showing the final “completed” state’s metdata at the end of the chain. After the final action is confirmed, the client already has the metadata to display and update the UI with what ever “completed metadata” the action api wants the user to see.\nFor example, if a blink is used to mint an nft, the api can return the nft artwork and other metadata when they send the transaction to the user. After the transaction is confirmed, show the nft the user just minted\n\n[tsmbl]: nickfrosty:\nThis is all in the blink-client though right?\nYes, it’s about current blink-client behaviour, that is possible because client assumes that there’s a single action.\n nickfrosty:\nIf there are more actions in the chain the user can continue to execute them, until when there is no longer any “next actions” set, then the blink-client can render this completed state.\nI think there is a scenario when this criteria is not sufficient: when the final action in chain requires submission of signature. Let’s say as an action developer I would like like to submit tx or message signature to actions API as a final step, no further user activity is required afterwards, so I expect Completed state to be rendered. Based on current the proposal, links.next is a part of POST response, but linking always returns ActionGetResponse - this response will be rendered instead of Completed state.\n nickfrosty:\nI think we should make sure to support this case, and be able to smartly handle both cases of the final actions wanting to update the metadata AND final actions not wanting to update the metadata.\nYes, agreed here. So, option (a), as formulated above, doesn’t fit in this framing. But I still believe we need an explicit Completed state indication somewhere to achieve this.\n nickfrosty:\nAdding a type field to the GetResponse (which we should likely rename now lol) I think is the best way to declare this. Add in type with a default of action for backwards compatibility.\nThis sounds close to option (b) from my message above. Can you elaborate on adding type to ActionGetResponse? Do you expect to have separate data structures for each type, or type should serve as a Completed state indicator?\nYes, def should rename, lol.\n nickfrosty:\nSaving one network call is still useful imo. But the other benefit that I see is for showing the final “completed” state’s metdata at the end of the chain. After the final action is confirmed, the client already has the metadata to display and update the UI with what ever “completed metadata” the action api wants the user to see.\nFrom my perspective, optimizing to save a network call at this stage might be a bit premature. Using POST as a single mechanic can achieve the same UX, for example same approach has been successfully implemented and proven to work in Farcaster frames. Generally, I think it’s simpler and more consistent to have a single option if we don’t sacrifice flexibility or feature completeness.\n\n[nickfrosty]: This sounds close to option (b) from my message above. Can you elaborate on adding type to ActionGetResponse ? Do you expect to have separate data structures for each type , or type should serve as a Completed state indicator?\nLet’s say we update the spec types/interfaces to this:\n/**\n * A single Solana Action\n */\nexport interface Action {\n /**\n * @default `action`\n */\n type?: \"action\" | \"completed\";\n /** image url that represents the source of the action request */\n icon: string;\n /** describes the source of the action request */\n title: string;\n /** brief summary of the action to be performed */\n description: string;\n /** button text rendered to the user */\n label: string;\n /** UI state for the button being rendered to the user */\n disabled?: boolean;\n /** */\n links?: {\n /** list of related Actions a user could perform */\n actions: LinkedAction[];\n };\n /** non-fatal error message to be displayed to the user */\n error?: ActionError;\n}\n/**\n * Response body payload returned from the initial Action GET Request\n */\nexport type ActionGetResponse = Action;\n/**\n * Response body payload returned from the Action POST Request\n */\nexport interface ActionPostResponse {\n /** base64 encoded serialized transaction */\n transaction: string;\n /** describes the nature of the transaction */\n message?: string;\n /** support action chaining */\n links?: {\n /**\n * - when `next` is type=`string` aka url =&gt; make a POST to this address with a payload of `NextActionPostRequest` to retrieve the next action\n * - note: this url is required to be same origin as the POST request returning this response\n * - when `next` is type=`Action` =&gt; after transaction is confirmed, render this data\n */\n next: string | Action;\n };\n}\nexport interface NextActionPostRequest extends ActionPostRequest {\n /** signature produced from the previous action (either a transaction id or message signature) */\n signature: string;\n}\nThe initial GET response returns the metadata to render the blink. When the user clicks a button to begin interacting with an action, it makes the POST request (just as it does now).\nIf a user does NOT want to chain an action, they do NOT return links.next. After the transaction is confirmed, the blink-client can render the “completed” state. Just as they do now.\nAny time ActionPostResponse does NOT return links.next, the blink-client knows this is the final transaction in the chain and can render the completed state after the transaction is confirmed. Just as they do now.\nIf a user wants to chain an action:\nthe POST response returns a transaction and includes the links.next which is the callback url (same origin only) to give the api server the tx id and fetch the next action\nsince the links.next was declared, you know the action chain is not yet in the final “completed” state\nafter the transaction is confirmed, the blink-client makes a POST request to this links.next url which returns the next action (aka Action aka ActionGetResponse) which declares a type value to determine what the UI should do, including handling the “completed” state\ntype=action (the default value) =&gt; regular action chain. not the terminal/completed state\ntype=completed =&gt; terminal state of the action. this allows developers to return updated metadata for the UI to render after the transaction is confirmed. (aka the completed state, but with custom updated metadata)\nif this action includes either type=action or does not declare type since it defaults to action, then this response is the next action in the chain and the user can interact with it as if a “fresh action”. it goes through the lifecycle of requests to ultimatly get the ActionPostResponse with another transaction and maybe another chained action via links.next\nthis loop will continue until either:\nno links.next is returned via the ActionPostResponse (implicitly declaring “this is the final action”). the blink-client can then render a standard “completed” ui just like it does now. no additional UI metadata gets updated (like the image, title, description, etc)\nor the action api returns type=completed, the blink-client now explicitly knows this is the final action in the chain and can render the “completed” state and also update the metadata displayed to the user (since this payload just gave it to you)\nI also think a the type value of completed is maybe not the best word to use. Something closer to the desire to “update the UI with this metadata”.\nThe type interface for Action should likely be updated to not allow including links when type=completed since this is effectively the terminal state of metadata to render to the user, and no further action exists in the chain.\nIn my opinion the only potential benefit of chaining using ActionGetResponse is saving one network call, but the performance benefits are not clear to me in this case. So, not sure if ActionGetResponse case needs to be specially covered.\nI think it will be very useful for people and improved the developer experience of building blinks. And can be handled by blink-clients with basically a single if statement check.\nIf you are really hung up on it and so opposed to having it, we can just cut it so we can finalize this sRFC and get action chaining shipped to the public.\n\n[tsmbl]: @nickfrosty, this looks conceptually complete and should work from the request/response flow and information model perspective.\n nickfrosty:\nI also think a the type value of completed is maybe not the best word to use. Something closer to the desire to “update the UI with this metadata”.\nI think we can figure out the exact semantics for type and polish other aspects such as type safety on later stages during the PR in spec repo, wdyt?\n nickfrosty:\nI think it will be very useful for people and improved the developer experience of building blinks. And can be handled by blink-clients with basically a single if statement check.\nIf you are really hung up on it and so opposed to having it, we can just cut it so we can finalize this sRFC and get action chaining shipped to the public.\nNot strongly against conceptually, we can keep it if you still think it’s useful. Let’s just ensure it’s well typed and extensible during implementation.\n\n[nickfrosty]: Agreed on getting the types all working well in the PR.\nI will work on opening a PR tomorrow then!\n\n[0xaryan]: I don’t know how I missed this discussion.\nThe idea of links.next along which includes the signature and POST URL is a banger.\n\n",
            "comment_count": 18,
            "original_poster": "spacemandev",
            "activity": "2024-08-04T04:52:52.046Z"
        },
        {
            "id": 1892,
            "title": "sRFC 31: Compatibility of Blinks and Actions",
            "url": "https://forum.solana.com/t/srfc-31-compatibility-of-blinks-and-actions/1892",
            "created_at": "2024-07-29T15:51:50.978Z",
            "posts_count": 5,
            "views": 438,
            "reply_count": 2,
            "last_posted_at": "2024-07-31T16:34:13.130Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "sRFC 31: Compatibility of Blinks and Actions\nContext &amp; Problem\nActions Spec Version Compatibility\nDifferent Blink clients, such as wallets, update at different rates. This means they might not support the latest features, which can lead to situations where Blink is rendered incorrectly, with elements not being visible or displayed correctly, or being completely non-functional.\nBlockchain Compatibility\nCurrently, the action doesn’t specify which chain to target, other than the recent blockhash being invalid on another Solana network. Also, the community has started building actions for non-Solana chains following the spec. As actions are built for various chains, there is a risk of consuming and unfurling incompatible actions, which can break client functionality.\nProposed Solution\nProposing to include compatibility metadata across Action API responses, specifically\nThe version of the spec being used.\nThe chains it supports.\nThis approach allows Blink clients to decide whether to unfurl the action based on the data provided by the server. The benefits of the approach are\nMinimizes breaking changes, including CORS configuration for existing action providers.\nEnsures low friction for end action developers by reducing the code required to maintain consistent and expected behaviour across different versions of clients.\nTo ensure smooth operation, Blink clients should follow the best-practices below:\nIf the action version is higher than the action spec version used in the client, clients should render a fallback UI indicating that version is incompatible and suggesting to update if possible. This practice encourages wallets to adopt newer spec versions and users to upgrade their clients.\nIf the client doesn’t support the blockchain, the action should not be unfurled.\nImplementation Proposal\nTechnically the proposal is to introduce compatibility metadata in the Actions API response headers. Headers are suitable for passing such metadata for several reasons:\nConsistency: Uniform application across all types of requests, including GET requests where bodies are not applicable.\nClarity: Keeps versioning metadata separate from request data, maintaining a clean separation of concerns.\nEfficiency: Headers facilitate making HEAD requests to check compatibility before fetching the entire action, improving performance.\nIn order to ensure consistent encoding and identification of blockchain networks, the proposal is to utilize chain-agnostic standards that are gaining traction in the industry, specifically CAIPs.\nSo, two response headers are proposed to be returned by action developers:\nX-Action-Version to show what spec version the action API server is using. For example, X-Action-Version: 2.1.3.\nX-Blockchain-Ids to list blockchains the action supports, using CAIP-2 compatible strings. For example, X-Blockchain-Ids: solana:5eykt4UsFv8P8NJdTREpY1vzqKqZKvdp for Solana mainnet or X-Blockchain-Ids: solana:EtWTRABZaYq6iMfeYKouRu166VU2xqa1 for Solana devnet. See Solana Namespace - Addresses | Chain Agnostic Namespaces for more details.\nFor backward compatibility, Blink clients should treat:\nThe absence of the X-Action-Version header as the last pre-compatibility release.\nThe absence of the X-Blockchain-Ids header as a Solana mainnet action.\nNote: Wallets cannot get custom response headers from browser requests by default. The Action Provider needs to add Access-Control-Expose-Headers: X-Blockchain-Ids, X-Action-Version to the response to make metadata available to scripts running in the browser.\nOut of Scope &amp; Future Work\nIntroducing compatibility metadata in Blink client requests:\nRequest Header: X-Accept-Action-Version to show the max spec version the Blink client supports.\nRequest Header: X-Accept-Blockchain-Ids to list blockchains the client supports, using CAIP-2 compatible strings.\nThis will allow for accurate handling when necessary by relying on client request headers and having extra logic to handle different client versions. While it is possible that action developers might not accurately handle the version provided by the client initially, it could be useful to have this data.\nProposing to include this change in separate sRFC to maintain backward compatibility, as current APIs would break unless existing servers update CORS settings to allow 2 extra CORS headers Access-Control-Allow-Headers: X-Accept-Action-Version, X-Accept-Blockchain-Ids.",
            "comments": "[nickfrosty]: Having clear guidelines and best practices around Actions and blinks is super important. Especially as blink clients, like wallets, ship update on different timelines (just like you called out).\nWith a versioning system like you describe (both the blink client and action server stating which they support), we can ensure the best possible user experience. I am very much for this update! Especially by using standardized headers sent by BOTH blink clients and action API servers.\nIf the action version is higher than the action spec version used in the client, clients should render a fallback UI indicating that version is incompatible and suggesting to update if possible. This practice encourages wallets to adopt newer spec versions and users to upgrade their clients.\nI think for some future features in the spec, their maybe be a reasonable fallback option to give to users instead of simply rendering a fallback UI stating its incompatible. In some cases, sure this will not be possible. But for many types of updates, I suspect having some reasonable fallback functionality should be expected on the client side.\nHow would a multi-blockchain action declare the chains it supports? Should the X-Blockchain-Ids be a comma separated list? Or something else?\nTo help with adoption of these headers, I can add helper functions, constants, and types into the @solana/actions sdk and the @solana/actions-spec package\n\n[tsmbl]: Thanks for feedback!\n nickfrosty:\nWith a versioning system like you describe (both the blink client and action server stating which they support)\nI agree that ideally, both the client and server should state what versions and chains they support and this is how I see this generally.\nHowever, I have mixed feelings about the implementation sequencing. Adding compatibility metadata in request headers from the Blink Client will break existing action providers due to incompatible CORS configurations. On the other hand, adding compatibility metadata to Action API response headers is safe and backward compatible.\nTherefore, I would propose starting with adding compatibility metadata to Action API response headers, but postponing the implementation of Blink client headers.\nWhat are your thoughts on this concern?\n nickfrosty:\nI think for some future features in the spec, their maybe be a reasonable fallback option to give to users instead of simply rendering a fallback UI stating its incompatible. In some cases, sure this will not be possible. But for many types of updates, I suspect having some reasonable fallback functionality should be expected on the client side.\nAgreed here\n nickfrosty:\nHow would a multi-blockchain action declare the chains it supports? Should the X-Blockchain-Ids be a comma separated list? Or something else?\nRealistically, I think it will be a single blockchain id in most cases for the nearest future. However, it’s comma separated list for extensibility, several examples of what could be possible in the future\nDefine actions that support both devnet and mainnet:\nX-Blockchain-Ids: solana:5eykt4UsFv8P8NJdTREpY1vzqKqZKvdp, solana:EtWTRABZaYq6iMfeYKouRu166VU2xqa1\nSupport multiple blockchains in single action (e.g. donation blink that supports both Solana and some other chain)\nX-Blockchain-Ids: solana:5eykt4UsFv8P8NJdTREpY1vzqKqZKvdp, eip155:1\nThat said, if we want full multi-chain experience, I think we will need extra sRFC that will specify several extra aspects of operating in such environment.\n nickfrosty:\nTo help with adoption of these headers, I can add helper functions, constants, and types into the @solana/actions sdk and the @solana/actions-spec package\nThis would be awesome! Def will simplify adoption\n\n[Gabynto]: Introducing compatibility metadata in the Action API responses sounds like a smart move to handle version mismatches and blockchain support issues more effectively. It’ll help Blink clients avoid breaking changes and improve overall compatibility with diverse blockchain actions.\n\n[nickfrosty]: tsmbl:\nTherefore, I would propose starting with adding compatibility metadata to Action API response headers, but postponing the implementation of Blink client headers.\nI think your plans makes the most sense. Straightforward to accomplish.\n tsmbl:\nThat said, if we want full multi-chain experience, I think we will need extra sRFC that will specify several extra aspects of operating in such environment.\nAlso makes sense, future sRFC it is.\n\n",
            "comment_count": 4,
            "original_poster": "tsmbl",
            "activity": "2024-07-31T16:34:13.130Z"
        },
        {
            "id": 1860,
            "title": "sRFC 30: Account Abstraction Interfaces",
            "url": "https://forum.solana.com/t/srfc-30-account-abstraction-interfaces/1860",
            "created_at": "2024-07-25T06:27:54.094Z",
            "posts_count": 1,
            "views": 239,
            "reply_count": 0,
            "last_posted_at": "2024-07-25T06:27:54.199Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Summary\nThis proposal aims to introduce a standardized interface for Account Abstraction (AA) in the Solana ecosystem. By following this interface, any protocol interested in AA development on Solana can unify the operation object, allowing users to focus on their intended instructions rather than dealing with different AA provider interfaces.\nMotivation\nCurrently, only a few protocols in the Solana ecosystem are developing Account Abstraction (AA) solutions, each with varying interfaces. This early stage presents an ideal opportunity to create a unified interface for AA operations. A standardized interface will simplify integration, reduce fragmentation, and enhance the overall developer and user experience. By focusing on constructing a unified operation object, EOAs (Externally Owned Accounts) can ensure interoperability and streamline interactions across different AA vendors, ultimately reducing complexity and fostering a more cohesive ecosystem.\nSpecification\nUserInstruction Struct\npub struct UserInstruction {\n pub inner_instructions: Vec&lt;InnerInstruction&gt;,\n // additional fields for replay attack\n pub nonce: u64,\n // fields for timestamp validation\n pub valid_from: u64,\n pub valid_until: u64,\n // executed with modulars ids\n pub modulars: Vec&lt;u32&gt;,\n}\npub struct InnerInstruction {\n // conventional Solana Instruction fields, except data\n pub program_id: Pubkey, //_dstAddress\n pub keys: Vec&lt;AccMeta&gt;, // solana req\n pub data: Vec&lt;u8&gt;, //_payload\n}\nUserInstruction Struct Fields Explanation\nThe UserInstruction struct is designed to encapsulate the necessary information for executing a series of instructions on the Solana blockchain while addressing concerns such as replay attacks and timestamp validation. Here is a detailed explanation of each field:\ninner_instructions: Vec&lt;InnerInstruction&gt;\nType: Vector of InnerInstruction structs\nPurpose: Holds a list of inner instructions that define the specific operations to be executed on the blockchain. Each InnerInstruction represents a single action, such as transferring tokens or invoking a contract.\nDetails: This field is primarily used for multi-call operations, allowing multiple inner instructions to be batched together. By grouping multiple inner instructions into a single UserInstruction, the transaction size is reduced, and the most relevant fields are batched together.\nnonce: u64\nType: Unsigned 64-bit integer\nPurpose: Provides a mechanism to prevent replay attacks by ensuring that each UserInstruction has a unique value.\nDetails: The nonce should be incremented for each new UserInstruction sent by the same user or entity. If an instruction with the same nonce is detected, it can be rejected as a replay attack.\nvalid_from: u64\nType: Unsigned 64-bit integer (timestamp)\nPurpose: Specifies the earliest time at which the UserInstruction can be executed.\nDetails: This field ensures that the instruction is not processed before a certain time, providing control over the timing of execution. This feature allows for future-dated transactions, meaning a transaction can be set to execute at a future time. Such transactions can be executed by anyone once the valid_from time is reached, enabling scheduled operations and enhancing automation capabilities on the Solana network.\nvalid_until: u64\nType: Unsigned 64-bit integer (timestamp)\nPurpose: Specifies the latest time at which the UserInstruction can be executed.\nDetails: This field prevents the execution of stale instructions by setting an expiration time. If the current time exceeds valid_until, the instruction should be rejected. Combined with valid_from, this field ensures that transactions can be precisely scheduled within a specific time window.\nmodulars: Vec&lt;u32&gt;\nType: Vector of unsigned 32-bit integers\nPurpose: Identifies the modular components or modules that are relevant for the execution of this UserInstruction.\nDetails: This field allows for the extension and customization of the execution logic by specifying which modules should be considered during execution. Each module might represent different aspects of the operation, such as pre-execution hooks, post-execution hooks, validation, authorization, or custom business logic. The modular approach enables flexible and reusable execution patterns, ensuring that additional logic can be incorporated before and after the main execution flow.\nCustomization and Security\nThe inclusion of nonce, valid_from, and valid_until fields not only provides replay attack prevention and timestamp validation but also allows for customized signatures within the hash value, instead of relying on block hash signatures in Solana. This design enables greater flexibility and security in transaction validation and execution.\nInnerInstruction Struct Fields Explanation\nThe InnerInstruction struct represents individual instructions that make up a UserInstruction. Here is a detailed explanation of its fields:\nprogram_id: Pubkey\nType: Public key\nPurpose: Specifies the Solana program to be called by this instruction.\nDetails: The program_id determines which on-chain program will process the instruction. In the context of Solana, this field is used to perform a Cross-Program Invocation (CPI) call, which allows one program to invoke another program on the blockchain.\nkeys: Vec&lt;AccMeta&gt;\nType: Vector of AccMeta\nPurpose: Lists the accounts that are read or written by the instruction.\nDetails: Each AccMeta includes information about the account’s role in the instruction (read-only, writable, signer, etc.). This ensures that all necessary accounts are correctly referenced and used during execution. Proper account metadata is crucial for CPIs to ensure that the invoked program has access to the required accounts with the correct permissions.\ndata: Vec&lt;u8&gt;\nType: Vector of unsigned 8-bit integers (bytes)\nPurpose: Contains the instruction-specific data, typically the parameters or payload to be processed by the program.\nDetails: This field encodes the actual operation or command to be executed by the target program. The data field allows for the passing of specific parameters required for the execution of the CPI call, enabling the target program to understand and process the instruction accordingly.\nUserInstruction Summary\ninner_instructions: List of operations to be performed.\nnonce: Unique identifier to prevent replay attacks.\nvalid_from: Start time for when the instruction can be executed.\nvalid_until: Expiry time after which the instruction cannot be executed.\nmodulars: Identifiers for additional modules relevant to the execution.\nIn the context of Solana and the provided code snippet, serializeInIx refers to the serialization of the InnerInstruction object into a format that can be hashed. This serialization process converts the InnerInstructionstruct into a byte array that can be processed further, such as being hashed using Keccak256.\nExplanation of Serialization with UserInstruction\nIn the context of Solana and the provided code snippet, serializeInIx refers to the serialization of the UserInstruction object into a format that can be hashed. This serialization process converts the UserInstructionstruct into a byte array that can be processed further, such as being hashed using Keccak256.\nSerialization using Borsh\nBorsh (Binary Object Representation Serializer for Hashing) is a binary serialization format designed to serialize data structures in a compact, deterministic manner. In Rust, Borsh is commonly used for serializing data structures before storing them on-chain or hashing them.\nTo serialize a UserInstruction using Borsh, you would typically implement the BorshSerialize trait for the struct, if not already provided, and then use the .try_to_vec() method to serialize it.\nExample:\nuse borsh::{BorshSerialize, BorshDeserialize};\n#[derive(BorshSerialize, BorshDeserialize)]\npub struct UserInstruction {\n pub inner_instructions: Vec&lt;InnerInstruction&gt;,\n pub nonce: u64,\n pub valid_from: u64,\n pub valid_until: u64,\n pub modulars: Vec&lt;u32&gt;,\n}\n#[derive(BorshSerialize, BorshDeserialize)]\npub struct InnerInstruction {\n pub program_id: Pubkey,\n pub keys: Vec&lt;AccMeta&gt;,\n pub data: Vec&lt;u8&gt;,\n}\n// Usagelet serialized_user_ix = user_ix.try_to_vec().unwrap(); // Serialize the UserInstruction\nExplanation of the Hash Construction\nThe provided hash construction uses Keccak256, a cryptographic hash function, to create a unique hash for the UserInstruction object, including its associated program ID and serialized inner instructions.\nHere’s a breakdown of the code:\nethers.keccak256(\n Buffer.concat([\n Buffer.from(aaFactory.programId.toBytes()),\n ethers.getBytes(ethers.keccak256(ethers.hexlify(serializeInIx(userIx)))),\n ])\n);\nSummary Steps Explained:\nSerialize UserInstruction:\nserializeInIx(userIx): This function serializes the UserInstruction object (userIx) into a byte array. In a Solana program using Rust, this would be done using Borsh serialization as shown above.\nConvert to Hex and Hash:\nethers.hexlify(serializeInIx(userIx)): Converts the serialized byte array into a hex string.\nethers.keccak256(...): Computes the Keccak256 hash of the hex string representing the serialized UserInstruction.\nConcatenate with Program ID:\nBuffer.from(aaFactory.programId.toBytes()): Converts the program ID to bytes.\nBuffer.concat([...]): Concatenates the bytes of the program ID and the Keccak256 hash of the serialized UserInstruction.\nFinal Hash:\nethers.keccak256(Buffer.concat([...])): Computes the Keccak256 hash of the concatenated buffer, producing the final hash that includes both the program ID and the serialized UserInstruction.\nExecution Function\nIn the context of Account Abstraction (AA) on Solana, the execution function is a standardized interface that allows any UserInstruction signed by the user to be executed across different AA provider programs. This unification is crucial for ensuring interoperability and reducing complexity for developers and users.\nPurpose\nThe purpose of the execution function is to provide a common method that AA provider programs can implement to process and execute UserInstruction objects. By following this standard interface, users can sign a UserInstructiononce and use it with any compliant AA provider program, fostering a more seamless and consistent experience across the Solana ecosystem.\nFunction Definition\npub fn execute_operation(\n accounts: &amp;[AccountMeta],\n instruction_data: &amp;[u8],\n) -&gt; Result&lt;(), ProgramError&gt; {\n // Deserialize the UserInstruction \n let user_instruction: UserInstruction = UserInstruction::try_from_slice(instruction_data)?; \n \n // Logic to process and execute the UserInstruction \n for inner_instruction in user_instruction.inner_instructions { \n // Process each inner instruction \n // Invoking another program with CPI \n }\n \n Ok(())\n}\nExplanation\nFunction Signature:\naccounts: &amp;[AccountMeta]: A slice of account metadata that provides the necessary account information for the execution context.\ninstruction_data: &amp;[u8]: A slice of bytes that represents the serialized UserInstruction object.\nResult&lt;(), ProgramError&gt;: Returns a result type to handle execution success or errors.\nDeserialization:\nlet user_instruction: UserInstruction = UserInstruction::try_from_slice(instruction_data)?;: Deserializes the byte array into a UserInstruction object using Borsh.\nProcessing the UserInstruction:\nIterates over each InnerInstruction within the UserInstruction.\nProcesses each inner instruction (e.g., invoking another program using CPI).\nConclusion\nThe design choices for the UserInstruction and InnerInstruction structs, as well as the unified execution interface, prioritize flexibility, extensibility, simplicity, and security. These design choices include:\nUserInstruction can encapsulate multiple InnerInstruction objects, enabling complex operations to be executed atomically. The modulars field allows for customizable pre-execution and post-execution hooks, validation, authorization, and other business logic.\nThe unified execution interface (execute_operation) standardizes processing across different AA provider programs, fostering interoperability and simplifying development. This standardization ensures a consistent experience for developers and users, reducing fragmentation and complexity across different AA providers.\nSecurity is enhanced through the nonce field, preventing replay attacks by ensuring unique UserInstruction instances, and valid_from and valid_until fields, providing timestamp validation to prevent executing stale instructions.\nEfficiency is achieved by batching multiple InnerInstruction objects into a single UserInstruction, reducing transaction size and overhead, improving network efficiency. Keccak256 hashing, including the program ID, ensures cryptographic consistency and unique, secure instructions.\nBy adopting this standardized interface for Account Abstraction (AA) in the Solana ecosystem, the operation object can be unified across different AA provider programs. This unification enhances AA capabilities, streamlines user interactions, and simplifies the development process for protocols and users alike, fostering a more integrated, efficient, and user-friendly blockchain environment. The Solana ecosystem benefits from improved interoperability, consistency, flexibility, security, and efficiency, ultimately contributing to a more robust blockchain infrastructure.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "Elwin",
            "activity": "2024-07-25T06:27:54.199Z"
        },
        {
            "id": 1804,
            "title": "sRFC 29 - Input types of blinks and actions",
            "url": "https://forum.solana.com/t/srfc-29-input-types-of-blinks-and-actions/1804",
            "created_at": "2024-07-15T19:12:28.961Z",
            "posts_count": 16,
            "views": 410,
            "reply_count": 10,
            "last_posted_at": "2024-07-24T13:06:08.274Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "sRFC 29 - Input types of blinks and actions\nTLDR\nAdd new input types within blink clients to improve the user experience and unlock new use cases for Action builders.\nRationale\nThe current Solana Actions and blinks specification only supports a single, generic input type of a non-structured text box. While this basic input is useful, having more explicit and declarative input types would allow Action builders and blink clients to unlock new use cases and improve the user experience of using blinks across the internet.\nCurrent Spec\nPer the current specification, an Action api will declare what user input fields it desires using the structured response (ActionGetResponse) from the initial GET request, specifically via any child item of links.actions containing the parameters field.\nThe current specification is this:\n/**\n * Response body payload returned from the Action GET Request\n */\nexport interface ActionGetResponse {\n /** image url that represents the source of the action request */\n icon: string;\n /** describes the source of the action request */\n title: string;\n /** brief summary of the action to be performed */\n description: string;\n /** button text rendered to the user */\n label: string;\n /** UI state for the button being rendered to the user */\n disabled?: boolean;\n /** */\n links?: {\n /** list of related Actions a user could perform */\n actions: LinkedAction[];\n };\n /** non-fatal error message to be displayed to the user */\n error?: ActionError;\n}\n/**\n * Related action on a single endpoint\n */\nexport interface LinkedAction {\n /** URL endpoint for an action */\n href: string;\n /** button text rendered to the user */\n label: string;\n /** parameters used to accept user input within an action */\n parameters?: ActionParameter[];\n}\nAny ActionParameter declared will result in a plain text input be rendered to the user (with the optional placeholder text) in order to accept their input, which ultimately gets sent the Action api endpoint via the POST request.\nAdditionally, the user input data is only sent to the Action api via query parameters and template literals (e.g. {name}) declared within the specific linked action’s href field. This presents limitations and complexities when accepting longer user input values or more complex input types.\n/**\n * Parameter to accept user input within an action\n */\nexport interface ActionParameter {\n /** parameter name in url */\n name: string;\n /** placeholder text for the user input field */\n label?: string;\n /** declare if this field is required (defaults to `false`) */\n required?: boolean;\n}\nProposal\nTo support sending longer and more complex user input to the Action API (via the POST request), user input should be optionally sent via the POST request body, not just templatized query parameters (like the user’s account address is already being sent).\nAll user input should be sent to the Action API as follows for each LinkedAction:\nany input parameters specified via template literals in query parameters of the href value should have their respective user input values sent via the its named query parameter (this is how the spec currently works)\nall remaining input parameters (aka those that do NOT have a template literal declared in the href ) should be sent in the body of the POST request (along side the user’s account, but should never be able to modify the account value)\nNote: if a template literal for any of the parameters is found, it should not be sent via the body. This will reduce the total data sent over the network and is always a best practice to not send duplicate data.\nIn the end, this allows developers to accept user input via query parameters (not breaking existing applications) or via the POST request body. And effectively set an implicit default of using the POST body to receive the user input, just like a typical HTML form.\nUpdate the ActionParameter to support declaring different types of user input. In many cases, this type will resemble the standard HTML input element.\nThis new new type attribute should have a type declaration as follows:\n/**\n * Input type to present to the user \n * @default `text`\n */\nexport type ActionParameterType =\n | \"text\"\n | \"email\"\n | \"url\"\n | \"number\"\n | \"date\"\n | \"datetime-local\"\n | \"checkbox\"\n | \"radio\"\n | \"textarea\"\n | \"select\";\nEach of the proposed type values should normally result in a user input field that resembles a standard HTML input element of the corresponding type (i.e. &lt;input type=\"email\" /&gt;) to provide better client side validation and user experience:\ntext - equivalent of HTML “text” input element\nemail - equivalent of HTML “email” input element\nurl - equivalent of HTML “url” input element\nnumber - equivalent of HTML “number” input element\ndate - equivalent of HTML “date” input element\ndatetime-local - equivalent of HTML “datetime-local” input element\ncheckbox - equivalent to a grouping of standard HTML “checkbox” input elements. The Action api should return options as detailed below. The user should be able to select multiple of the provided checkbox options.\nradio - equivalent to a grouping of standard HTML “radio” input elements. The Action api should return options as detailed below. The user should be able to select only one of the provided radio options.\nOther HTML input types not specified above (hidden, button, submit, file, etc) are not supported at this time.\nIn addition to the elements resembling HTML input types above, the following user input elements are also supported:\ntextarea - equivalent of HTML textarea element. Allowing the user provide multi-line input.\nselect - equivalent of HTML select element, allowing the user to experience a “dropdown” style field. The Action api should return options as detailed below.\nWhen type is set as select, checkbox, or radio then the Action api should include an array of options that each provide a label and value at a minimum. Each option may also have a selected value to inform the blink-client which of the options should be selected by default for the user (see checkbox and radio for differences).\n interface ActionParameterSelectable extends ActionParameter {\n options: Array&lt;{\n /** displayed UI label of this selectable option */\n label: string;\n /** value of this selectable option */\n value: string;\n /** whether or not this option should be selected by default */\n selected?: boolean\n }&gt;;\n}\nIf no type is set or an unknown/unsupported value is set, blink clients should default to text and render a simple text input (just as they do now).\nThe Action API is still responsible to validate and sanitize all data from the user input parameters, enforcing any “required” user input as necessary.\nFor platforms other that HTML/web based ones (like native mobile), the equivalent native user input component should be used to achieve the equivalent experience and client side validation as the HTML/web input types described above.\nNote: As with the current spec, if a LinkedAction does not declare the parameters attribute, then no user input is requested by the Action api and blink clients should continue to render a single button that performs the POST request to the href endpoint. This proposal does not change that.\nClosing Notes\nWith the support of more input types, developers will be able to build more complex blinks and accept more user input. Having “too many” user input fields could result in a reduced user experience and also make users less likely to actually enter all the requested inputs.\nTo avoid UI bloat and degrading user experiences, blink-clients are likely to impose “soft limits” on the number of input fields they display. While this Actions/blink specification avoids taking an opinionated approach on the UI layer of blinks, the following is a reasonable guideline (and used by Dialect’s blinks SDK today):\n10 buttons + 3 inputs (if separate Actions)\n10 inputs (if it’s a form, and other actions are not rendered if there is a form in the response)\nAs such, Action API developers should limit the number of input parameters they request as blink-clients may limit how many input fields get shown to users. Developers should also keep in mind that if a user does not see all input fields (because the blink-client soft limits them as described above), then the user will have no way to enter a value. Therefore if all these fields are required by the Actions API, the user will NOT be able to actually execute your Action and get a transaction.",
            "comments": "[0xaryan]: For the user input, instead of having different types for text , email , url, number , a general tag regexp can be used, which would provide more flexibility.\nThe ActionParamaterType can then be modified to\n/**\n * Input type to present to the user \n * @default `regexp`\n */\nexport type ActionParameterType =\n | \"regexp\"\n | \"date\"\n | \"datetime-local\"\n | \"checkbox\"\n | \"radio\"\n | \"textarea\"\n | \"select\";\nThe following regex pattern can be used for the text, email, url , number\ntext - .*\nemail - ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\nurl - ^(https?|ftp):\\/\\/[^\\s/$.?#].[^\\s]*\nnumber - ^-?\\d+(\\.\\d+)?\nany other regex can be used for more specific input types.\nIf ActionParamterType is set to regexp then the input field will be like\n&lt;input type=\"text\" id=\"emailInput\" name=\"emailInput\" pattern=\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\" required&gt;\nWith the support of regexp, the input field provides a more flexible approach.\n\n[nickfrosty]: Having a regex option makes a lot of sense, but could be really annoying for people out of the box that just want the benefit of simple input types like email or url.\nWe could support both though! Adding an additional type=regexp and accepting an additional regexp value in the ActionParameter type. This way people get the best of both worlds\nEdit:\n@0xaryan we could also simplify even more by adding the regexp attribute to the ActionParameter so it could be applied to any user input type, no matter the type.\nThe benefit of using the natively support input types (like email, url, etc) is that browsers and native clients will give the better UX and help enforce that input type in a native way (like if email is set and you try to enter a non email, the browser will not let you submit the form). Vice if you only use a regex, then the blink client would be the only layer of client side validation there, allowing users to still submit\n\n[nickfrosty]: This is my official update to the proposal above, specifically point (2)\nPS: For some reason I could not actually update the original post\nUpdate the ActionParameter to support declaring different types of user input and allow providing regular expression patterns for more complicated input.\nThe updated ActionParameter should be updated as follows:\n/**\n * Parameter to accept user input within an action\n */\nexport interface ActionParameter {\n /** input field type */\n type?: ActionParameterType;\n /** regular expression pattern to validate user input client side */\n pattern?: string;\n /** human readable description of the `pattern` */\n patternDescription?: string;\n /** parameter name in url */\n name: string;\n /** placeholder text for the user input field */\n label?: string;\n /** declare if this field is required (defaults to `false`) */\n required?: boolean;\n}\nThe pattern should be a string equivalent of a valid regular expression. This regular expression pattern should by used by blink-clients to validate user input before before making the POST request. If the pattern is not a valid regular expression, it should be ignored by clients.\nThe patternDescription is a human readable message that describes the regular expression pattern. If pattern is provided, the patternDescription is required to be provided.\nIf the user input value is not considered valid per the pattern, the user should receive a client side error message indicating the input field is not valid and displayed the patternDescription string.\n(the remaining contents of section (2) remains the same)\n\n[0xaryan]: Having the input types coupled with regexp makes more sense, and I strongly agree on making a better UX for the user.\nThe pattern and patternDescription will create a good UX and also offer more design space to the developer.\n\n[nickfrosty]: PR has been opened on the Actions spec repo:\n \n github.com/solana-developers/solana-actions\n \n \n \n \n \n \n \n \n [SPEC] v2.1 blink input types\n \n \n solana-developers:main ← solana-developers:spec-blink-input-types\n \n \n \n opened 05:41PM - 19 Jul 24 UTC\n \n \n \n \n nickfrosty\n \n \n \n \n +604\n -4\n \n \n \n \n \n ## TLDR\nAdd new input types within blink clients to improve the user experien…ce and unlock new use cases for Action builders.\n## Rationale\nThe current Solana Actions and blinks specification only supports a single, generic input type of a non-structured text box. While this basic input is useful, having more explicit and declarative input types would allow Action builders and blink clients to unlock new use cases and improve the user experience of using blinks across the internet.\n## SRFC\nSee SRFC #29 - https://forum.solana.com/t/srfc-29-input-types-of-blinks-and-actions/1804\nThe `next` tag has been published with this PR at version `2.1.0-beta`\n```\nnpm install @solana/actions-spec@2.1.0-beta\n```\n\n[fsher]: Given current support of passing arguments is limited to url encoded query parameters (through string templates) and there is still a possibility to pass them as such, I think it’s worth specifying how are they passed for multiple option types (checkbox). I propose for types that are considered multi-option (checkbox, or maybe select with multiple in the future) to be inserted as a single comma-separated string.\nExample:\nhref: \"/?test={test}\" , where test is a parameter type checkbox with options (just values for simplicity): 1, 2, 3, 4 . If user selects 1, 4 , then the request will be made with ?test=1,4 (url encoded)\nPOST body would return actions with multiple values as arrays of strings.\n\n[fsher]: Also several other suggestions:\nPOST request parameters\nSince the Actions spec may change in the future with proposals to add additional fields to POST request body, I think it’s worth moving parameter values to a separate object inside POST request body. This would avoid potential conflicts with incoming parameter action names.\nPOST request body type can look something like this:\n{\n account: string;\n /** \n Map of parameter values from user's interaction\n key - parameter name\n value - input value (by default `string`, if multi-option, `Array&lt;string&gt;`\n */\n data?: Record&lt;string, string | Array&lt;string&gt;&gt;;\n}\nMin/Max options\nMost of the parameter types can support min and max natively in some way or another. Adding them would strongly help developers achieve better UX with forms or single actions.\nExpected behavior:\ntext, url, email, textarea\nmin and max represent the minimum and maximum string length required. (should follow minlength and maxlength HTML attributes)\ndate, datetime-local\nmin and max represent the borders for date selection (should follow min and max HTML attributes)\nnumber\nmin and max represent the minimum and maximum for the numerical value (see date, datetime-local links)\ncheckbox\nmin and max represent the amount of checked boxes that user can select\nselect, radio\nnot applicable\nIf select will support multiple options, then it would follow the same rules as checkbox.\nPattern &amp; Pattern Description\npattern can be provided to all parameter types (except select, radio, checkbox, date, datetime-local).\nIf pattern present, type is not empty and not equal to text, then type would become a more stylistic property (for blink clients to render the input styled for the type) and use pattern to validate the entered value.\npatternDescription is definitely important for cases when pattern is specified, but also can be valuable for other parameter types for better explanation to the user. Which is why I’d like to suggest renaming to just description or more specific settingsDescription or validationDescription.\nWould love some feedback around these points.\n\n[nickfrosty]: This is a good callout @fsher, thanks for bringing it up. In my head I was assuming that a multi select option like checkbox would function the same as a traditional HTML form, but this could become inconsistent easily. Especially when considering non-web based platforms.\nSo I agree with making it more explicit in the spec, and your proposal looks good to me.\nMy one thought on it is that if the comma separated value is sent in a url query param: should we specify anything about the value being uri encoded, but not the commas?\nDepending on the value that the Action API declares in the option, it could have spaces or special characters which would be required to uri encode/decode. And I think having the comma uri encoded might potentially having issues when parsing it server side?\nIn my head, ideally people would not use a query param for checkboxes and instead have it sent in the body, but you never know what a dev might do lol\n\n[nickfrosty]: POST request parameters\nLove this. it helps keep things more clean. You have my vote!\nDo you think it is worth it in the spec types to allow developers to specify the keys for the data fields? This could improve DX if they decide to use it.\nSomething like this:\n/**\n * Response body payload sent via the Action POST Request\n */\nexport interface ActionPostRequest&lt;T = string&gt; {\n /** base58-encoded public key of an account that may sign the transaction */\n account: string;\n /** \n Map of parameter values from user's interaction\n key - parameter name\n value - input value (by default `string`, if multi-option, `Array&lt;string&gt;`\n */\n data?: Record&lt;keyof T, string | Array&lt;string&gt;&gt;;\n}\nMin/Max options\nAdding min/max options make sense as it can provide a better UX on the client side and is an easy add too. Using them with checkboxes is an interesting idea too, I don’t think I have ever seen that in the wild anywhere, but it could be useful for people.\nPattern &amp; Pattern Description\nYour clarification on pattern not actually applying to select, radio, checkbox, date, datetime-local is fair. I had the assumption of exactly this when writing it up, but we can make it more explicit in the spec details. Especially to help craft better types in the @solana/actions-spec package.\nFor the description, are you suggesting that this could simply be displayed to the user for any parameter field that’s declared regardless of if a regex pattern is being used?\nIf so, I’m game. I think it helps developers provide a better UX\nIf not, can you clarify?\n\n[fsher]: My one thought on it is that if the comma separated value is sent in a url query param: should we specify anything about the value being uri encoded, but not the commas?\nI believe since it’s in query params, the whole string should be encoded, including the commas. Servers should parse that into a regular string, or potentially just decode with decodeURIComponent (or similar in other languages). But I definitely agree, this is something that can be hard to debug. At the same time, since the spec now prefers sending values inside POST body, it could be ok (just a limitation that can be documented).\nWhat do you think?\n\n[fsher]: Do you think it is worth it in the spec types to allow developers to specify the keys for the data fields? This could improve DX if they decide to use it.\nYes, love it! Better typing, better DX!\nAdding min/max options make sense as it can provide a better UX on the client side and is an easy add too. Using them with checkboxes is an interesting idea too, I don’t think I have ever seen that in the wild anywhere, but it could be useful for people.\nYep, it’s just an option that can be implemented. E.g. devs can be doing questioners or group together checkboxes to make them all required (like have 2 checkboxes and min to be 2). Again, just wild guesses, I’m sure people will think of something!\nFor the description, are you suggesting that this could simply be displayed to the user for any parameter field that’s declared regardless of if a regex pattern is being used?\nThe first one, yes. Sorry, should’ve made myself clearer :D. A description for the field to explain what’s expected in the input and potentially an explanation why the validation fails.\nagain, wild (and not the best) example:\ntype: \"url\", max: 20, description: \"Please provide a shortened link (e.g. bit.ly)\"\ndescription can initially show as a label, but if user somehow entered more that 20 symbols (assuming it’s possible), that description could become an error indicator inside the client.\n\n[nickfrosty]: Understood! I like it all \nthe whole string should be encoded, including the commas.\nI think this might cause issue with complicated values, but I think it is fine just being a limitation because the POST body.\nI’m okay with the simple blanket statement to the effect of “uri encode the comma separated list, but we recommend using the query param for it”\n\n[fsher]: Just to be clear, POST body should receive an array of strings. If parameter used in query params, then it should become a comma-separated uri-encoded string.\n\n[Gabynto]: Sure! It sounds like adding more input types to blinks will really enhance what developers can do with them. Keeping the user experience in mind by limiting the number of input fields makes a lot of sense too.\n\n[nickfrosty]: Yup, we are on the same page\n\n",
            "comment_count": 15,
            "original_poster": "nickfrosty",
            "activity": "2024-07-24T13:06:08.274Z"
        },
        {
            "id": 1741,
            "title": "Displaying all NFT media types in Blinks",
            "url": "https://forum.solana.com/t/displaying-all-nft-media-types-in-blinks/1741",
            "created_at": "2024-06-29T03:44:31.663Z",
            "posts_count": 7,
            "views": 698,
            "reply_count": 2,
            "last_posted_at": "2024-07-08T16:39:36.298Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Displaying all NFT media types in Blinks\nSummary\nAdding “animation_url” and “category” to the blinks response would allow the unfurler/extension to include more exciting media in Blinks. (“category” coming from the metaplex nft standard)\nThis would greatly benefit digital artists sharing their art more and more of which is not just images. Especially generative artists who currently have no way to share their code based art live on social media.\nVr/models might be more challenging to implement afaik (would require using the model-viewer library)\nBut video and html should be pretty straight forward. The “category” would determine how the media is rendered, if not present just fall back to icon/image.\nvery basic/crude example would be:\n{category === \"html\" &amp;&amp; (\n &lt;iframe\n src={animation_url}\n sandbox=\"allow-scripts allow-same-origin\"\n referrerPolicy=\"no-referrer\"\n style=\"pointer-events: none; width: 100%; height: 100%\"\n /&gt;\n)}\n{category === \"video\" &amp;&amp; (\n &lt;video\n src={animation_url}\n autoPlay\n muted\n loop\n controls=\"false\"\n style=\"width: 100%; height: 100%\"\n /&gt;\n)}\nConcerns to discuss\nSecurity risks with iframe/html\nWebsite performance\nI’m not a security expert so please correct me if I’m wrong. But to my knowledge sandboxing the iframe should prevent most attack vectors. We also eliminate risks by preventing user interactions with “pointer-events: none” or an invisible overlay on top of the iframe.\nFor performance, I wonder how difficult it would be to add an intersection observer to each Blink, and only render the media when its in view (or even only render the highest Blink when multiple are in view)?\nThere would always be an icon/image to fall back on in the case of errors loading the media.\nCurious to know everyones thoughts!",
            "comments": "[EV3]: @c4b4d4 and @nickfrosty would love to get your input here!\n\n[c4b4d4]: I think HTML has some implications that it wouldn’t be easy to decide if it’s good or not to include.\nIt is safe as in what is being hosted in the iframe cannot read/write its parent, but is not preventing people making HTMLs that look like Twitter asking for their password \nOn the code you shared I wouldn’t use allow-same-origin, to make it secure, I think having it set makes it be treated as in the “same-origin” in certain validations\n EV3:\nsandbox=\"allow-scripts allow-same-origin\"\n3D would be pretty cool, I think model-viewer could work.\nHiding the element when not visible does help on performance (also adds complexity). I think Twitter in a normal screen size has ~15 pre-loaded tweets on its HTML\nFor the videos, on Twitter when you have multiple of them, if you had one playing and want to play another one, the first one stops and then the second one starts playing. Would catch it to keep a good UX.\n\n[EV3]: Regarding phishing attacks, I think that could easily be prevented with either “pointer-events: none” or a invisible div overlay to prevent user interactions.\nFor “allow-same-origin” I’m open to removing that if it creates a security risk. Although it would greatly reduce what could be displayed (speaking from personal experience, many of my html nfts fetch other nfts/images or fonts)\n3D would be rad, I guess I was thinking that it would be asking a bit much to add the dependency, but if thats not a concern, then yes absolutely we should add that to the spec as well\n\n[nickfrosty]: I personally think the idea of more rich experiences could be interesting, at least displaying more than a simple image (even though this could be an animated gif already)\nVideo could be possible but depending on the file size and the streaming medium, this could get hard to handle and cache. Especially when using proxy services to help protect user’s privacy. Proxy streaming a video can be expensive. And some services do not actually allow streaming (I think you cannot stream videos off of arweave for some reason)\nThe animation_url idea could be useful, but even still today not all popular Solana wallets support this. So I suspect they would be unlikely to support it this way. I suspect it is due to some of the proxy/streaming issues I mentioned above.\nFor html, while this could be really cool I personally think it is too much of a security risk for people and honestly not likely to be supported. Even if in an iframe. Too many attack vectors, even if you get everything else right.\nI am not familiar with VR models at all, would action-aware clients (like wallets and other extension) have to enable this individually?\n\n[EV3]: hmm, yah hadn’t thought about the proxy aspect for video, figured it would all be client side.\nCan you be more specific on what the security risks for a sandboxed iframe would be?\nIf that is an impassable issue, then perhaps we could come up with a standard with tight guardrails for uploading p5 projects directly, so that there’s never any raw html\n\n[nickfrosty]: I am personally not sure of the security issues with iframes, I just know there can be lots. So it throws up read flags in my head\n\n",
            "comment_count": 6,
            "original_poster": "EV3",
            "activity": "2024-07-08T16:39:36.298Z"
        },
        {
            "id": 283,
            "title": "sRFC 00017: Token Metadata Interface",
            "url": "https://forum.solana.com/t/srfc-00017-token-metadata-interface/283",
            "created_at": "2023-06-01T11:56:57.016Z",
            "posts_count": 17,
            "views": 3209,
            "reply_count": 10,
            "last_posted_at": "2024-06-26T13:42:00.448Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Token-Metadata Interface\nSummary\nToken-metadata is a very complex space, but at its base, all creators of\nfungible and non-fungible tokens need a way to upload information about their token\non-chain. This proposal contains a spec for a simple token-metadata state\nand instruction interface for SPL token mints. The interface can be implemented\nby any program.\nWith a common interface, any wallet, dapp, or on-chain program can read the metadata,\nand any tool that creates or modifies metadata will just work with any program\nthat implements the interface.\nMotivation\nToken creators on Solana need all sorts of functionality for their token-metadata,\nand the Metaplex Token-Metadata program has been the one place for all metadata\nneeds, leading to a feature-rich program that still might not serve all needs.\nAt its base, token-metadata is a set of data fields associated to a particular token\nmint, so we propose an interface that serves the simplest base case with some\ncompatibility with existing solutions.\nWith this proposal implemented, fungible and non-fungible token creators will\nhave two options:\nimplement the interface in their own program, so they can eventually extend it\nwith new functionality or even other interfaces\nuse a reference program that implements the simplest case\nSpec\nToken-Metadata Struct\nA program that implements the interface must write the following data fields\ninto a type-length-value entry into the account:\ntype Pubkey = [u8; 32];\ntype OptionalNonZeroPubkey = Pubkey; // if all zeroes, interpreted as `None`\ntype TlvDiscriminator = [u8; 8];\nstruct TokenMetadata {\n discriminator: TlvDiscriminator,\n length: u32,\n update_authority: OptionalNonZeroPubkey,\n mint: Pubkey,\n name_len: u32,\n name: [u8; 32],\n symbol_len: u32,\n symbol: [u8; 10],\n uri_len: u32,\n uri: [u8; 200],\n}\nThis struct has some ABI-compatibility with the Metaplex\nMetadata struct.\nThe discriminator here is larger, at 8 bytes instead of 1, and is a different\nvalue, but the following 318 bytes can be interpreted in the same way by wallets,\nprograms, and indexers.\nThe discriminator must be hashv(&amp;[\"token-metadata-interface::state\"])[0..8].\nBy storing the metadata in a TLV structure, a developer who implements this\ninterface in their program can freely add any other data fields in a different\nTLV entry.\nYou can find more information about TLV / type-length-value structures at the\nspl-type-length-value repo.\nInstructions\nHere are the instructions that must be implemented by a program conforming to\nthe interface.\nInitialize Token Metadata\nDiscriminator: hashv(&amp;[\"token-metadata-interface::initialize\"])[0..8]\nAccounts:\n0. [writable] Metadata account\n[] SPL mint account\n[signer] Mint authority\n[] Update authority\nData:\n0. name: String\nsymbol: String\nuri: String\nThe instruction processor must do the following:\ncheck that the mint account is an SPL mint\ncheck that the correct mint authority signed\ncheck that the name / symbol / URI fit in the limits of the struct\ncheck that the metadata account does not already have metadata written to it\nwrite all of the information into the metadata account\nNOTE: This instruction only covers initialization and assumes that the provided\naccount is properly created, meaning that it has enough space for the data, and\nenough lamports to be rent-exempt.\nUpdate Token Metadata\nDiscriminator: hashv(&amp;[\"token-metadata-interface::update\"])[0..8]\nAccounts:\n0. [writable] Metadata account\n[signer] Update authority\nData:\n0. name: String\nsymbol: String\nuri: String\nThe instruction processor must do the following:\ncheck that the metadata account is owned by the program and contains a valid\ntoken-metadata TLV entry\ncheck that the update authority signed\ncheck that the name / symbol / URI fit in the limits of the struct\nwrite the new information\nNOTE: Strings are utf-8 encoded bytes, preceded by a little-endian u32\ngiving the number of bytes.\nWhile these instructions aren’t strictly required to adhere to the interface,\nthey will integrate more nicely with other tooling.\nFor example, the JS SDK for token-interface can allow targeting any program id,\nso if a program implements these instructions properly, then they can easily get\nmore usage.\nAlternatives\nAs described at the start of this proposal, the space of token-metadata is vast\nand comprises all sorts of functionality, including fees, programmability,\ntransferability, minting, etc.\nThis proposal is deliberately not specific to fungible or non-fungible tokens,\nso it includes the base required for both, and nothing more.\nFor example, functionality for royalties could be implemented through a separate\ninterface.\n(Optional) Program-Derived Address Convention\nThis base proposal only defines the functionality required to implement the interface,\nand does not define any program-derived addresses for where the metadata should\nbe stored.\nThis approach is deliberate: by not requiring a particular address derivation,\nit is possible for a token program to also implement the metadata interface!\nOn the other hand, since many metadata programs will likely not be token\nprograms, we include an optional convention for deriving a program address for\nmetadata as the following function call:\nuse solana_program::pubkey::Pubkey;\npub fn metadata_account_address(metadata_program_id: &amp;Pubkey, mint: &amp;Pubkey) -&gt; Pubkey {\n Pubkey::find_program_address(&amp;[b\"metadata\", mint.as_ref()], metadata_program_id).0\n}\nPoint for discussion: this is different from the Metaplex token-metadata derivation,\nwhich repeats the metadata program id. This approach feels reasonable, given that\nthis interface already isn’t fully compatible with the Metaplex token-metadata accounts.\nClient libraries need to parse Metaplex token-metadata accounts differently from\naccounts that implement the token-metadata interface anyway, so special-casing\nthe address derivation for Metaplex seems reasonable.\nOn the flip-side, we can also completely omit this point and leave it for\nanother proposal regarding “token-metadata discoverability”, which may include an\ninterface for a token-metadata registry.\nFurther Work\nThis interface defines the minimum struct and instructions that a program must\nimplement in order to be considered a “token-metadata program”. It does not address\ndiscoverability of token-metadata accounts.\nFor discoverability within the mint, spl-token-2022 will add a mint “extension”\nto store the metadata account address.\nAs a proof-of-concept, spl-token-2022 will also implement this interface, and store the\nmetadata fields directly in the mint account.",
            "comments": "[Jonas.Hahn]: I love this. Could the interface also include an optional list of traits? That would be great for dynamic nfts and games for example. I think it could open lots of possibilities.\n\n[joec]: joncinque:\nThis interface defines the minimum struct and instructions that a program must\nimplement in order to be considered a “token-metadata program”. It does not address\ndiscoverability of token-metadata accounts.\nFor discoverability within the mint, spl-token-2022 will add a mint “extension”\nto store the metadata account address.\nAs a proof-of-concept, spl-token-2022 will also implement this interface, and store the\nmetadata fields directly in the mint account.\nThis is great, Jon!\nSo Token2022 will demonstrate a program adhering to the metadata interface by:\nImplementing the required instructions in extension(s)\nUsing its Mint account to also store Metadata state\nFor anyone who might be confused, Token2022 is implementing this interface through extensions, but that particular way of implementing interfaces is specific to Token2022, and you can choose to implement the instructions/state however you see fit, so long as everything checks out with things like instructions, required accounts, discriminators, etc.\nAs Jon mentioned above, something we still need to hash out going forward is the discoverability of PDA accounts with Metadata state, when devs choose not to pack both states into the Mint.\nI’m assuming we wouldn’t want to set any kind of standard for seeds (ie. “your PDA must be derived like this”)? This would make discoverability easier, but inhibit flexibility for devs.\nAn alternative might be Anchor-like discriminators - since we’re already enforcing a discriminator for the instructions?\n\n[joec]: I think the idea with interfaces - especially when it comes to Token Metadata - is to have separate, modular interfaces for every “new” type of metadata or “extension” on metadata.\nFor example, you might implement:\nJon’s interface above for the base Metadata (name, symbol, uri)\nJonas’s interface for traits (traits, etc.)\n*optionally any other interface you want\nAnd then that’s how you can build expanded metadata, and you can choose whether or not to follow Metaplex’s idea of PDAs or pack it all into one state\n\n[joncinque]: Can you describe these optional traits more? Do you mean something like “my token has some base metadata, but it also has other metadata that can change”?\nIf that’s the case, my guess is that they should be done through another interface, as Joe mentioned, since this is just for token metadata. But if you have a more complete view about how this can fit in, please let me know!\n\n[joncinque]: I’m assuming we wouldn’t want to set any kind of standard for seeds (ie. “your PDA must be derived like this”)? This would make discoverability easier, but inhibit flexibility for devs.\nWe could! That would be part of the interface, especially where it makes sense. For token metadata, basing it off the mint address seems like a slam dunk. For more complicated cases like trading programs though, it would probably inhibit flexibility if you had to use just a mint address, and not a user wallet, for example.\nAn alternative might be Anchor-like discriminators - since we’re already enforcing a discriminator for the instructions?\nDo you mean discriminators for seeds? We could certainly do that! On the flipside, since Pubkey::find_program_address is already hashing everything together, a byte-string would probably be simplest and also most flexible since you’re not beholden to any particular size.\n\n[austbot]: Hey @joncinque\nWRT my comment about changing the data layout I think in order to gain more flexibility the data layout needs to support any size of string uri and custom schematized content blocks .\nThis can be done by having a fixed size header and then manually deserializing blocks of typed data after the header. View functions on the rust or ts libs that allow a user to grab the specific content blocks that relate to uri or on chain full metadata or what have you.\nI dont see the point of this unless its better than token metadata by alot. Unless you allow the developers to do more than they can now then this may not succeed . We also know that “composability” sucks in practice when you need to compose several accounts together to get a single entity. So allowing custom data blocks to be added will allow more use cases that other interfaces can describe like a traits interface, grouping interface, multi owner interface etcetera\n\n[ngundotra]: tl;dr Proposal is missing indexing standard\n joncinque:\n(Optional) Program-Derived Address Convention\nIf this is optional, how are indexers / wallet supposed to show metadata for programs that adhere to this sRFC?\nOne of the main challenges for Metadata programs is that they will never be shown in wallets.\nThe main benefit of something like a Metadata spec should be giving each metadata program equal opportunity to be shown in a wallet, which I imagine would require some form of indexing standard as well.\n joncinque:\nuse solana_program::pubkey::Pubkey;\npub fn metadata_account_address(metadata_program_id: &amp;Pubkey, mint: &amp;Pubkey) -&gt; Pubkey {\n Pubkey::find_program_address(&amp;[b\"metadata\", mint.as_ref()], metadata_program_id).0\n}\nI think this makes sense, but this could also be extremely cost ineffective for large metadata collections.\nAnother missing item is the ability to support indexing by the following:\ncollection/grouping ID,\nowner\ndelegate\ncreators\nThese are crucial to marketplace functionality that drives the economic usefulness of the already existing token-metadata program.\nTo be frank, I think this is useful metadata interface for tokens but without the indexing standard, I don’t think it’s useful for NFTs.\n\n[joncinque]: austbot:\nWRT my comment about changing the data layout I think in order to gain more flexibility the data layout needs to support any size of string uri and custom schematized content blocks .\nThat makes sense. I’ll update the proposal to reflect something more general. I’m thinking a Vec&lt;(String, String)&gt; which allows for key-value pairs. I worry that other types would overcomplicate the proposal, and ultimately go against the core of the proposal, which is storing non-functional data.\nIf an account has additional functional data, like multiple owners, then it’s no longer metadata, and should be accomplished through a different interface. Thanks to TLV data structures, it’s possible to store all of these in the same account, but to get different views on the same account.\nBut thankfully, if people want to violate that, they can always store JSON strings\n\n[joncinque]: ngundotra:\nhow are indexers / wallet supposed to show metadata for programs that adhere to this sRFC?\nWe discussed this offline, and the solution we came up with was to add a view function which emits the data as an event, and to mark the “state” portion of the interface as optional, so that programs can have maximum flexibility in the implementation.\nPrograms that omit the “state” portion of the interface may face more integration challenges with wallets or programs that read the account data without a simulation or CPI.\n\n[jacobdotsol]: I plan to review this more in-depth but from first glance uri_len can be a u8\n\n[joncinque]: Note this has been implemented in https://github.com/solana-labs/solana-program-library/tree/master/token-metadata/interface along with an example program at https://github.com/solana-labs/solana-program-library/tree/master/token-metadata/example and an implementation in token-2022 at https://github.com/solana-labs/solana-program-library/tree/master/token/program-2022/src/extension/token_metadata, so feel free to put in an issue to SPL if you have any additional comments!\n\n[Hamster]: Appreciate this a lot! Glad to see its implemented and will check the example out.\naustbot\nHey @joncinque\nWRT my comment about changing the data layout I think in order to gain more flexibility the data layout needs to support any size of string uri and custom schematized content blocks.\nYes this is extremely important! For example: with an increased uri size of 3-4000 characters, we can essentially have whole NFT metadata (JSON with traits and images) on-chain, within the same account. That size fits SMB’s, Tensorians, etc. and of course Blockrons.\n\n[jacksondoherty]: Hey everyone! I just proposed a “Field Authority Interface (sRFC 23)” that works alongside this, check it out!\n\n[jacksondoherty]: After working with clients to adopt this, I’d recommend pivoting to just parsing account data as the standard read method. emit() requires running a transaction with a signer – clients that want to just read will therefore need to audit the program first, making it harder for new programs to get adopted.\ncc @ngundotra\n\n[joncinque]: The idea of emit() is to use it with transaction simulation or as part of a running transaction, and not as a standalone instruction, unless you wanted to maintain proof of the state of an account at a certain point in time. Although reading the bytes from the account is easier, not all programs need to support it, which is the other reason to have the instruction.\n\n[jacksondoherty]: Ahhh that makes sense, got it.\n\n",
            "comment_count": 16,
            "original_poster": "joncinque",
            "activity": "2024-06-26T13:42:00.448Z"
        },
        {
            "id": 1721,
            "title": "sRFC 27: Blockchain Links (Blinks)",
            "url": "https://forum.solana.com/t/srfc-27-blockchain-links-blinks/1721",
            "created_at": "2024-06-25T13:13:22.158Z",
            "posts_count": 1,
            "views": 354,
            "reply_count": 0,
            "last_posted_at": "2024-06-25T13:13:22.208Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Blockchain links (”blinks”, for short) are fully-qualified URLs intended to expose Solana Actions to numerous clients. Blinks come in a few forms:\nactions.json published at the origin of a domain\nThis file acts as a configuration file to allow remapping of human-readable, domain-specific URLs to alternative URLs for resolving Actions.\nURL-encoded Solana Actions endpoint included in the ?action= query parameter prefixed with solana-action:\nA URL with the solana-action: URI scheme\nRationale\nBlockchain applications suffer from many user experience issues. In order for an application to prompt the user to sign and send a transaction via a browser wallet extension, the application needs to be wallet-aware, limiting the surface area of possible blockchain-related use-cases across the internet to only a handful of websites.\nBlinks provide an opportunity to “break out” of that requirement by establishing a consistent standard for surfacing Solana Actions for direct invocation without having to leave the website. This can happen because browser extension wallets have full visibility of everything on the page, including blinks. In conjunction with the Solana Actions associated with the blink, the wallet is able to derive enough information to construct the transaction, simulate the transaction’s behavior, and present it to be signed and sent by the user, all without the originating website knowing anything about blockchain, or anything about Solana. Ultimately, the Solana blockchain is the always the operational source of truth, and the need for rigorous authentication schemes (i.e., click on link, login, recommit to the transaction) are pushed to the cryptographically-simple signature verification of the Solana protocol itself, making typically complex transactions like e-commerce as easy as signing and sending a transaction.\nPut in layperson’s terms, blinks allows any website on the Internet that can display a URL to potentially be the start of a Solana transaction.\nAdditionally, given the richness of metadata (including OpenGraph) returned by blinks, other clients like Discord, Telegram, or even iMessage may be able to provide rich, interactive wallet-aware experiences without the native platform knowing anything about Solana.\nSafety and Security\nBlinks are associated to one or many Solana Actions, and each Solana Action lives at a domain that must be trusted by the user in some form or capacity; the Solana transaction itself is not embedded without context into the URL. The transaction returned by the Action API must still proceed through simulation, and still requires the user to sign and send in order to be “successful”, hence Actions surfaced via blinks have similar trust assumptions as going directly to the dApp, connecting to wallet, and subsequently signing / sending the transaction.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "jnwng",
            "activity": "2024-06-25T13:13:22.208Z"
        },
        {
            "id": 1720,
            "title": "sRFC 26: Multi-Actions (Solana Actions v2)",
            "url": "https://forum.solana.com/t/srfc-26-multi-actions-solana-actions-v2/1720",
            "created_at": "2024-06-25T13:12:48.597Z",
            "posts_count": 1,
            "views": 150,
            "reply_count": 0,
            "last_posted_at": "2024-06-25T13:12:48.651Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Authors: nick@dialect.to, alexey@dialect.to\nThe Multi-Action Specification (MAS) is a extension of sRFC 25: Solana Actions and retains interoperability (but not strict backwards-compatibility). See detailed specification here.\nTwo notable differences:\nThe metadata returned from GET https://example-action-endpoint.xyz may include more than just label and icon, and may include more informational data (e.g., title, description) as well as a set of up to four linked Action endpoints.\nLinked Actions introduces the concept of an input with dynamic input. An Action with dynamic input cannot be the first Action as backwards-compatibility with the v1 Actions Spec does not allow input. Actions with dynamic input receive additional parameters in the body of the POST request alongside the account public key.\nAll linked Actions themselves respect the v1 Solana Actions specification, and can individually respond to the GET / POST flow, with the exception of Actions with dynamic input.\nThe transaction returned from POST https://example-action-endpoint.xyz will always be the first of the set of linked Actions from the metadata retrieval step.\nRationale\nAllowing the return of additional metadata + multiple Actions allows for more expressive, Action-driven interfaces to be built.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "jnwng",
            "activity": "2024-06-25T13:12:48.651Z"
        },
        {
            "id": 1719,
            "title": "sRFC 25: Solana Actions v1",
            "url": "https://forum.solana.com/t/srfc-25-solana-actions-v1/1719",
            "created_at": "2024-06-25T13:05:58.697Z",
            "posts_count": 1,
            "views": 158,
            "reply_count": 0,
            "last_posted_at": "2024-06-25T13:05:58.755Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "v1 of the Solana Actions API specification is identical to the Solana Pay transaction request specification, with one key differentiator. Instead of the solana: URI scheme, solana-action: is used to provide an opportunity for clients to treat Actions differently that existing payments schemes related to Solana Pay.\nIn part, this is due to a common failure case related to QR codes on iOS devices. The use of solana: was ambiguously used by various applications that lacked support for Solana Pay, resulting in native camera scans prompting the opening of apps with no action item. As a result, on iOS devices, the oldest installed application that has registered this URI scheme will handle the Action by default, and we expect a higher success rate since the use of solana-action: is much more deliberate.\nThis does not affect Android as the OS lets the user select the app that would handle that URI scheme.\nThe use of Universal Links was dismissed as this targets a specific URL and application, with no opportunity for other apps to equivalently serve that request.\nsolana: can be used as a fallback scheme; Actions-aware clients will support both schemes\nSimilarly, wallets like Phantom / Solflare / Backpack / TipLink made assumptions that URIs of this form would strictly be payments-related, and the resultant UX flow might be confusing for generalized use-cases unrelated to payments.\nThe use of APIs in conjunction with Solana programs provides the ability to express rich combinations of offchain / onchain business logic, reducing time to market and the use of the right technology for any given purpose.\nRationale\nSolana Actions builds on the power of Solana Pay transaction request, but with an explicit reframing of the premise to encompass use-cases well-beyond payments. “Solana Actions” captures the wide-swath of diverse products built on the Solana blockchain, including (but not limited to) governance, games, and digital assets.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "jnwng",
            "activity": "2024-06-25T13:05:58.755Z"
        },
        {
            "id": 1681,
            "title": "sRFC 0024: Extending off-chain message signing standard",
            "url": "https://forum.solana.com/t/srfc-0024-extending-off-chain-message-signing-standard/1681",
            "created_at": "2024-06-14T18:29:34.642Z",
            "posts_count": 1,
            "views": 396,
            "reply_count": 0,
            "last_posted_at": "2024-06-14T18:29:34.786Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "sRFC 0024: Extending off-chain message signing standard\nSummary\nOff-chain message signing (OCMS) is a standard used by software wallets, hardware wallets and protocols to sign arbitrary text messages. These messages have a variety of use cases that all revolve around the same workflow - proving wallet ownership without requiring the user to sign and submit a transaction that pays gas.\nCurrently, OCMS lacks line break support in its most simple and widely used encoding. This sRFC proposes changes to the standard to add line breaks to allow better formatted messages from apps to improve overall UX.\nReference\nCurrent OCMS standard version 0 is being used as a baseline for this sRFC.\nThe new standard version should emerge from forking the current reference and implementing the changes bellow.\nChanges\nChange the header version from 0 to 1.\nFrom:\nOnly the version 0 header format is specified in this document\nTo:\nOnly the version 1 header format is specified in this document\nChange the definition of “Restricted ASCII” to include the ‘\\n’ (0x0a) character in version 1.\nFrom:\n** Those characters for which isprint(3) returns true. That is, 0x20..=0x7e .\nTo:\n** Those characters for which isprint(3) returns true or the newline - \\n character. That is, 0x20..=0x7e or 0x0a.\nExamples:\nIn the examples bellow assume that all message encodings are set to 0 - Restricted ASCII, only header version changes\nThe following message works across both versions:\nA sample message with no newline character.\nThe following message fails when the header version is 0 but is to be accepted with the newly specified version 1:\nA sample message with line breaks.\nThis message allows apps to add line breaks and to better format to their messages.\nCompatibility\nAdding support to the new version is trivial, just checking the updated header version and the extra newline character on messages.\nIn order to be backwards compatible with the previous version for software that needs to parse and handle these messages, implementers just need to be slightly lax on header version checks. Instead of forcing the header_version == 0 one can check if header_version &lt;=1.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "qtmoses",
            "activity": "2024-06-14T18:29:34.786Z"
        },
        {
            "id": 370,
            "title": "sRFC 00020: RWA/Security Token Standard",
            "url": "https://forum.solana.com/t/srfc-00020-rwa-security-token-standard/370",
            "created_at": "2023-07-12T15:01:09.312Z",
            "posts_count": 15,
            "views": 4130,
            "reply_count": 6,
            "last_posted_at": "2024-04-30T05:46:11.986Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Summary\nThis sRFC proposes the development of an open-source Real World Asset (RWA) Security Token standard, specifically focused on real estate, but applicable to many other applications, to foster wider adoption of these assets on Solana. The establishment of a standardized framework for tokenizing real-world assets (“RWA”) holds the potential to significantly enhance transparency, accessibility, and efficiency in real estate transactions. This development would bring about substantial benefits for consumers and institutions involved in building and utilizing products within the DeFi space, ultimately contributing to the onboarding of the next 100 million users to the Solana ecosystem.\nIntroduction\nReal-world assets (RWAs) are physical or tangible assets that have value and exist in the real world. Examples of RWAs include real estate, artwork, commodities, vehicles, etc. These assets are typically illiquid due to their physical nature, and their ownership is governed by legal documents. However, blockchain technology presents a unique opportunity to digitize or “tokenize” these assets, making them easily tradable and accessible to a global audience.\nTokenization of RWAs, in the context of real estate, involves converting the rights to an asset into a digital token on a blockchain. Real estate tokenization could bring about a fundamental transformation in the way real estate is owned, traded, and financed. However, the lack of a unified and widely accepted standard for real estate tokenization in the Solana ecosystem is a major impediment to this transformation. This proposal seeks to address this issue by establishing an open-source, ecosystem-endorsed standard for tokenizing real estate on Solana.\nThe Problem\nAs it stands today, the Solana ecosystem lacks a unified, widely-accepted standard for real estate tokenization. While Ethereum hosts numerous token standards that apply to real world assets, Solana predominantly relies on Metaplex’s Token Metadata Standard which was created for broad use NFTs. While Metaplex’s standard is great for general NFTs, it lacks crucial features required to allow for the mass adoption of real world assets like escrow facilities, token freeze/nullification, and a standard approach to property data input.\nProposed Solution\nCreate an open-source, ecosystem-endorsed standard for tokenizing real estate on Solana. This Real World Asset (RWA) standard aims to not only simplify the tokenization process for new entrants but also get broader adoption from existing real estate developers, brokerages, institutions. The development of this standard would thus serve as a public good, spurring growth in Solana’s RWA landscape.\nFunctional Requirements of the RWA Standard\nCommoditized RWA Standard\nTitle Ownership: Ability to verify the legal ownership of the property by providing verified documentation or parsing via a third party data oracle.\nTitle Status: Ability to confirm whether the title is clean or dirty depending on liens (legal claims or holds on a property, either by financial institutions or other parties, to secure the repayment of a debt).\nGeneral Property Info Update (Updated via trusted RE Data Oracles)\nAddress Update: Ability to update the property’s address.\nSquare Footage Update: Ability to update the property’s square footage.\nHouse alteration / major renovations\nSecuritized RWA Standard\nIssuer Transaction Clawback: Ability for the issuer to clawback or reverse transactions if necessary.\nTrading Halt Functionality: Ability for the issuer to halt trading in certain circumstances.\nIssuance Whitelist: Ability to maintain and update a whitelist for issuance, controlling who can issue the tokens.\nSecondary Trading Whitelist: Ability to maintain and update a whitelist for secondary trading, controlling who can trade the tokens after issuance.\nEscrow Mechanism: Ability to implement an escrow mechanism, providing a secure way to hold funds or assets until specified conditions are met.\nRegulatory Filings Verification: Ability for the public to verify that regulatory filings have been completed, ensuring regulatory compliance.\nDividend Distribution: Ability for the issuer to distribute dividends (such as rental income) to token holders.\nDividend Distribution Schedule Updates: Ability for the issuer to specify and update the schedule for dividend distribution.\nPotential Drawbacks\nDespite the potential advantages, a few potential drawbacks could arise from this proposal.\nRegulatory Complexity: Due to the legal and financial nature of RWAs, particularly real estate, different jurisdictions may necessitate varying levels of regulatory compliance, potentially complicating the adoption process.\nOracle Dependency: The proposed standard would rely on trusted data oracles for real-time information updates, any disruption to these services could potentially impact the overall system’s operation.\nAdoption Challenges: Though the standard would simplify the process of tokenization, achieving wide-scale adoption might be challenging, particularly from traditional real estate players unfamiliar with blockchain technology.\nPrivacy Concerns: Handling sensitive property information on-chain could raise privacy concerns. We need to ensure the appropriate safeguards to prevent unauthorized access and maintain privacy.\nConclusion\nThe tokenization of real-world assets such as real estate holds the key to unlocking untapped value by democratizing access, enhancing liquidity, and improving efficiency. This proposal aims to provide a robust framework that will help fuel the growth of the RWA landscape within Solana, bringing us a step closer to realizing the vision of truly decentralized finance. The open questions mentioned are essential for the community to resolve together to ensure that the proposed standard is both practical and compliant with regulatory standards.\nOpen Questions\nAll of the above is open to comments. Some specific questions to Resolve with 3rd Party Partners are listed below:\nShould the standard support multiple US Security Regulations from the get go?\nHow should title claims be handled?\nHow are claims tracked?\nHow are claims proven?\nWhat data should be uploaded for title claims? (title reports)",
            "comments": "[joebuild]: Very cool. No strong takes overall, but having worked at a real estate data startup, you will quickly find that the fields in the ‘Commoditized RWA Standard’ as you have them are insufficient (there will be hundreds of fields worth tracking). Would recommend a flexible approach to fields there.\n\n[Whisky]: This is a great idea and a FANTASTIC start to solidify Solana as the ideal chain for RWA solutions. I agree the Metaplex standard does not sufficiently account for the unique nature of RWA’s and their specific requirements in a token.\nThe reliance on oracles is always a concern, particularly if they rely on government recognition of any of the documents. Do you propose that documents be uploaded to arweave and stored on-chain with a link in the metadata to the title? Would the plan be to eventually incorporate governmental oversight into the transactions, and allow for government offices to sign on-chain transactions and transfers?\nWallet security. The revocable nature of the token is beneficial in that if someone is hacked the original token can be voided and re-issued, however, this also gives power to individual bad actors to make changes to the title that would normally require governmental recognition. How would one determine whether someone was hacked and lost their tokens or if they actually completed an off-chain transaction and transferred the token to the new owner, and attempted to claim it back? If the solution is a multi-sig, who are the signers?\nKYC: If allowing for certain forms of financial actions or distributions would the token issuer be responsible for completing KYC? Would the tokens require permissioned transfers? How does one prevent potential violations of US Treasury sanctions or laws without a permissioned system?\nThe UCC also becomes particularly relevant here with regard to understanding one’s rights in the event of a default of an RWA in which the token represents the legal title. Can defaults trigger instant title transfers on chain in the event of a default? What would be physically necessary for the government to recognize those claims?\nThese are some of my initial thoughts off the cuff but I have been working on thinking through this issue for some time and would love to keep the conversation going here.\n\n[yashhsm]: Screenshot 2023-03-12 at 8.24.50 PM (1)1774×1004 60.1 KB\nThanks for initiating this discussion - much needed. Great points, but I would argue that a generic RWA standard like Token 2022 would make more sense, as it would allow for greater configuration flexibility. For a specific use-case like Real Estate, projects can build low/no-code solutions on top of these standards to enable even traditional firms to tokenize their assets.\nP.S. I was also working on designing an RWA token standard - essentially Permissioned Tokens (including the flowchart) - and would love to hear community comments.\n\n[Basile]: I do believe that at the core of this standard permissioned tokens are a requirement, but I slightly disagree that an RWA Token Standard is essentially Permissioned Tokens. My thoughts are more along the lines of a “configurable” permissioned token that would allow the issuer to specify whether or not this token will be treated as a commodity or security and implement their own respective logic. The distinction between the two is important especially in the case of institutional adoption (funds would trade as securities). Depending on the “RWA Type” the issuer would be responsible for implementing certain functions such as a “Title Verifier”/“Legal Verifier” required by the Standard.\nEx. You want to tokenize your car: Create an RWA token of type Commodity. Implement the “Title Verifier” function (and whatever else might be required by the Standard) and you’re good to go.\nWould love to hear your thoughts on this!\n\n[Odai]: Great input and deep need for this in the ecosystem.\nSome thoughts:\nClaims should be tracked &amp; proven by providing PDF or link to county’s recorder office. A PDF should suffice of the title report and/or deed.\nRegarding clawbacks, I think adding a freeze element would also be beneficial in the case where manual verification is needed.\nHow would you handle different regulatory requirements for a single offering? (Accredited Investor, Non-acccredited, Non-US) Specifically for different transfer restrictions for each? How are you verifying the end-buyer is confirming / acknowledging their acceptance of a restricted security token?\nAppreciate the pioneering efforts to bring RWA’s to Solana!\nProps &amp; Love to the Homebase Team\nOdai - LiquidProp\n\n[JoakimEQ]: Hey Dom, thanks for the well thought-out post. This is a great opener for what I think will be the killer app of Solana DeFi. RWA’s need fast chains. They are currently severly hampered by slog that is the EVM.\nThat being said, there are a few areas in your proposal that I’d like to comment on, most of which have also been pointed out by other members of this forum:\nReal-Estate Focus - The language in the post appears to be largely tailored to Real Estate and might not fully encapsulate the wide range of possibilities that RWAs present. As we’ve observed, tokenized funds have gained considerable traction and could be the current primary market fit for RWAs on the blockchain.\nUSA Focus - The proposal seems to focus heavily on the US jurisdiction, an environment we know is susceptible to frequent and substantial regulatory changes. Rather, I suggest we consider areas where blockchain standards already have governmental support, like the EU.\nUsage of Oracles - I recognize the need for updatable tokens/NFTs, but this brings up important security considerations that might pose obstacles to the broader adoption of the standard.\nRegarding your closing queries:\nI agree that we should take into account various regulations and perhaps work towards a flexible, jurisdiction-agnostic “category-based” model.\nTitle claims seem simple to track, at least based on my tech knowledge and talking with the people behind ERC-6065 (the EVM RWA standard). As each property would be a unique token, the tracking is inherent to the chain.\nOwnership of a unique token could indeed be used as proof of ownership for the underlying asset.\nWhile uploading data for title claims poses challenges, I believe that current developments in decentralized storage solutions, such as Filecoin and Arweave, show promise in addressing this issue.\nI would be thrilled to contribute further to these discussions. We have extensive experience in developing a MiCA-compliant stablecoin, a general EU-based tokenization framework, and have contributed to some Solana-based projects in the past. Additionally, I am also deep in the RWA scene on Ethereum, and have been following and discussing the ERC-6065 spec for some while now.\nPlease feel free to reach out to me at joakim(at)equilibrium.co or via Telegram at @jommi to continue this conversation. Would love to collaborate!\n\n[Dom]: Thanks for the feedback Joebuild!\nI started this post with a focus on real estate, but after multiple discussions with other teams, it’s clear we need a more general RWA standard that can then be customized for different verticals (trading cards, real estate, commodities, etc)\nCompletely aligned that for the specific real estate vertical, there needs to be flexible approach to field input\n\n[Dom]: Thanks for the feedback Odai!\nAgreed that claims should be tracked and proven either via PDF (ideally a document with some level of “official” stamp) or link to county recorder’s office\nFreeze element would be crucial, especially to comply with US securities regulation\nWith the assumption that the RWA standard would just be the framework for how to tokenize physical assets, depending on the jurisdiction, a regulatory wrapper would be needed specific to the country that you’re issuing the token in. In the case of the US wrapper, the issuer would still need to make sure they’re complying with US securities regulation when issuing the token (whether you sell to accredited investors only, non-accredited, etc). The RWA Standard would then ensure that you to have the right tools at your disposal to comply with different US securities regulation (For example: In the case of a Reg D filing, the tokens would automatically be frozen for 1 year after initial sale, etc). In addition, after the sale, the RWA Standard should ensure that the security tokens comply with Rule 144 (US specific).\nIn terms of verifying that the end-buyer is confirming / acknowledging their acceptance of a restricted security token, it’s likely that trading of security tokens (at least in the US) is primarily done on a permissioned DEX or CEX. The reason is that some entity needs to perform KYC on each user to comply with US regulation, and likely needs an alternative trading system license (ATS) to allow for P2P trading.\nIf you have any other thoughts on this, or disagree with any of my statements, would love the feedback! Very much thinking out loud here based on how I believe it’d work based on multiple discussions with lawyers, other teams, etc.\n\n[Basile]: Hey Whisky, thanks for being a part of the discussion!\nI agree that reliance on oracles would be a problem. In the initial state of the standard I envision document upload (title/deed + SEC docs if securities) to be done in a similar manner to NFT data. This would mean that issuers are responsible for uploading the correct documentation. In an ideal world we have some sort of Oracle provided by government or with government oversight that allows us to undeniably verify the validity of the uploaded documents. I would love more discussion around this piece!\nWallet security – this is an interesting one. It’s important to note that ideally the token truly represents legal title/ownership. Unfortunately there may be no way to universally guarantee that every token issuer has done the proper legal filing to support that without the involvement of governments/other third parties. To me, this means that it is very likely a decision that the issuer of the token will make as opposed to the standard per say. Security tokens require, legally, the capabilities for tokens to be voided and re-issued. How and when you do that is completely up to the issuer (ideally the issuer has a process in place for how they identify the legitimacy of a hack).\nKYC: At it’s core the standard would be a Permission Token. If issuing the token as a security, issuers would have the ability to whitelist holders upon completion of KYC through their own third-parties. @yashhsm shared a flowchart below that showcases this! This would dictate who can perform what financial actions such as transfers, receipt of dividends, etc…\nWith regards to UCC, in an ideal state, yes defaults should trigger instant title transfers. Especially if loans are made on-chain. However, absent government adoption, this is likely not possible in countries that wouldn’t enforce this title transfer legally.\nMy takeaway from several discussions is that the standard itself likely cannot make guarantees on the validity of the documents uploaded without the involvement of government bodies (ex. SEC allowing us to verify the validity of security filings on-chain for a certain RWA token). These processes also vary from locale to locale, therefore making it difficult to build a standard that satisfies everyone, everywhere. What we can do is create a standard that defines a structured set of data requirements (documents to upload, parameters to set, etc…) and functional requirements that are required amongst most/all locales and leave it to issuers to build systems on top of that. Hopefully as time goes on that standard evolves and gets support from government bodies to achieve that aforementioned ideal world scenario. In short, I think it’s important that the standard remains robust and quite open to implementation by the issuer, so that it can be used in various locales/jurisdictions.\n\n[Sherwood]: Thanks, Dom!\nAgree with @yashhsm above - we should also be looking to create a more universally programmable security token asset/standard that many different types of ST/RWAs can be tokenized with (equity, debt, rev shares, etc.). The tracking of deed ownerships and even geodata associated with a property is interesting AND I don’t know that there is a market asking for this…\nToken 22 was a good start and an even more adept ‘transfer restricted token standard’ or ‘security token’ standard can address RE RWAs and a broader spectrum of assets.\nOn NFTs, I have experience in security non-fungible token (sNFT) and while they’re novel for delivering financial upside to investors based on more origination (the creator) and copy/IP rights, they are far less adept than fungible assets when wanting to build (1) deep and high volume secondary markets and (2) composable finance. You can still have things like dividends, royalties and profit-taking distributions track against fungible token supplies all the same.\nSo, if we’re wanting a non-fungible RE specifically, this can be fine. I am unsure of the market size or problem being solved with such an asset standard. If we’re wanting to deliver a security token standard that is broadly configurable can create composable financial markets a la ERC-1404, I recommend going towards… that. This would open up far more developer contribution and value generation on Solana.\n\n[roinevirta]: Hello Dom,\nThank you for initiating the discussion. I strongly echo @JoakimEQ’s sentiment; a flexible, high-level data model with optional fields would likely serve issuers &amp; other counterparties better globally.\nMembrane Finance builds technologies to facilitate hybrid financial markets. We are currently bringing a MiCA-compliant stablecoin to Solana and we would be happy to contribute to the development of the new standard through our learnings and experience.\nPlease feel free to reach out to me at juuso.roinevirta(at)membrane(dot)fi or via Telegram at @roinevirta – we would be happy to collaborate and ensure global compatibility.\n\n[ekryski]: @Sherwood great commentary!\n Sherwood:\nthey are far less adept than fungible assets when wanting to build (1) deep and high volume secondary markets and (2) composable finance\nGenuine question, what makes you say this?\nIMHO this is not necessarily true, but more a function of how the DeFi markets have developed thus far. Specifically, that all systems have historically been built to simply pool assets because it’s “how things are done”. I’ll admit it is certainly easier and often also cheaper for compute, but I think primarily this is a result of historical implementation baggage that originated with Ethereum and have been perpetuated over time.\nSomewhat recently, from many of the more established players on Ethereum we’re seeing an evolution where platforms are creating more finely grained “pools” or other work arounds to solve for compliance and risk mitigation needs. Curve, Uniswap, Aave (and more) have all been taking these approaches which imho could be better solved by either ERC-1155 or ERC-1404 (or some improved smart contract designs around ERC-20 and ERC-721). I’ve long felt this way and did some early implementations 4-5 years ago but only recently started exploring this again more earnestly.\nCurious if I’m missing something. Happy to take to DMs to prevent polluting here.\n\n[Sherwood]: Love this thinking and feedback. When we think about 1155, that can be a great way to conceptualize indexed fungible assets associated with one originated (using securities language) asset.\nI think that you can absolutely go down these routes and, bc you want to develop markets* MORE than wanting to create just more assets* and fungibility is key to this. Making the standard more fungible-capable open at first would just be a simpler place to start and would accomplish the markets creation.\n\n[Cutler]: Hi Dom,\nThis is great stuff. I am a corporate/governance lawyer from Australia, working a digital legal contract management solution with self-building company asset registers. I think this could be the link for crypto RWA’s to find retail mass market adoption (i.e., an app that sits above the blockchain layer with an easy and familiar enterprise user interface). I have been looking for an RWA project that fits with this and what you are describing is perfect. Can we connect somehow to discuss working on minting RWA tokens for this platform as transactions flow through it?.\nI am new to this forum so not sure how to go about this.\nAny thoughts (anyone?)\n\n",
            "comment_count": 14,
            "original_poster": "Dom",
            "activity": "2024-04-30T05:46:11.986Z"
        },
        {
            "id": 1245,
            "title": "sRFC 22 - Extending support for mobile wallets in the Wallet Adapter",
            "url": "https://forum.solana.com/t/srfc-22-extending-support-for-mobile-wallets-in-the-wallet-adapter/1245",
            "created_at": "2024-03-21T14:26:53.036Z",
            "posts_count": 1,
            "views": 457,
            "reply_count": 0,
            "last_posted_at": "2024-03-21T14:26:53.152Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Mobile Wallet Support - Wallet Adapter\nSummary\nCurrently, the adapter’s support for wallet connection support is sub-optimal. Users that start a session within a mobile browser will lose session context when asked to connect their wallet. Existing wallets utilise deep-linking within the adapter to route the user to their wallets.\nThis however does not work all the time - there are instances whereby the user is re-directed to the app-store. On success, the user will lose session context most of the time and have to re-start their session within the in-wallet browser functionality thus leading to a poor experience for mobile users.\nTo solve for this, we propose to add a standardised interface within the wallet adapter that will handle mobile wallet connections utilising a third party to facilitate adapter to mobile wallet adapter communications in a similar fashion to how WalletConnect facilitates this on EVM based dApps.\nImplementation:\nThis is a simple reference point of how this flow would look like, curious to get thoughts from wallets / dApps on what they think about this.\nAt a high-level, the interface would extend the existing connect functionality such that if the user is selecting a wallet and they are on mobile, the third party service provider would facilitate the mobile wallet to adapter communication.\nAll communication would pass through the third party on mobile and act as a relayer between the dApp and the mobile wallet.\nHere is a sample diagram of how I imagine it’d look like:\n \n \n Whimsical\n \n \n \nWallet Standard - Extended\n \nWhimsical combines whiteboards and docs in an all-in-one collaboration hub.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "ZeroTimeDrift",
            "activity": "2024-03-21T14:26:53.152Z"
        },
        {
            "id": 1027,
            "title": "Update Base Fees",
            "url": "https://forum.solana.com/t/update-base-fees/1027",
            "created_at": "2024-02-07T09:41:21.307Z",
            "posts_count": 1,
            "views": 460,
            "reply_count": 0,
            "last_posted_at": "2024-02-07T09:41:21.383Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Title: Updated Base Fees\nSummary\nI had written an article about base fees and how the base fees should look depending on the cluster size.\n \n \n Notion\n \n \n \nNotion – The all-in-one workspace for your notes, tasks, wikis, and databases.\n \nA new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team\n \n \n \n \n \n \nI want to hear from the community about their opinion. There are multiple approaches to change the base fees.\n → Constant base fees\n → Linear base fees\n → Brackets based base fees\n → Exponential base fees\nI will explain the pros and cons of each in a few words.\nWhy change base fees ?\nCurrently, base fees are constant 5000 lamports / signature. So cost of executing transaction on the cluster is measured in SOL than in USDC. If SOL becomes more expensive, then it will be more expensive to execute transactions for the user, while if SOL becomes less expensive, then it will be harder for the validator to keep running the cluster.\nThe base fee is constant and is not based on the block space used by the transaction. So user pays the same base fee if they execute transactions consuming 10K CUs, 100K CUs or a million CUs. Base fees should consider the resources used to execute the transaction on the cluster. This makes everyone send transactions requesting the maximum CU possible of 1.4 million CU. If transactions are not correctly estimated for consumed CU, the scheduler finds it difficult to schedule the transactions and fill the block completely, wasting some of the block space.\nBy changing the base fees based on CU requests, we can calculate the resource usage in CU appropriately and charge the user based on resource utilization.\nWho will control the base fees\nThe base fees could be controlled by DAO or Multisig, giving them control over one or multiple variables. Various communities, validators, and developers can be part of the DAO. The current proposition does not consider this part, as it could be figured out later.\nRequirement of new base fees\nShould be based on resources used by the transaction.\nOptionally discourage spamming.\nOptionally encourage optimization of code by the programs.\nShould be in interest of “most” of the community. (i.e should not be too expensive for user nor punitive to run a validator)\nIn general we have to find a middle ground to encourage dapp devs to write optimized programs, for users to send transactions with better estimates for resource use at the same time not discourage existing users to use Solana. It should also encourage validators to pack the block the fullest i.e consuming 48 M CU.\nImpact\nThis proposal Validators, Dapps and everyone who uses Solana cluster to send and execute transactions.\nNew Base Fees\nConstant base fee\nFormula:\nBase Fee = constant value\nPros:\nEasy to calculate fee for transaction.\nAlready implemented just need to update the constant value.\nCons:\nDoes not discourage spamming of network.\nDoes not discourage large transactions consuming lot of CUs.\nDoes not encourage optimization of the programs.\nDoes not represent resource utilization.\nLinear base fee\nIn this model base fee will be linearly dependent on the CU requested with a minimum floor value of the base fees. The rate at which base fee will increase will be called rate of change of base fees or just Rate.\nFormula:\nBase Fee = round(max( Minimum Base fee value, Rate * CU requested ))\nPros:\nRelatively easy to calculate.\nEncourages optimization of programs.\nEncourages sending transactions with proper CU estimates.\nRate can be controlled by DAO or MultiSig.\nRate can be changed according to SOL/USD oracle price.\nDepending on MinValue, can incentivize validator to prioritize smaller transactions.\nCons:\nHard to find correct Rate which will please everyone.\nLower value of rate will not discourage sending of large transactions nor encourage program optimization.\nHigher value of rate may discourage users away from solana.\nBracketed base fee\nFormula:\nBase Fee = (0 to bracket_1) * rate 1 + (bracket_1 to bracket_2) * rate 2 …..\nPros:\nRelatively easy to calculate.\nEncourages optimization of programs.\nEncourages sending transactions with proper CU estimates.\nCan disincentives sending of large transactions.\nCons:\nAny multisig or DAO will have multiple parameters to set.\nCan have some weird behaviors.\nMay need some different burning rates for different rates.\nExponential base fee\nConsider a multiplier multiplier that has to applied to a certain CU CU multiplier (i.e we want transaction requesting 1 million CU pay 5 times the minimum value then multiplier is 5 and CU multiplier is 1 million).\nFormula:\nBase Fee = round( Minimum Value * multiplier ^ (CU requested / CU multiplier) )\nPros:\nEncourages optimization of programs.\nEncourages sending transactions with proper CU estimates.\nRate can be controlled by DAO or MultiSig with 3 params.\nRate can be changed according to SOL/USD oracle price.\nWill disincentivize spamming and heavy transactions.\nIncentivizes sending of smaller transactions.\nCons:\nRelatively hard to calculate.\nWill make users to split transactions doing multiple instructions into multiple transactions.\nNeed to update burn rate so that validators do prefer executing heavy transactions.\nRelatively hard to get consenseus over the variables.\nAs a community I would like to have your opinion .",
            "comments": "",
            "comment_count": 0,
            "original_poster": "gmgalactus",
            "activity": "2024-02-07T09:41:21.383Z"
        },
        {
            "id": 949,
            "title": "sRFC 21 - Nested Account Resolution",
            "url": "https://forum.solana.com/t/srfc-21-nested-account-resolution/949",
            "created_at": "2024-01-15T12:31:12.154Z",
            "posts_count": 1,
            "views": 585,
            "reply_count": 0,
            "last_posted_at": "2024-01-15T12:31:12.231Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Code: GitHub - ngundotra/srfc-21-nested-account-resolution\nSRFC 21 - Nested Account Resolution\nSummary\nThis sRFC introduces a standard protocol to enable on-chain and off-chain derivation of accounts needed to complete a transaction, allowing the definition of account-free program interfaces.\nMotivation\nComposing programs on Solana is hard because it is difficult to imagine being able to swap out any program for any other program. So program developers only call out to one program at a time. This process leads to hard-coded composition paths.\nThis is one of the most frustrating aspects of Solana programming. This specification aims to solve this by providing a base for community members to decide on program interfaces.\nBackground\nDefining program interfaces requires specifying accounts and instruction data. Interfaces with a strict list of accounts will restrict program implementation, because programs may be required to put all their information into a few accounts that fit the interface description. Interfaces defined with a strict list of accounts restricts innovation in program development, and fails to take advantage of the SVM’s composability.\nWe want to enable interfaces to specify required accounts without restricting additional account usage. We believe this will allow exciting developments of innovation with program interfaces.\nTwo problems with this\nHow can programs specify additional accounts they need for a specific instruction?\nIf a program is CPI-ing into a unknown program that requests additional accounts, how can it expose those requested accounts, and then reliably pass such accounts to the unknown program?\nSpecification: Nested Account Resolution\nWe propose to solve these two problems by allowing programs to request additional accounts for an instruction with an additional instruction that requests accounts using return data.\nOff-chain clients can simulate the additional instruction, parse the return data, and then construct a valid transaction for the program’s instruction using the requested additional accounts.\nAs of right now, this specification requires 3 things:\nProgram must have an Anchor IDL deployed on-chain\nInstructions that request additional accounts must have 8-byte instruction discriminator derived from their name and a corresponding entry in the program’s anchor IDL.\nThe additional instruction that requests additional accounts must also have an 8-byte instruction discriminator derived from the target instructions name, and a corresponding entry in the program’s anchor IDL.\nThese additional instructions are called aar instructions, e.g. additionalAccountsRequest, or additionalAccountResolution.\nLet’s go through how these are meant to be used before diving in any deeper. Our examples will focus on programs built with the anchor framework, since it meant to improve program composability and transparency on Solana.\nAdditional accounts request instructions, when implemented look like the following\nuse anchor_lang::prelude::*;\nuse additional_accounts_request::AdditionalAccounts;\nuse solana_program::program::set_return_data;\n#[program]\npub mod my_program {\n use super::*;\n pub fn transfer(ctx: Context&lt;Transfer&gt;, args: TransferArgs) -&gt; Result&lt;()&gt; {\n ...\n }\n pub fn aar_transfer(ctx: Context&lt;TransferReadonly&gt;, args: TransferArgs) -&gt; Result&lt;()&gt; {\n let mut requested_accounts = AdditionalAccounts::new();\n ...\n requested_accounts.add_account(&amp;pubkey, is_writable)?;\n ...\n set_return_data(bytemuck::bytes_of(&amp;additional_accounts));\n Ok(())\n }\n}\nOff-chain clients can compose a valid TransactionInstruction for transfer by simulating aar_transfer with TransferReadonly’s accounts successively until requested_accounts.has_more is false. Each successive simulation of aar_transfer must include the accounts requested in the previous simulation. This is to allow for account derivation logic that requires account data, which is only available in simulation when the account is provided.\nThe typescript pseudo code looks like:\n// Create base AAR instruction\nlet instruction: TransactionInstruction = await myProgram\n .methods\n .aarTransfer(args)\n .accounts({\n ...\n}).instruction();\nlet originalKeys: AccountMeta[] = instruction.keys\nlet additionalAccounts: AccountMeta[] = [];\nwhile (hasMore) {\n // Add previously requested accounts to instruction\n instruction.keys = originalKeys.concat(additionalAccounts.flat());\n // Simulate instruction and get return data\n let returnData = await simulateTransactionInstruction(\n connection,\n instruction,\n );\n // Deserialize return data into the Additional Accounts\n let additionalAccountsRequest = AdditionalAccountsRequest.fromBuffer(returnData);\n // Store requested accounts\n hasMore = additionalAccountsRequest.hasMore;\n additionalAccounts = additionalAccounts.concat(additionalAccountsRequest.accounts);\n}\nHere’s the structure of the return data to be set by any additional accounts request method.\npub const MAX_ACCOUNTS: usize = 30;\n#[zero_copy]\n#[derive(Debug, AnchorDeserialize, AnchorSerialize)]\npub struct AdditionalAccounts {\n pub protocol_version: u8,\n pub has_more: u8,\n pub _padding_1: [u8; 2],\n pub num_accounts: u32,\n pub accounts: [Pubkey; MAX_ACCOUNTS],\n pub writable_bits: [u8; MAX_ACCOUNTS],\n pub _padding_2: [u8; 26],\n}\nWe chose AdditionalAccounts to have the maximum number of account meta information possible, while still keeping the total size under 1024 bytes (maximum amount of Solana return data).\nAn additional requirement was keeping the struct byte-aligned so that the entire struct could be used with zero copy to excess heap usage.\nPassing exposed accounts requested by CPIs\nThe hard part here is knowing which of the requested accounts should be used for a given CPI, especially if you have multiple CPIs to unknown programs (e.g. swapping ownership of assets between two programs).\nWe propose a low compute solution that uses an “account delimiter” to group account segments together of variable length. The account delimiter for a program is a PDA with seeds &amp;[\"DELIMITER\".as_ref()].\nAn example swap implementation is given here:\npub fn swap(ctx: Context&lt;Swap&gt;) -&gt; Result&lt;()&gt; {\n ...\n // Track consumed accounts\n let mut delimiter_idx = call(\n ix_name_0,\n cpi_ctx_0,\n args_0,\n get_delimiter(&amp;crate::id()),\n 0\n )?;\n \n // Filter out consumed accounts\n delimiter_idx = call(\n ix_name_1,\n cpi_ctx_1,\n args_1,\n get_delimiter(&amp;crate::id()),\n delimiter_idx,\n )?;\n Ok(())\n}\nExposing accounts requested by CPI\npub fn preflight_swap(ctx: Context&lt;SwapReadonly&gt;) -&gt; Result&lt;()&gt; {\n # find number of account delimiters\n let mut latest_delimiter_idx = 0;\n let mut stage = 0;\n ctx.remaining_accounts\n .iter()\n .enumerate()\n .for_each(|(i, acc)| {\n if acc.key() == delimiter {\n stage += 1;\n latest_delimiter_idx = i;\n }\n });\n # Based off # of delimiters, I can derive how many CPIs have finished requested account\n match stage {\n 0 =&gt; {\n # CPI to `aar` instruction for unknown program A\n # pass all undelimited accounts to program A\n if done { \n addtional_accounts.add_account(&amp;get_delimiter(&amp;crate::id()), false)?; \n }\n }\n 1 =&gt; {\n # CPI to `aar` instruction for unknown program B\n # pass all undelimited accounts to program B\n }\n _ =&gt; {\n msg!(\"Too many delimiters passed\");\n Err(ProgramError::InvalidInstructionData.into())\n }\n }\n}\nLimitations\nThis spec does not support requesting additional signer accounts.\nAllowing unknown programs to request the signature of accounts could be a security vulnerability, and would be technically challenging to implement. So supporting the signer accounts must come in the form of a new specification.\nResolving additional accounts can be slow\nSuccessively simulating the aar instruction with results of the last call can be quite slow, and there is no maximum number of iterations defined by this specification. There is no current guide or set of heuristics on how to cache requested additional accounts yet either. This means that applications may see increased RPC calls for simulateTransaction and increased latency when showing end-users transactions.\nFuture Work\nAnchor macros to build aar instructions\nIt is possible to design macros that build aar instruction at compile-time entirely from the anchor instruction struct and a list of CPI callsites. We hope that if the community adopts this sRFC, additional developer tooling will be made available here.\nSupport for state compression\nOnce this specification proves valuable, it would be quite possible to support state compression, since the proofs to a ConcurrentMerkleTree only require a list of accounts. However, doing so would require referencing off-chain indexers. This seems best suited for different spec and protocol version of AdditionalAccountsRequest.\nFaster account resolution\nIt seems quite possible to write a thin wrapper around simulateTransaction that uses Geyser to stream account updates to Solana Banks Test, so that transaction simulations are faster and the results can be cached for quicker lookup. This would require a lot of work, but this would probably greatly increase adoption speed.\nCode\nLibrary with helper functions for implementing this spec is available at: docs.rs/additional-accounts-request\nIntegration tests for the library are available at GitHub - ngundotra/srfc-21-nested-account-resolution: Nested account resolution library, PoC, examples, and documentation with yarn install &amp;&amp; yarn test.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "ngundotra",
            "activity": "2024-01-15T12:31:12.231Z"
        },
        {
            "id": 149,
            "title": "sRFC 00011: A smart contract that allows for easy storage and retrieval of data on-chain",
            "url": "https://forum.solana.com/t/srfc-00011-a-smart-contract-that-allows-for-easy-storage-and-retrieval-of-data-on-chain/149",
            "created_at": "2023-04-27T17:03:10.159Z",
            "posts_count": 5,
            "views": 1838,
            "reply_count": 2,
            "last_posted_at": "2023-10-01T17:46:34.192Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "On-chain Data Storage\nSummary\nA smart contract that allows for easy storage and retrieval of data on-chain\nMotivation\nCurrently in Solana there is no standard for data storage on-chain. By data, I mean anything that can be stored as bytes like a text file, a HTML document, a PNG image, NFT JSON Metadata etc. This RFC proposes a solution for storing data on-chain and having it be retrieved easily.\nImplementation\nTo solve this issue, I have developed the Solana Data Program. This is a Solana smart contract that allows users to initialize a Data Account to store any data as bytes and have its metadata be stored in a separate PDA\nFlow Diagram\nHere’s a flow diagram to describe how the accounts are linked:\nAccounts\nMetadata Account:\nIn the Metadata PDA Account, we store the metadata regarding the data in the Data Account. A few important fields stored in it are:\nauthority: The authority needs to be a signer in any instruction that involves the Data Account - updating the data and/or data type, finalizing the data, transferring the authority, and/or closing the account\ndata_type and serialization_status: Currently the supported data types are:\ni. CUSTOM: To store custom data or a currently unsupported data type\nii. IMG: To store image data as raw bytes\niii. HTML: To store a HTML file as raw bytes\niv. JSON: To store minified JSON as raw bytes\nThe motivation behind having set data_types is that it helps the client application determine how to display the data. It also opens doors for data verification (denoted by serialization_status). Currently when JSON data being updated, the user could optionally verify to ensure it is valid JSON.\ndynamic: A dynamic Data Account can realloc (up or down) while a static account will always stay a fixed size. This could be useful when users want to pay a fixed amount for the storage space (static) in the Data Account rather than a only-pay-how-much-is-needed approach (dynamic)\nData Account:\nA Data Account is any Solana account that is owned by the Data Program and has an associated Metadata PDA Account. The data is stored directly as raw bytes so one could easily retrieve it using a single getAccountInfo RPC call as such:\nconst data = (await connection.getAccountInfo(dataKey, commitment)).data;\nThe data account need not be created by the Data Program. You can also pass in a previously created Data Account to the Initialize instruction and it will assign it to the Data Program\nInstructions\nInitializeDataAccount: This instruction creates and initializes the Metadata Account and optionally creates a Data Account.\nUpdateDataAccount: This instruction updates the data_type field in the Metadata Account and the data in the Data Account.\nUpdateDataAccountAuthority: This instruction updates the authority of the Data Account by updating the value in the Metadata Account. It requires both the old and new authority to be signers to prevent accidental transfers.\nFinalizeDataAccount: This instruction finalizes the data in the Data Account by setting the data_status in the Metadata Account to be FINALIZED. Finalized data can no longer be updated.\nCloseDataAccount: This instruction closes the Data Account and the Metadata Account and transfers the lamports to the authority.\nUsage\nSolD Website\nSolD is a website that acts as an editor for the Data Program:\nIt allows users to view the metadata and data associated with a Data Account via the /&lt;datakey&gt;?cluster=&lt;cluster&gt; route\nIt allows users to connect with their wallet and upload files directly to the Data Program by going to the /upload route\nIf the user is logged in as the authority, it allows the user to edit the data, finalize it and/or close the metadata and data accounts.\nUsers can also view all data accounts they “own” via the /authority/&lt;authority&gt; route and perform group actions on them\nTypescript SDK\nsolana-data-program is a Typescript SDK that exposes APIs to interact with the Data Program and helper methods to parse the data, metadata etc. of a Data Account\nNFTs\nOne potential use case of the Data Program are fully on-chain dynamic NFTs. By this I mean an NFT with:\nJSON metadata stored on-chain\nImage data stored on-chain\nJSON metadata that can be updated via an on-chain crank\nImage data that can be updated via an on-chain crank\nHere’s a link to such an NFT: Quine NFT\nOn clicking View original you will see the original HTML file that was pulled from on-chain. You can also inspect the Metadata and Image data on SolD separately:\nImage: HoyEJgwKhQG1TPRB2ziBU9uGziwy4f25kDcnKDNgsCkg\nMetadata JSON: Hb9vkWax5AeLWvCtYSjSvWrN6gTw324gKMa28kcBsgT3\nP.S. The NFT image is a quine. The code on the surface of the rotating sphere is the code used to generate the sphere with code on its surface\nComposability\nAn important consideration went into making sure that the Data Program is easy to use and composable. To demonstrate that, I have also made example smart contracts (two of which involve minting NFTs including the Quine NFT ) that CPI into the Data Program: solana-data-program/examples at main · nvsriram/solana-data-program · GitHub\nURI Standard for Data Retrieval\nCurrently, to have the data be pulled from on-chain I have an API route /data/&lt;dataKey&gt;?cluster=&lt;cluster&gt;&amp;ext=&lt;ext&gt; that just returns the data as is (or in the extension format specified by ext). It would be more handy to have a URI standard which might look something of the sort:\nsol://&lt;datakey&gt;?ext=&lt;ext&gt;\nto get the data stored in datakey in the format specified by ext\nOR\nsol://meta/&lt;datakey&gt;\nto get the metadata associated with the datakey\nConclusion\nThis RFC discusses the features of the Data Program and how it can be used to store data on-chain. It presents the SolD website editor and Typescript SDK that make it easy to interact with the Data Program. It showcases potential use cases in fully on-chain dynamic NFTs and proposes a URI standard for data retrieval.\nReferences\nData Program Smart Contract: GitHub - nvsriram/solana-data-program: Solana smart contract that handles on-chain data storage\nSolD Website Editor: https://sold-website.vercel.app/\nTypescript SDK: solana-data-program - npm\nQuine NFT: Solscan\nQuine NFT Image: https://sold-website.vercel.app/HoyEJgwKhQG1TPRB2ziBU9uGziwy4f25kDcnKDNgsCkg?cluster=Devnet\nQuine NFT Metadata: https://sold-website.vercel.app/Hb9vkWax5AeLWvCtYSjSvWrN6gTw324gKMa28kcBsgT3?cluster=Devnet\nExamples: solana-data-program/examples at main · nvsriram/solana-data-program · GitHub",
            "comments": "[joec]: Hey, this is actually really cool!\nI have some qualms over the potential for this to be widely adopted as a standard. Instead, I could see this being either a protocol itself for data storage or a program library that can add a layer of abstraction over the management of the accounts/metadata.\nHow do you envision this being used as a standard?\n\n[nvsriram]: Thank you, I am glad you like it! :))\nI do agree that its role as a protocol or program library are more straightforward with how it currently is and it also makes it easier to adopt that way.\nHowever, I do think that a standard for general data retrieval would be quite useful. The idea I had for this was to use the URL format to make any data stored on Solana easily accessible. This would be the same idea as with Solana pay URL and could also be an extension to the same solana URI scheme (would save the effort of having to register a new URI scheme).\nBut the end result of this would be that any user could just use this URL format and paste it in a browser to get the data associated with that data account. All the dApps would have to do to be compliant with this is parse the datakey part of the URL (sol://&lt;datakey&gt;) and return the data using a simple line of code like so:\nreturn (await connection.getAccountInfo(dataKey, commitment)).data;\nIf this URL format were to be adopted as a standard, any user could “upload” their data directly into a data account and have it be easily retrieved via a simple URL. And because the data can be verified when uploaded, it has exciting interactions, say with validating NFT JSON metadata.\n\n[Hamster]: This is really cool. I think its important to have a website to pull account data and has a lot of uses for on-chain nfts.\nI have tested it out with my own on-chain nfts and I can pull the raw data but it does not parse correctly due to usage of anchor (hence the raw data will have bits in front). Any way for the implementation to detect data:image/URLs or “{” (used in json) and start parsing from that byte?\n\n[nvsriram]: Thank you!\nI didn’t really test it out with anchor so didn’t run into the bit padding issue with JSON so currently it just removes all the whitespace bits (that could be introduced when the account is resized etc.) and tries to parse as JSON. But like you mentioned, changing the implementation to parse data between first ‘{’ and last ‘}’ should be fairly straightforward and could be a easy fix.\nAs of right now, it should still display the invalid JSON in the error or in the HTML/CUSTOM data type (although not as nice).\nAs for reading data:image/URLs, that would be a bit tricky. Currently, if the data type is said to be of type image, it returns the raw bytes along with the appropriate content type. The website (under HTML datatype) would pass the URL returning the raw bytes to an iframe as its src. For it to read it as intended, it would need to pass the raw bytes as the URL instead.\n\n",
            "comment_count": 4,
            "original_poster": "nvsriram",
            "activity": "2023-10-01T17:46:34.192Z"
        },
        {
            "id": 402,
            "title": "Bank Capture File",
            "url": "https://forum.solana.com/t/bank-capture-file/402",
            "created_at": "2023-07-24T15:12:31.551Z",
            "posts_count": 2,
            "views": 545,
            "reply_count": 0,
            "last_posted_at": "2023-07-24T23:38:12.686Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Summary\nResolving bank hash mismatches between different validators and validator releases is an arduous process.\nThe most widely used approach involves\ndumping bank hash pre-images to the validator log files (shared with arbitrary other log output)\nusing a log parsing tool to extract information\naccumulating the changes every slot, then constructing a diff\nBackground\nThe bank hash commits to the execution inputs and state changes of a slot\nA bank hash mismatch occurs when two Solana runtime implementations output different bank hashes for the same inputs (same state, same slot)\nThis implies that these two runtime implementations are incompatible, which is a severe bug that has to be fixed\nThe bank hash pre-image refers to the raw inputs fed into the hash function\nThis pre-image is highly useful for debugging, as it pin-points the input that is different\nThere is no standard for encoding the pre-image; All solutions so far rely on hacks that are incompatible across different validator code bases\nRequirements\nThe following pseudocode describes the declarations of the hash constructions part of the bank hash.\naccount_hash := blake3 {\n le u64 lamports\n le u64 slot\n le u64 rent_epoch\n []u8 data\n u8 executable\n [32]u8 owner\n [32]u8 key\n}\naccounts_delta_hash := merkle {\n leaf = [32]byte account_hash\n branch = sha256 {\n [1..=16][32]byte node\n }\n}\nbank_hash := merkle {\n leaf = [32]byte account_hash\n branch = sha256 {\n [1..=16][32]byte node\n }\n}\nThe solution must be able to serialize all of the above data in a language-agnostic format. There should be consensus among validator developers, and every team should be willing to implement and work with this format.\nThe serialized size is estimated to be hundreds of megabytes per slot.\nTherefore, the serialization scheme used should also be efficient.\nStretch Goals\nIdeally, this file format should support streaming use and compress well.\nPerhaps, we could wrap the Protobuf blobs in a binary container format, such as .tar.zst or a custom format.\nPossible Solutions\nDesigning a data structure representing the above information is trivial.\nIt is not obvious which serialization scheme should be used however.\nJSON\nsteviez at Solana Labs has been working on a JSON-based solution.\nThis format can be easily upgraded, but we’d argue it is a little too free form, and does not offer great performance.\nCustom Binary Format\nI’ve worked on a custom binary format for maximum performance.\nThere are a number of obvious shortcomings:\nIt is not easily upgradable\nIt is more difficult to implement and debug\nProtobuf\nAfter meeting with the Firedancer team on this topic, we settled on the mix between the above two. A Protobuf schema can be upgraded just like JSON structures, but it also features powerful cross-language tooling, a schema language for coordinating these upgrades, as well as decent performance. Finally, Solana validators already use the Protobuf stack for RPC.\nWe would like to request comments from client developers, and invite validator developers to collaborate on a solution.",
            "comments": "[ripatel-jump]: Published a draft of the file format here: Initial solcap API by ripatel-fd · Pull Request #543 · firedancer-io/firedancer · GitHub\n\n",
            "comment_count": 1,
            "original_poster": "ripatel-jump",
            "activity": "2023-07-24T23:38:12.686Z"
        },
        {
            "id": 104,
            "title": "sRFC 00009: Sign-In with Programmable (Smart) Wallets using Off-Chain Delegates",
            "url": "https://forum.solana.com/t/srfc-00009-sign-in-with-programmable-smart-wallets-using-off-chain-delegates/104",
            "created_at": "2023-04-16T22:36:10.054Z",
            "posts_count": 13,
            "views": 1448,
            "reply_count": 6,
            "last_posted_at": "2023-06-30T13:23:16.618Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Summary\nThis SRFC proposes an application standard for authenticating users of dapps, specifically those using Programmable Wallets (a.k.a Smart Wallets or Smart Contract Wallets), through the use of off-chain delegates. This standard addresses the challenge of authenticating users who interact with an app using a Programmable Wallet, which does not have a private key for signing messages. By enabling Programmable Wallets to appoint a delegate with a private key to sign messages on their behalf, a secure and verifiable way to authenticate users is achieved.\nIntroduction\nA lot of dapps use the authentication process called off-chain message signing. The mechanism takes advantage of the fact that user keypair can be used to cryptographically sign any arbitrary messages such that anyone can verify that a message is signed by the party controlling the private key for the corresponding public key. The algorithm for this process is as follows:\nThe app produces a unique message and gives it to the user (wallet) to sign.\nThe user signs it and returns the signature back to the app.\nThe app verifies that the signature is indeed valid and is for the message it provided originally.\nIf true, the app issues an auth token that the client can use for future interactions with the app’s API.\nThe Problem of Programmable Wallets\nNot every Solana address has a corresponding private key. A good example is Programmable Wallets - accounts owned by a Solana program that is typically controlled by one or many regular (keypair) wallets and interacts with the chain following the rules of the program.\nFor example, a multisig program owns a vault account shared by multiple owners of the multisig, tracks the status of voting on proposals, and allows executing the proposals once they are approved by the quorum. The vault account has a public key but lacks the private key; the program programmatically “signs” on its behalf.\nHow can we authenticate a user who uses a Programmable Wallet address to interact with a dapp?\nProposed Solution\nIntroducing Off-chain Delegates.\nSo Programmable Wallet accounts cannot sign a message off-chain as they lack a private key. However, they can appoint a delegate - a regular account that has a private key - authorized to sign messages on behalf of the Programmable Wallet account. This delegation can be fully registered on-chain, making it verifiable by anyone.\nLet’s take a look at how the sign-in flow would work when modified to support Programmable Wallets:\nThe app produces a unique message and gives it to the user (Programmable Wallet) to sign.\nThe wallet knows which address controls the Programmable Wallet account and signs the message with the delegate keypair. It then returns the signature along with information about the Programmable Wallet account and delegate addresses to the app.\nThe app verifies that the signature is produced by the delegate.\nThe app verifies that there’s a delegation record (DelegateToken account) for the given delegate created by the account on-chain and that it has not expired.\nIf true, the app issues an auth token that the client can use for future interactions with the app’s API.\nImplementation on the Programmable Wallet programs’ side can vary between Programmable Wallets. Each program can decide and implement the mechanism of creating delegate records in whatever way they want. They just need to make a CPI (cross-program invocation) into the Off-chain Delegate Program that manages those records and sign that with the PDA seeds of the Programmable Wallet account they control.\nDrawbacks\nThis mechanism introduces a slight overhead compared to off-chain signing for regular wallets. The overhead is the DelegateToken account that must be created and stored on-chain for each delegate. The current reference implementation, Off-chain Delegate Program, uses 77 bytes, which is about 0.0014268 SOL per account. The rent can be reclaimed when the delegate is removed, but it’s still worth highlighting.\nAnother problem is additional logic that needs to be implemented on the dapp side. Unfortunately, I don’t see how the mechanism can be implemented entirely in the wallet apps/extensions without explicit support from the dapps. If someone has ideas, please feel free to comment.\nConclusion\nThe proposed SRFC standardizes the process of off-chain authentication for users with Programmable Wallets through the use of off-chain delegates. Although it introduces some overhead due to the on-chain storage of DelegateToken accounts, this mechanism provides a secure and verifiable way to authenticate users interacting with an app using a Programmable Wallet.\nImplementation: Off-chain Delegate Program",
            "comments": "[vovacodes]: Here’s a flow diagram that illustrates the proposed process of authentication.\noff-chain-delegate-14000×2104 152 KB\n\n[silo]: mentioned this on twitter but i’ll paste my comment here for proper discussion:\nWhat if instead of having a DelegateToken account, there was a standard by which a key could be checked for membership (as a controller, or possibly for a particular role) of a programmable wallet?\nThis still wouldn’t solve the problem on the dapp side, as a check would still have to be issued, but having a programmable wallet membership verification mechanism could have a broader set of applications aside from delegated login.\n\n[vovacodes]: silo:\nhaving a programmable wallet membership verification mechanism could have a broader set of applications aside from delegated login.\n@silo Thanks for your comment. Do you have any specific application in mind. Just curious whether DelegateToken isn’t able to cover that.\nThe thing is, DelegateToken is such a simple concept that it can be used in many cases. It might make sense to change the name of the account and the program if it isn’t universal enough to fit the use-cases, but in order to do so, we need to understand what those use-cases are.\n\n[silo]: Sure. So in this proposal, the programmable wallet is asking a member wallet to act as a delegate on behalf of the pw. It knows who the delegates are and provides a delegate to the dapp when requested. And it needs to do this every time a delegate is used. I’m saying maybe we can flip that, and basically have a “registry” where anyone can look up the member wallets of pw. Now the dapp can easily verify membership without having to make the request to the pw every time. This also allows dapps to automatically bootstrap member wallets and link them to a pw account and they can observe changes over time and use that information accordingly vs making the request every time and essentially using a “snapshot” of data (just the single delegate at that instant). Could even use a reverse-lookup sort of mechanism, whereby given a member wallet, the pw information can be looked up.\n\n[vovacodes]: First of all, with the current design it is still possible to fetch all the Programmable Wallets (PWs) where the user is a delegate of, it’s a relatively simple getProgramAddresses call with filtering by the delegate field.\nvs making the request every time and essentially using a “snapshot” of data\nIn my view, this is a feature actually. As a dapp, you want to check the current snapshot always, because PWs might remove a delegate (e.g. a compromised key) at any moment.\nallows dapps to automatically bootstrap member wallets and link them to a pw account\nBut what if a user account has multiple PWs associated with it. How would the dapp choose which one to link to? How I see the typical use case is that your wallet should tell the dapp which address to use (cuz that’s your interface with the web3 world), hence the PW → Delegate relationship rather than the other way around.\nMaybe this is because I’m having s specific scenario in mind (signing in with a multisig wallet) but I’m having difficulty with understanding why the reverse relationship (Delegate → PW) would be easier to use.\n\n[silo]: Gotcha. So a registry would never have stale data. It would always reflect current membership. The reason for having a reverse lookup (or registry), would be so that the dapp could have a “unified view” of the client. While a user might be using a specific delegate (member wallet) to access a dapp, the dapp may use information in the other wallets. For example, think of a something that has token-gated access. I might have a specific NFT that grants me access to something sitting safely in my hardware wallet, while I use my mobile wallet as a delegate to gain access. (This is actually the specific use case that we have with our gaming NFTs. Kinda like a delegated-login-with-NFT.)\nThe fact that the member wallet might have multiple PWs associated with it is a good point. For us, we actually don’t allow it and instead use a domain concept (a member can only be linked to a singe PW within a domain).\n\n[vovacodes]: Thank you for sharing your viewpoint. I now completely understand your perspective. It seems that the use-case you mentioned is already being addressed through “Wallet Linking” by several existing dApps, such as Tensor, Magic Eden, and Dialect.\nWhile I agree that this use-case might warrant its own standard, I suggest separating it from the current proposal. This is because, firstly, it doesn’t necessarily require support from the Wallet Standard side, as dApps can independently maintain a list of “Linked Wallets.” Secondly, this feature is already functional for many dApps, in contrast to the present proposal which aims to address a need that is not yet fulfilled by any dApp due to the absence of a standardized approach.\n\n[silo]: Yea, if this is just for a delegated sign-in mechanism then a separate standard would make sense.\nRight now the wallet linking most dapps use isn’t done on-chain and usually requires the wallets to be in the same browser as normal browser sessions are used for the linking. So there’s no way to link, say, a mobile wallet to an existing desktop wallet.\n\n[joec]: Hey @vovacodes I’m wondering what you might think of the work we’re doing on sRFC 00007 and how that approach might compare to what you’ve suggested here.\nTake a look at my response here and let me know what you think of the spec I added.\nIn short, I think we have some overlapping approaches to delegating signatures. In my spec for sRFC 00007, I’m obviously targeting encryption keys, but we might be able to accomplish something very similar for delegated wallets using the same functionality but adapted for on-chain registered delegation.\nThe idea would simply be to use this protocol spec (possibly even the same program?) to store on-chain the delegate keypair so anyone can verify.\nI suggest leveraging the same program only because with your particular use case(s), we need only store a Solana address in the PDA, rather than the complex data layout I’ve designed for the Keying program. However, since our use cases are somewhat related (verifying some key/keypair is in fact owned by you and authorized by you), it might make sense to share the same program namespace.\nLet me know!\n\n[vovacodes]: @joec, I believe your program addresses this use-case perfectly. I suppose we can use a unique discriminator for the “message signing delegates” and use additional configuration section for adding parameters like expiration, etc\n\n[joec]: Actually I’m curious how important you think the concept of “expiration” is? One potential issue is that’s really only an “honest man’s config”. What I mean is, sure the on-chain program can specify that they key is expired, but anyone querying that information must make the decision at their implementation (client) level to honor the expiration date or ignore it\n\n[vovacodes]: Yeah it’s a fair concern. A pro-expiration argument would be that the proposed process already relies upon the “decision at their implementation (client) level to honor” the account stored on-chain, so expanding it slightly to include expiration doesn’t change the trust model much but adds a bit more flexibility.\nI would personally like to hear what other folks in the ecosystem think about it, especially the app developers. We could also start without expiration and see if we really need it.\n\n",
            "comment_count": 12,
            "original_poster": "vovacodes",
            "activity": "2023-06-30T13:23:16.618Z"
        },
        {
            "id": 65,
            "title": "sRFC 00007 - Encryption Standard for Solana Keypairs",
            "url": "https://forum.solana.com/t/srfc-00007-encryption-standard-for-solana-keypairs/65",
            "created_at": "2023-04-08T13:02:39.224Z",
            "posts_count": 8,
            "views": 2726,
            "reply_count": 5,
            "last_posted_at": "2023-06-28T23:56:45.417Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "As the Solana ecosystem evolves, the need for an encryption standard for Solana key-pairs becomes increasingly important for securing sensitive data such as account state or files on distributed file systems like IPFS and Arveawe. While Ed25519 key-pairs are effective for signing messages, they are not usable for asymmetric encryption, which is crucial for user privacy and data protection.\nTo address this issue, I propose creating a standard for converting Ed25519 key-pairs to Curve25519 key-pairs that are designed for the Diffie Hellman Key exchange protocol, which would enable asymmetric encryption without having to generate seperate keys to perform this kind of action. This conversion allows users and software wallets to leverage libraries like TweetNaCl for robust and easy-to-use encryption implementations.\nImplementing an encryption standard for Solana key-pairs would give Solana one more use-case which could drive a lot of traction to the chain.\nI made a reference implementation as a feature for this kind of conversion in the Wallet Adapter example repository. Check it out here: wallet-standard/solanaWallet.ts at encryption · valentinmadrid/wallet-standard · GitHub\nWhile there is a consensus within the Solana community that encryption is essential, there may be different approaches to implementing it; thus, I welcome everyone to provide their feedback and suggestions on this proposal to ensure the most effective solution is adopted.",
            "comments": "[valentin]: Here is a demo implementing the conversion from Ed25519 to Curve25519 keypairs showing off the performance and safety of using Tweetnacl using these: https://solana-encryption-demo.vercel.app/ \nThe following paper suggests that such a “conversion”/usage for signing and encryption using the same key should be safe: On the Joint Security of Encryption and Signature in EMV\nThis answer on the cryptography Stackexchange suggests the same thing: What happen if the curve used in key agreement protocol also used in signature inside of protocol? - Cryptography Stack Exchange\nAgain, this proposal has been made to come to an agreement on how Solana Keypairs should be derived to be compatible for Diffie Hellman Key exchange or other kinds of encryption mecanisms. Once an agreement on this topic is reached, wallets could start to implement encryption features as part of the Wallet Standard.\n\n[valentin]: As an alternative to deriving a Curve25519 keypair from a regular Solana Keypair to perform encryption, Jordan Sexton suggested that a keypair could be derived from a signed message by any wallet to derive an encryption key(including hardware wallets).\nThis could have upsides to my proposal, you can read about his suggestion here: https://twitter.com/jordaaash/status/1645173328624361472?s=46&amp;t=B3h7suAANXSyY_bAa_5fbw\nA concern with this approach may be that the cryptography could be broken once two dApps request a signature with the same seed. There would be a need for some kind of one time nonce that has to be different on each encryption, which would be hard to achieve.\n\n[Alchemize]: deriving a keypair from a signed message by a wallet as an encryption key looks nice.\ncan having 4-8 digit PIN manually entered by the user be used a seed to be signed ?\ncan this help in avoiding seed collision ?\n\n[valentin]: This is how we’ve agreed on this proposal for now:\nThe user provides an input message (plaintext) intended for encryption, along with the recipient’s public key, and invokes the encryption function available in the browser’s window.\nThe plaintext message is passed on to the encryption module within the user’s digital wallet.\nThe wallet derives a Curve keypair from the signature generated by the user’s stored keypair. The size of the derived Curve keypair is determined by the byte length of the signature.\nThe wallet then performs a box encryption operation (asymmetric) utilizing the tweetnacl cryptographic library. The encryption process incorporates: a. The recipient’s public key b. The plaintext message c. A randomly generated nonce (number used once) by the wallet d. The Curve keypair, which has been derived from the signature\nUpon successful encryption, the wallet outputs the resulting ciphertext and the generated nonce to the user.\nThis constraint implies that dApps must implement a mechanism to handle the storage, retrieval, and sharing of public keys associated with encryption. The dApps could maintain an on chain program or database, which stores public keys provided by wallet holders explicitly for encryption purposes.\nPlease give me your thoughts on this.\n\n[Alchemize]: the Idea behind having a PIN than a fixed string was that ,when the user enters that specific PIN, the wallet signs it to derive a new Curve keypair. so the derivation of Encryption keypair is dependent on the PIN.\nAdvantage : PIN acts as another layer of security to the cipher text\nDisAdvantage : Decryption cannot happen if the PIN is lost/ forgotten\nFYI : just wanted to explore some new directions… might not be the best way…\n\n[valentin]: A PIN would mean the encryption Keypair would not be recoverable using a private key if the PIN is lost, therefore the mecanism to prevent the encryption from being broken is that the wallet generates a random nonce additionally to the deterministic message for the encryption that is given back to the client afterwards. The random generated nonce by the wallet can be publicly shared.\n\n[joec]: Just wanted to circle back here to share an update on this sRFC workflow!\n valentin:\nThis constraint implies that dApps must implement a mechanism to handle the storage, retrieval, and sharing of public keys associated with encryption. The dApps could maintain an on chain program or database, which stores public keys provided by wallet holders explicitly for encryption purposes.\n@jordaaash and I have been working on an implementation for exactly this, with an on-chain program.\n \n \n GitHub\n \n \n \nGitHub - buffalojoec/keyring-program: Keyring program\n \nKeyring program. Contribute to buffalojoec/keyring-program development by creating an account on GitHub.\n \n \n \n \n \n \nI’ve documented how this program works in the repository’s README, but the TL/DR is:\nThis program does exactly what @valentin suggested: stores public keys associated with encryption\nIt makes use of a “Keystore” PDA mapped to a particular wallet address\nIt’s specifically tailored to implement serialization/deserialization in the off-chain client, so that the program can remain frozen while new encryption algorithms may be added in the future\nIt’s inception should go hand-in-hand with a robust review process - akin to sRFCs or simply Pull Requests - to add support for new encryption algorithms\nAt the time of writing this reply, the encryption algorithms depicted in the client are for demonstration purposes only, and any associated configurations may look different than this spec in production.\nThe implementation is also in active development and subject to change.\nWould love anyone’s feedback on the code, though!!\n\n",
            "comment_count": 7,
            "original_poster": "valentin",
            "activity": "2023-06-28T23:56:45.417Z"
        },
        {
            "id": 292,
            "title": "sRFC 18: Generalized Small State Compression",
            "url": "https://forum.solana.com/t/srfc-18-generalized-small-state-compression/292",
            "created_at": "2023-06-06T19:57:50.495Z",
            "posts_count": 2,
            "views": 654,
            "reply_count": 0,
            "last_posted_at": "2023-06-27T13:21:48.990Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Generalized Small State Compression\nThe following repo describes an event interface and instruction behavior that allows programs to compress state up to 300 bytes per asset.\n \n \n GitHub\n \n \n \nGitHub - ngundotra/srfc-compressed-state\n \nContribute to ngundotra/srfc-compressed-state development by creating an account on GitHub.",
            "comments": "[joec]: This appears to iterate directly on sRFC 00016, yeah?\n\n",
            "comment_count": 1,
            "original_poster": "ngundotra",
            "activity": "2023-06-27T13:21:48.990Z"
        },
        {
            "id": 340,
            "title": "sRFC 00019: Wallet Standard - Get Ephemeral Signers feature",
            "url": "https://forum.solana.com/t/srfc-00019-wallet-standard-get-ephemeral-signers-feature/340",
            "created_at": "2023-06-26T17:20:54.861Z",
            "posts_count": 1,
            "views": 740,
            "reply_count": 0,
            "last_posted_at": "2023-06-26T17:20:55.066Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Wallet Standard - Get Ephemeral Signers feature\nSummary\nEphemeral Signers (ES) is a pretty common pattern in the Solana apps ecosystem. ES are keys that are expected to sign the transaction and be discarded right after. An example of when they are needed is the SystemProgram.createAccount instruction. When creating a non-PDA account, this instruction requires the new account to be a signer to verify that whoever calls this instruction actually holds authority over this account.\nMost of the time, app developers use Keypair.generate() to create Ephemeral Signers. Then they sign a transaction that requires an ES with the Keypair and throw it away immediately after. This pattern works pretty well when the signed transaction is immediately sent and executed. That said, it prevents a certain class of use-cases when the transaction is stored on chain and executed later when a certain condition is met, e.g. time lock is released, multisig approval threshold is reached, such as with some Smart Wallet implementations (Realms, Squads, Clockwork etc). In these cases, the Ephemeral Keypair will not be available to sign the “execute” transaction because it has already been “forgotten”.\nProposed Solution\nPlease refer to the linked Pull Request for the details\nImplementation\n \n github.com/solana-labs/wallet-standard\n \n \n \n \n \n \n \n \n `solana:getEphemeralSigners` feature\n \n \n solana-labs:master ← vovacodes:feat/get-ephemeral-signers\n \n \n \n opened 05:04PM - 26 Jun 23 UTC\n \n \n \n \n vovacodes\n \n \n \n \n +27\n -0\n \n \n \n \n \n ## Problem\nEphemeral Signers (ES) is a pretty common pattern in the Solana ap…ps ecosystem. ES are keys that are expected to sign the transaction and be discarded right after. An example of when they are needed is the&nbsp;`SystemProgram.createAccount`&nbsp;instruction. When creating a non-PDA account, this instruction requires the new account to be a signer to verify that whoever calls this instruction actually holds authority over this account.\nMost of the time, app developers use&nbsp;`Keypair.generate()`&nbsp;to create Ephemeral Signers. Then they sign a transaction that requires an ES with the Keypair and throw it away immediately after. This pattern works pretty well when the signed transaction is immediately sent and executed. That said, it prevents a certain class of use-cases when the transaction is stored on chain and executed later when a certain condition is met, e.g. time lock is released, multisig approval threshold is reached, such as with some Smart Wallet implementations (Realms, Squads, Clockwork etc). In these cases, the Ephemeral Keypair will not be available to sign the \"execute\" transaction because it has already been \"forgotten\".\n## Proposed Solution\nWe suggest a new Wallet Standard feature -`getEphemeralSigners(numberOfSigners: number): Promise&lt;Array&lt;Pubkey&gt;&gt;` that would&nbsp;allow apps to request any number of ESs from the wallet. The wallet would return an array of public keys that the app developer can use in their transactions.\nImplementation of this feature can vary depending on the type of the wallet being used. For instance, for a multisig wallet, an ES can be a PDA owned by the Multisig Program, enabling the Program to sign the transaction on behalf of that account. While for a regular wallet (Phantom, Glow, etc.), it can be a Keypair generated by the wallet itself and stored securely in the extension's background storage over the course of the browser session.\nRegardless of the implementation, the app developer doesn't have to worry about how the ES was generated; they can assume that the public key will be a signer of the transaction when it comes to the execution time.\n## Benefits for Apps\nBy detecting and using this Wallet Standard feature, app developers can be sure that their app will work with all existing and future wallet implementations out of the box and no hacks are needed.\nThere’s also a security benefit of never exposing the Ephemeral Signer Keypair to your app code directly - it is kept in the wallet secured storage, so a malicious dependency can’t steal it and use in any sort of re-initialization attacks.\n## Benefits for Wallets\nBy exposing this functionality via the Wallet Standard interface, wallets reserve a place for themselves to innovate and add Smart Wallet features in the future without changing their public interface.\n## Usage\nDetecting the feature and requesting an Ephemeral Signer can be as easy as this:\n```ts\nconst ephemeralSignerPubkey = \"standard\" in adapter &amp;&amp; \"solana:getEphemeralSigners\" in adapter.wallet.features &amp;&amp; await adapter.wallet \n .features[\"solana:getEphemeralSigners\"]\n .getEphemeralSigners(1)[0]\n```\nHere we check that our wallet adapter is a Wallet Standard implementation and if it exposes the `solana:getEphemeralSigners` feature. If so we request 1 Ephemeral Signer public key from the wallet, which now can be passed as an account into a transaction that needs an extra signer.\n## Proof of Concept\nWe ([Squads](https://squads.so)) implemented a proof of concept for this feature as part of our multisig-powered Smart Wallet - Fuse. We tested the integration with Jupiter Limit Orders product and are currently in conversation with other app developers regarding the adoption. We also received some positive initial feedback from other ecosystem participants (Realms). We believe, standardizing this feature will significantly help with its adoption and enable proliferation of Smart Wallets ecosystem on Solana.\n## Next steps\n- [ ] Create an sRFC for this feature proposal and refine the design if necessary",
            "comments": "",
            "comment_count": 0,
            "original_poster": "vovacodes",
            "activity": "2023-06-26T17:20:55.066Z"
        },
        {
            "id": 215,
            "title": "sRFC 00013 - CPI Events",
            "url": "https://forum.solana.com/t/srfc-00013-cpi-events/215",
            "created_at": "2023-05-08T20:22:55.153Z",
            "posts_count": 6,
            "views": 890,
            "reply_count": 1,
            "last_posted_at": "2023-06-16T17:51:10.908Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "GitHub\n \n \n \nGitHub - ngundotra/srfc-event-indexing: Event based indexing for Solana programs\n \nEvent based indexing for Solana programs. Contribute to ngundotra/srfc-event-indexing development by creating an account on GitHub.",
            "comments": "[nickfrosty]: for check 1: why is it important for the data to start with those specific 8 bytes? is it just to have a standard discriminator to know it is a “cpi event” that should be indexed\n\n[mschneider]: Appreciate work on this problem. Events are an essential but very neglected part of the runtime. I feel conflicted about trying another “runtime hack” for this purpose. Curious what a design from scratch would look like.\n\n[mschneider]: I looked more in detail into the proposed spec, one issue I see, is that the CPI can reference accounts via ALT, which would break the current parser. Given how basic events are for client programs and how difficult it is to have accurate ALT copies on client side, I am more convinced that we should make events a first class citizen so that it’s easy to subscribe to them using a websocket similar to ETH:\n \n docs.ethers.org\n \n \n \nEvents\n \nDocumentation for ethers, a complete, tiny and simple Ethereum library.\n \n \n \n \n \n \nInner instructions are currently part of TransactionMeta, which means you can either subscribe to all txs in a block (very unscalable) or just one Transaction.\n\n[mschneider]: Another argument against CPI would be that the current program is passed to the emit call as an account, this obviously creates a lot of overhead on the runtime, as loading the a fairly average program means loading 3MB of memory.\n\n[jarry]: Triton has implemented a transactionSubscribe RPC call (can filter by accounts, success/failure) which helps a lot in mitigating the problem of streaming CPI events\n\n",
            "comment_count": 5,
            "original_poster": "ngundotra",
            "activity": "2023-06-16T17:51:10.908Z"
        },
        {
            "id": 66,
            "title": "sRFC 00008: IDL Standard",
            "url": "https://forum.solana.com/t/srfc-00008-idl-standard/66",
            "created_at": "2023-04-10T17:11:26.948Z",
            "posts_count": 9,
            "views": 1863,
            "reply_count": 4,
            "last_posted_at": "2023-06-09T18:17:18.294Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "sRFC 00008: IDL Standard\nSummary\nIt’s no question that the introduction of an IDL alongside a Solana program was one of the biggest value-adds provided by Anchor.\nWith an IDL, indexers, explorers, and many other tools can gain insight into a deployed program that they otherwise couldn’t have gotten - especially deserialization of PDA data.\nRight now, we continually run into problems when trying to develop tools or solutions that encounter two types of programs: one with an IDL published, and one without an IDL.\nImmediately these projects have to essentially “skip” or “omit” programs that are deployed without an IDL, because their solution simply will not work without one.\nWe need a standard way to manage IDLs for programs across Solana, and it shouldn’t matter which framework or tools you use to build your program.\nQuestions\nWhat should an IDL’s interface look like?\nTwo existing and popular tools for creating IDLs - Anchor and Shank - actually generate very similar IDLs.\nHere’s part of a simple example:\n{\n \"version\": \"0.1.0\",\n \"name\": \"car_rental_service\",\n \"instructions\": [\n ...\n {\n \"name\": \"BookRental\",\n \"accounts\": [\n {\n \"name\": \"rentalAccount\",\n \"isMut\": true,\n \"isSigner\": false,\n \"desc\": \"The account that will represent the actual order for the rental\"\n },\n {\n \"name\": \"carAccount\",\n \"isMut\": false,\n \"isSigner\": false,\n \"desc\": \"The account representing the Car being rented in this order\"\n },\n {\n \"name\": \"payer\",\n \"isMut\": true,\n \"isSigner\": false,\n \"desc\": \"Fee payer\"\n },\n {\n \"name\": \"systemProgram\",\n \"isMut\": false,\n \"isSigner\": false,\n \"desc\": \"The System Program\"\n }\n ],\n \"args\": [\n {\n \"name\": \"bookRentalArgs\",\n \"type\": {\n \"defined\": \"BookRentalArgs\"\n }\n }\n ],\n \"discriminant\": {\n \"type\": \"u8\",\n \"value\": 1\n }\n },\n ...\n ],\n \"accounts\": [\n ...\n {\n \"name\": \"RentalOrder\",\n \"type\": {\n \"kind\": \"struct\",\n \"fields\": [\n {\n \"name\": \"car\",\n \"type\": \"publicKey\"\n },\n {\n \"name\": \"name\",\n \"type\": \"string\"\n },\n {\n \"name\": \"pickUpDate\",\n \"type\": \"string\"\n },\n {\n \"name\": \"returnDate\",\n \"type\": \"string\"\n },\n {\n \"name\": \"price\",\n \"type\": \"u64\"\n },\n {\n \"name\": \"status\",\n \"type\": {\n \"defined\": \"RentalOrderStatus\"\n }\n }\n ]\n }\n }\n ],\n \"types\": [\n ...\n {\n \"name\": \"RentalOrderStatus\",\n \"type\": {\n \"kind\": \"enum\",\n \"variants\": [\n {\n \"name\": \"Created\"\n },\n {\n \"name\": \"PickedUp\"\n },\n {\n \"name\": \"Returned\"\n }\n ]\n }\n }\n ],\n \"metadata\": {\n \"origin\": \"shank\",\n \"address\": \"8avNGHVXDwsELJaWMSoUZ44CirQd4zyU9Ez4ZmP4jNjZ\",\n \"binaryVersion\": \"0.0.12\",\n \"libVersion\": \"0.0.12\"\n }\n}\n(Full IDL here)\nUltimately, we want to figure out if we can use this (or Anchor’s IDL) as the interface for all IDLs across Solana.\nWhat Should be the Standard for Implementing the IDL Interface?\nIf we consider the IDL above - or something similar - to serve as our IDL interface, what should be the standard for implementing?\nCan you just add whatever fields you want, as long as you still have the fields from the interface? For example:\n\"accounts\": [\n ...\n {\n \"name\": \"RentalOrder\",\n \"type\": {\n \"kind\": \"struct\",\n \"fields\": [\n {\n \"name\": \"car\",\n \"type\": \"publicKey\"\n },\n {\n \"name\": \"name\",\n \"type\": \"string\"\n },\n ],\n \"myCustomConfiguration\": {\n \"someConfig\": 1,\n \"someOtherConfig\": \"2\",\n }\n }\n }\n ],\nConsiderations:\nThis would break any type-oriented IDL parsers by introducing new fields that otherwise weren’t part of the type schema\nA lack of limits might inflate the size of IDLs unexpectedly\nWhat crate/lib/types Should be Used?\nWe could do something like introduce a standard crate with the interface types as the basis for all IDLs on Solana.\nWe then could modify these types to allow for pluggable custom configurations, or some other means for easily implementing the interface for an IDL leveraging this crate.\nA great candidate for this standard library of IDL types (interface) is Iron Forge’s Solana IDL - an open-source crate containing the IDL types from Shank compatible with serde.\nIt would be great to hear thoughts on:\nElecting this crate as the IDL type interface\nModifying this crate to serve as an implementable interface, with customizable type configurations\nImplementations &amp; Ideas\nHere’s a PR introduced by Noah from Solana Labs to introduce the idea of an IDL program within the Solana runtime.\nHere’s a crate produced by Thorsten from Iron Forge to lay down the Rust types for creating an IDL leveraging serde.\nConclusion\nIn short, like any other interface or standard proposal, this is a migration that could be done in a way that hurts only once, but allows for easy integrations and added support in the future.\nA quick recap on the questions proposed:\nWhat should an IDL’s interface look like?\nWhat should be the standard for implementing the IDL interface?\nWhat crate/lib/types should be used?",
            "comments": "[kevinheavey]: I have a few thoughts about this. The first one is: what will typed bytecode (BTF) give us, and does it cover any things that IDLs cover?\n\n[jarry]: I really like the shank + anchor IDL format. Specifically, I like the fact that it makes the discriminant (i.e. instruction identifier) explicit. One of the nice things (some may disagree) about Solana is how much control it gives to the developer to come up with the semantics of how a program instruction should be dispatched. Anchor makes a lot of opinionated decisions on behalf of the devs, which ends up improving devex + security and reducing boilerplate, but I think those decisions should be explicit in the IDL rather than inferred from the client.\nIMO the IDL should explicitly have 100% of the information needed to generate an instruction from a client as opposed forcing the client to read the instruction name and compute some bespoke hash before generating the instruction.\nNote that these IDLs can never capture 100% of all execution paths because there is a bunch of custom logic that can be injected in a program to dictate how accounts should be deserialized. However, the shank + anchor IDL seems usable for like 99% of programs, so I think it makes sense as a standard.\n\n[callensm]: For reference (coming from twitter thread) something that I had previously helped work on related to this topic:\n \n \n GitHub\n \n \n \nGitHub - solana-idl-foundation/solana-idl-spec: Specifying a Solana IDL...\n \nSpecifying a Solana IDL Standard, inspired from the Anchor IDL Standard - GitHub - solana-idl-foundation/solana-idl-spec: Specifying a Solana IDL Standard, inspired from the Anchor IDL Standard\n\n[joec]: This is where the gray area comes into play, though.\nI agree Shank/Anchor IDLs cover a wide range of use cases and would be a great start for an interface, but what if you start to see an increase in alternate ways of deserializing accounts or instructions?\nHow do we begin to lobby indexers and tools to support these added configs in the IDL?\nWe may need to - at that time - introduce an interface/standard on top of an interface \nSomething like deserializing data is integral to a lot of the tools that would need to know about these added configs, so that’s a use case where you might weigh “should ser/deser configs go into every IDL then? Or just ones that vary from the norm? What is the norm anyhow?”\nA different custom config to an IDL - such as details on how your PDA’s addresses relate to each other (ie. seeds expanded) might be something that matters less to an indexer/explorer/UI of sorts. In this case, it might make more sense for the developer(s) and their amassed following to lobby these tooling providers to support their custom configs for some value-add reason, but ultimately it’s no sweat to the indexer tools if they don’t.\n\n[joec]: I guess I’ll be the one to ask what you’re talking about, since I don’t know lol. Care to elaborate?\n\n[kevinheavey]: twitter.com\n \n \n \ntoly 🇺🇸\n@aeyakovenko\n Typed bytecode is coming to @solana with ABIv2. It will be so f'ing awesome! So what is it? 🧵\n 6:47 PM - 9 Feb 2023\n \n \n \n \n 360\n \n \n \n \n \n 80\n\n[joec]: Do we know the timeline for ABIv2?\nI don’t really see IDL interfacing/standardizing as a huge lift compared to other standards, so it might not hurt to sort out despite this impending change. Timeline-dependent, of course.\n\n[Marche]: While working on soda, have given significant consideration to the standardization of IDLs within the Solana ecosystem. I often find myself contemplating an expanded version of an IDL, including supplementary information that enables the incorporation of specific features. However, it is crucial to address a fundamental question: What is the primary purpose of an IDL, and what should it encompass? With this in mind, I believe we should carefully consider the following points:\nShould the IDL strictly adhere to its namesake and serve solely as an interface description, or should it serve additional purposes?\nIs it necessary to make seed information a mandatory component of the IDL?\nShould the IDL incorporate information about the inner workings of the smart contract in some manner?\nWhile I don’t possess definitive answers to these questions, I firmly believe it is imperative to establish clear boundaries regarding the purpose of an IDL. Instead of blindly adding various elements to the IDL, we should explore alternative sources and leverage additional data structures when necessary. If every developer starts incorporating the keys they deem necessary, we run the risk of ending up with divergent and non-standardized implementations that result in overlapping keys.\n\n",
            "comment_count": 8,
            "original_poster": "joec",
            "activity": "2023-06-09T18:17:18.294Z"
        },
        {
            "id": 267,
            "title": "sRFC 00015: Interfaces",
            "url": "https://forum.solana.com/t/srfc-00015-interfaces/267",
            "created_at": "2023-05-24T14:29:01.363Z",
            "posts_count": 11,
            "views": 1517,
            "reply_count": 5,
            "last_posted_at": "2023-06-09T04:11:10.671Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Interfaces for programs\nSummary\nGoal is to define a simple interface specifications for programs that avoid creating additional CPIs during execution.\nInterfaces must be discoverable from the program elf.\nDiscovery shouldn’t require a CPI\nCalling an interface should be as fast as calling a non interfaced program\nTransactions should be the same size\nThere needs to be some way to add interfaces to current set of programs on solana\nImplementation: \n1 interface defines 1 method. So instead of a “Token” interface, there is a specific interface for the Transfer method. The interface is identified by a u128 GUID, a random number that can be generated by a dev.\nPublishing Interfaces\nThe interface is published as in .rs files as documentation for the GUID in mod interfaces\nFor Example:\n /// interfaces.rs\n /// Transfers tokens from one account to another either directly or via a\n /// delegate. If this account is associated with the native mint then equal\n /// amounts of SOL and Tokens will be transferred to the destination\n /// account.\n ///\n /// Accounts expected by this instruction:\n ///\n /// * Single owner/delegate\n /// 0. `[writable]` The source account.\n /// 1. `[writable]` The destination account.\n /// 2. `[signer]` The source account's owner/delegate.\n ///\n /// * Multisignature owner/delegate\n /// 0. `[writable]` The source account.\n /// 1. `[writable]` The destination account.\n /// 2. `[]` The source account's multisignature owner/delegate.\n /// 3. ..3+M `[signer]` M signer accounts.\n /// Transfer {\n /// /// The amount of tokens to transfer.\n /// amount: u64,\n ///},\n const TRANSFER: u128 = 0x2423423fda2344321u128;\nRegistering Interfaces\n///program's instruction.rs\ninterfaces!([(interfaces::TRANSFER, interface::Instruction::U8(TokenInstruction:Transfer))])\nThis macro takes an array of mapping the GUID to the instruction id. The instruction type is coerced into the right format by Instruction::U8, so types of any kind of instruction index can be handled. The macro generates a segment that is included in the program ELF file that can be easily parsed from the program bytecode.\nDiscovering Interfaces\n///implementation code\nfn get_interface(program_account:: Account, guid: u128) -&gt; Result&lt;interface::Instruction&gt;\nThis helper function finds the lookup table for the interfaces at a well defined spot in the program byte code and finds the interface instruction index.",
            "comments": "[toly]: Github Issues related to BTF changes that would make interfaces discoverable\n \n github.com/solana-labs/solana\n \n \n \n \n\t \n \n \n \n Loader v3 Built-In Program\n \n \n \n opened 05:27PM - 24 Jan 23 UTC\n \n \n \n \n Lichtso\n \n \n \n \n \n runtime\n \n \n \n \n - New loader built-in program\n - Pin account allocations host ptrs by reservin…g (without allocating) host address space for account resizing\n - Support multiple entrypoints (generalized methods instead of one `main`) per executable in RBPF\n - [Optional] Use page table `dirty` bit to track which parts of accounts were actually modified and report that back to the accounts DB to allow for a partial write back to disk. This could be done using either using [`/proc/PID/pagemap`](https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/mm/soft-dirty.rst) or using CPU virtualization.\n- Actual ABI changes:\n - Rework executable / program account ownership chain and `is_executable` flag (workaround for avoiding finalization)\n - [Not MVP] Dynamic ABI memory layout using BTF relocations\n - Unify the currently separate virtual address spaces and remove address translation at runtime\n - Share the host address space for all programs in a transaction (similar to [Native Client](https://developer.chrome.com/docs/native-client/))\n - Dynamic function calls replacing CPI and Syscalls\n - Replace VM nesting by dynamic linking using two levels of indirection\n - [Not MVP] Replace syscalls by CPI to built-in programs\n - Allocation and lifetime tracking\n - Stack allocation for internal types\n - Normal pointers with memory layout information: Load always possible, store only if pointer is mutable\n - Heap allocation (transaction global) for external types and persistent structures (accounts)\n - Opaque pointers: No load or store possible\n - Runtime provides table of these opaque pointers for programs to lookup their members\n - [Not MVP] How to inline dynamic arrays / vectors into structs (especially accounts)?\n \n \n \n \n \n \n \n \n github.com/solana-labs/solana\n \n \n \n \n\t \n \n \n \n Generate and Verify BTF\n \n \n \n opened 05:22PM - 24 Jan 23 UTC\n \n \n \n \n Lichtso\n \n \n \n \n \n runtime\n \n \n \n \n - BTF type info in toolchain\n - [Not MVP] ELF dynamic loader instructions for …defining on-chain addresses of dependencies\n - [Optional] cargo (dependencies)\n - rustc (attribute)\n - LLVM: Lift C type restriction, inject runtime code for lifetime tracking\n- New stricter verifier in RBPF\n - [Not MVP] Reject cyclic dependencies when deploying a program\n - [Not MVP] Enforce redeployment maintains existing interfaces (function signatures and types), optionally support type migration\n - Type inference\n - Emit type line info for transmutation / reinterpretation\n - Reject ptr transmutation / reinterpretation\n - Forbid ptr introspection or order based comparison, only allow equality test (check for aliasing)\n - Track types of stack slots\n - Canonicalize bounds checks of sub-slices\n - Rustc needs to emit the canonical conditional branch\n - Verifier checks that all sub-slicing has such a runtime bound check\n - Restrict control flow\n - Forbid jumps outside of the current function\n - Enforce that functions end with an `exit` instruction\n - Constrain `call` and `callx` to functions with the same signature\n\n[toly]: Github Issues related to BTF changes that would make interfaces discoverable\n \n github.com/solana-labs/solana\n \n \n \n \n\t \n \n \n \n Program-Runtime v2 - Road Map\n \n \n \n opened 05:32PM - 06 Nov 22 UTC\n \n \n \n \n Lichtso\n \n \n \n \n \n enhancement\n \n \n work in progress\n \n \n runtime\n \n \n \n \n The performance optimizations we have been working on inside the runtime under t…he ABIv2 project the past year will be deployed separately as [\"account data direct mapping\"](https://github.com/solana-labs/solana/pull/28053). The redesign of the ABI between runtime and on-chain program is pushed out further as we changed the goals again. A discussion with @alessandrod and @pgarg66 led to the following:\n \n### Tasks\n- [ ] #29803\n - [x] #29654\n - [x] #30154\n - [x] #30139\n - [x] #30282\n - [x] #30336\n - [x] #30337\n - [x] #30348\n - [x] #30371\n - [x] #30275\n - [x] #30425\n - [x] #30803\n - [x] #30900\n - [x] #30902\n - [x] #30561\n - [x] #30703\n - [x] #30900\n - [x] #30902\n - [x] #30940\n - [x] #30945\n - [x] #30950\n - [x] #30959\n - [x] #31034\n - [x] #31036\n - [x] #31116\n - [x] #31118\n - [x] #31142\n - [x] #31311\n - [x] #31331\n - [x] #31395\n - [x] #31413\n - [x] #31465\n - [x] #31493\n - [x] #31494\n- [ ] #20323\n - [x] https://github.com/solana-labs/rbpf/pull/454\n - [x] https://github.com/solana-labs/rbpf/pull/460\n- [ ] #29863\n- [ ] #29864\n - [x] #29728\n - [x] #30579\n - [x] #30614\n - [x] #30464\n - [x] #30693\n - [x] #30893\n - [x] #31007\n - [x] #31088\n - [x] #31244\n - [x] #31221\n - [x] #31244\n - [x] #31324\n - [x] #31329\n - [x] #31345\n - [x] #31429\n - [x] #31488\n- [Not MVP] Adjust other built-in programs and the testing framework\n\n[ripatel-jump]: The addition of a GUID and the interfaces macro seems redundant. It also seems susceptible to type confusion security issues if an attacker manages to create two distinct function signatures with the same ID.\nHow about the following?\nAdjust the compiler to generate BTF for all public and externally linked entrypoints\nAdjust the compiler to generate BTF for all imported entrypoints\nWhen generating BTF for a function, also generate BTF for all transitive types (the types of the function arguments and the return type, recursively)\nThe ELF format only supports the specification of one entrypoint, so we could instead signal what is public through a custom flag in the dynamic symbol table, e.g. STV_PUBLIC_ENTRYPOINT.\nThe Solana SDK could make this more developer-friendly through a macro annotations, like so:\n// Callee\n#[solana_program::entrypoint]\npub fn transfer(bla: u32) -&gt; Result&lt;u64, String&gt;\n// Caller\nuse callee::transfer;\nfn bla() {\n transfer(...);\n}\nIn both the caller and callee, the ELF of both programs would contain the BTF definitions of types\nResult&lt;u64, u64&gt;\nString\ntype of callee::transfer\nThe runtime would then check both types for equality before execution.\nThis avoids the use of GUIDs and brings it more in line with regular dynamic linking.\nThe drawback of this is that it uses more space, as the caller ELFs will now have to store copies of the BTF. I would expect the BTF footprint to be negligible though unless devs make excessive use of templates.\n\n[toly]: Security issues aren’t a concern because the callee never trusts the caller and has to validate all inputs.\nIn the BTF approach the runtime does that at link time. I generally think it’s the better option, but we need an actual design for the conventions we want programs to use. Something needs to do the dispatching from a wallet signed message string to the public entry points.\n\n[alessandrod]: \"ripatel-jump:\nHow about the following?\nAdjust the compiler to generate BTF for all public and externally linked entrypoints\nAdjust the compiler to generate BTF for all imported entrypoints\nWhen generating BTF for a function, also generate BTF for all transitive types (the types of the function arguments and the return type, recursively)\n[snip]\nThe drawback of this is that it uses more space, as the caller ELFs will now have to store copies of the BTF. I would expect the BTF footprint to be negligible though unless devs make excessive use of templates.\nThis is all already planned and even mostly implemented in LLVM: emitting BTF for a type recursively triggers BTF emission of all the referenced types - this includes function prototypes and definitions. The footprint is indeed negligible - BTF for the whole linux kernel (millions of LOC) is 4.5MB today. Also since BTF is only emitted for types reachable from public entrypoints - and not emitted for unused types - even depending on crates with a large API surface like solana_program won’t significantly impact ELF size.\nFor CPI, the idea is that this will work completely transparently. There’s no explicit dynamic dispatch or discovery needed. We’re planning to teach rustc, cargo and the linker to use the type info generated when compiling programs, so you’ll be able invoke a callee program just like any other function, you’ll get compile time errors if you try to misuse something etc. Compile time errors are just for improved developer experience, but obviously the runtime will still not trust the resulting bytecode.\nAt load time then the program runtime will resolve links, apply BTF relocations and enforce whatever security constraints can be enforced based on type info. Higher level properties that can’t be expressed via the type system will be enforced at runtime.\nHaving said all that, as @toly pointed out we do still need a way to invoke entrypoints from a tx, so we do need a &lt;something&gt; =&gt; &lt;entrypoint&gt; mapping. Since the names of public entrypoints will likely not be mangled - the rust mangling scheme is not stable yet and we need to interop with C and one day move programs too - could we use symbol names? If we can use symbol names then essentially we don’t have to do anything special in the runtime, we already have a symbol table so we can just lookup.\n\n[toly]: So with Runtime V2 something like this should work\n///token.rs\n///Token Transfer interface\nstruct Token {\n authority: Pubkey,\n balance: u64\n};\ntrait Token {\n fn transfer(from: &amp;mut Token, to: &amp;mut Token, amount: u64) -&gt; Result&lt;()&gt;\n}\nthen the implementation can look like this\n///mytoken.rs\n///my token implementation, counts the number of balance transfers\nstruct MyToken {\n token: Token,\n counter: u64,\n}\nimpl Token {\n fn transfer(from: &amp;mut Token, to: &amp;mut Token, amount: u64) {\n let from: &amp;mut Account = to_account(from);\n let from: &amp;mut MyToken = from_account(from);\n from.counter += 1;\n from.token.transfer(to, amount);\n }\n}\n\n[ripatel-jump]: @toly would PRv2 support dynamically dispatching this trait? This looks very interesting. How would we address (lack of) ABI stability in rustc v1.x.x?\n\n[toly]: It should be equivalent to a global extern function in C.\nDo you mean dispatching from the transaction message processor or between programs? I think the tricky part will be figuring out which token implementation gets called when more then one is present.\nI think we will need to be able call the extern functions from the program object.\n\n[splintr]: I’d like to suggest that interface GUID’s should be the first 128 bits of a hash of the specification detailing what accounts/data the interface expects. This convention should prevent contention/confusion over specific ID’s (ie low # ID’s).\nAdditionally, I’d like to note that there is a need for data interfaces. For example, for any ownable object, it should be possible to determine the owning address. In different implementations that owner may be stored at different offsets into an account. It would be a huge burden on indexers/applications to require that these offsets be found manually. Additionally, a single program may have multiple different account types, so these offsets might be different within a single program.\nI propose a solution in two parts: Account discriminators and an offsets section within the ELF. The idea would be that, within the binary, each different discriminator would be followed by a list of interface-offset pairs, designating the location within each account that the interface’s data can be found.\nNote: If I remember correctly, some current implementations vary account type via account size. These implementations will need to be manually grandfathered in by indexers. In fact, since these implementations already exist and are indexed, it would require minimal work. However, all future programs would need to adhere to the discriminator system.\n\n[cavemanloverboy]: if I understand correctly, it seems to me like this goes against last year’s trend of composability (since everyone has their own impl of a primitive), will contribute to chain bloat, and opens up a can of worms re: vulnerability.\nWhy are these issues not a concern?\nHow big is the byte code for each interface implementation (e.g., for these simple transfers)?\nAre there any guarantees about state transitions that can be provided beyond Rust/Move aliasing rules? For example, instead of a &amp;mut self, can we mark only part of state as mutable for a given implementation? Also, as another example, can we provide default implementations for particular methods that cannot be overwritten.\nTo illustrate this last point, consider another take on your last example:\nstruct Token {\n authority: Pubkey,\n balance: u64\n};\ntrait Token {\n fn transfer_hook(from: &amp;mut Token, to: &amp;mut Token, amount: u64) -&gt; Result&lt;()&gt;\n \n #[immutable]\n fn transfer(from: &amp;mut Token, to: &amp;mut Token, amount: u64) -&gt; Result&lt;()&gt; {\n let to_account: &amp;mut MyToken = to_account(to);\n let from_account: &amp;mut MyToken = from_account(from);\n from_account.token.transfer(to_account, amount);\n transfer_hook(from, to, amount)\n }\n}\nand\nstruct MyToken {\n token: Token,\n counter: u64,\n}\nimpl Token for MyToken {\n fn transfer_hook(from: &amp;mut Token, to: &amp;mut Token, amount: u64) {\n let from: &amp;mut MyToken = from_account(from)\n from.counter += 1;\n }\n}\nwhich provides the same functionality while providing a base implementation that need only be audited once.\n\n",
            "comment_count": 10,
            "original_poster": "toly",
            "activity": "2023-06-09T04:11:10.671Z"
        },
        {
            "id": 246,
            "title": "sRFC 00014: Rethinking SPL Token",
            "url": "https://forum.solana.com/t/srfc-00014-rethinking-spl-token/246",
            "created_at": "2023-05-16T10:54:29.986Z",
            "posts_count": 7,
            "views": 1959,
            "reply_count": 4,
            "last_posted_at": "2023-06-07T15:34:07.370Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "sRFC 00014: Rethinking SPL Token\nSummary\nThis sRFC highlights a critical issue with the current implementation of the SPL token program: the tightly coupled interface and implementation. This results in performance degradation and significant barriers to innovation. To overcome these challenges, the sRFC proposes a modular token program that offers flexible functionality while maintaining security.\nProblem\nOne of the biggest differences between the SVM and the EVM is the SVM’s separation of state and code. This mechanism is useful because it enables parallel processing of transactions. However, it has also had a less-desirable consequence: tightly coupling interfaces and implementations.\nThis issue is particularly relevant to Solana tokens. Unlike EVM chains with a standard ERC20 interface and multiple implementations, Solana requires all projects to utilize the same shared implementation for token adoption within the ecosystem. Consequently, all token instructions flow through a common codepath. This presents two major drawbacks:\nPerformance degradation due to ‘lowest common denominator’ approach: The shared implementation must accommodate all possible token features, even if they are unnecessary for certain tokens. As a result, token instructions often traverse unnecessary codepaths, leading to performance inefficiencies and increased data storage requirements. For example, all transfers check if the source and destination accounts are frozen, even if a token doesn’t need / isn’t using the freeze functionality. Further, 66 bytes of the 75-byte mint account (88%) and 88 bytes of the 136-byte token account (65%) are only used for a subset of tokens. Given the mass-market adoption that Solana aims to achieve, these differences could add up to tens of millions of dollars of added cost to Solana end-users.\nRaised barriers to innovation: While the Token-2022 initiative aims to add more features, it fails to address the core problem of inhibiting permissionless innovation. Many projects on Ethereum, such as MakerDAO with DAI, Compound with cTokens, OlympusDAO with OHM, and others, have required custom token implementations tailored to their specific needs. The inability to create alternative token programs easily limits experimentation and adoption within the Solana ecosystem.\nOpaque governance: As the token program is singleton and unpredictable features are expected, it must be upgradeable. Currently, the governance of the token program is driven by Solana Labs, with validators theoretically having the ability to prevent an upgrade. However, this arrangement raises concerns about centralization and the lack of practical oversight by validators.\nSolutions\nTo address these issues, several potential solutions are proposed:\nWrapper contract\nOne idea discussed by @joncinque is the utilization of a wrapper contract that automatically freezes token accounts. Additional logic can be implemented in this wrapper contract, imposing it on users during token transfers.\nHowever, this approach does not sufficiently address the problem of permissionless innovation, as widespread support for custom wrappers from various applications and wallets is unlikely.\nChange the runtime to allow fine-grained control over CPIs\nEnhancing the Solana runtime to provide developers with fine-grained control over Cross-Program Invocations (CPIs) is another solution. Developers could specify specific access rights for callees, such as restricting the ability to pass signed accounts to other programs via CPIs. Similar to Linux’s containerization tools, these runtime additions would offer enhanced security and control.\nA modular token program\nThe advocated approach in this sRFC is the adoption of a modular token program. This program would allow anyone to create a token handler program and register it with the main token program. The token program would then pass along all calls to the relevant handler (this is defined at the mint), taking care to never pass along signed user accounts. It would do so through two means:\nUpon receiving an initialize_x call, such as initialize_token_account, it would pre-allocate any accounts and pass ownership to the handler. This way, users can prevent passing any account initialization ‘payer’ to the handler.\nUpon receiving a call where someone needs to be authorized (e.g., from.authority, in the case of a transfer), the token program would authorize the user on behalf of the handler and pass the handler a different signed account to signify that the relevant user has signed.\nA proof-of-concept of such a program can be found here.\nOpen questions\nShould a token handler be able to specify extra accounts required for basic instructions like transfers? If so, how should this be standardized?\nHow important is backwards-compatibility, and what steps can be taken to ensure compatibility with the existing SPL token? How would migration be facilitated?\nAre there any security vulnerabilities in this design, and if so, how can they be addressed?\nConclusion\nThis proposal introduces a method for Solana developers to create new token mechanisms while preserving end-user security. Feedback and questions within this forum are greatly appreciated, particularly from esteemed SPL contributors such as @joncinque, criesofcarrots, and mvines.",
            "comments": "[joncinque]: Thanks for bringing this up and thinking so much about the problem. Certainly, the lack of composability with token programs is a huge hindrance to open development in the Solana ecosystem, and I would love to see a better solution than the current monolith of Tokenkeg....\nI view this in a very similar way, but rather than having everything go through a centralized program, I’ve always preferred simply having programs that implement many interfaces. For example, Tokenkeg... can really be broken down into a program that contains many different interfaces:\ntransferable: transfer and transfer_checked\nmintable: mint_to\nfreezable: freeze and thaw\ncloseable: close_account\napprovable: approve and revoke\nburnable: burn\ninitialize mint / account\nIf we can write and implement program interfaces for each of these, then we can re-compose everything. sRFC 00010: Program Trait - Transfer Spec is the first step towards this future.\nIn the interface specs, we can allow for an arbitrarily more accounts, to be implemented through an instruction, or through some sort of lookup account as in the “transfer-hook” interface created for token-2022 https://github.com/solana-labs/solana-program-library/pull/4147.\nWe also need to figure out “state interfaces”, ie for defining initialize_mint and initialize_account, similar to your example, but doing it through a spec / interface rather than a centralized program.\nWhat do you think about this interface approach?\n\n[metaproph3t]: This interface approach makes a lot of sense and pairs nicely with Solana’s use of Rust.\nRegarding state interfaces, I’ve pondered potential solutions. One idea is to incorporate a preflight function that returns a structure resembling:\n[\n {\n field_name: \"mint_authority\",\n type: Pubkey,\n },\n {\n field_name: \"supply\",\n type: u64,\n },\n {\n field_name: \"freeze_authority\",\n type: Option&lt;Pubkey&gt;,\n }\n]\nThe calling program could then match the field names with its own knowledge (e.g., desiring an Alice mint_authority and a supply of 1000), use None for unknown Option fields, and trigger a revert if encountering non-optional, unknown fields.\nHowever, my primary concern with the interface approach, unless mediated by a program like the one I’ve developed, lies in security. How can we ensure implementors don’t misuse signed inputs? This becomes crucial, especially considering implementors can request additional accounts. Without safeguards, a malicious program might grief the user by creating a 1KB account when it only needs 100 bytes. Couldn’t a program also use the pre-flight mechanism to request a legitimate token program and the user’s token account at that program, thus allowing them to steal the user’s legitimate tokens?\nTo some extent, user wallets simulate transactions to prevent such risks (e.g., identifying a transaction attempting to steal SOL and aborting it). However, there are ways to bypass these protections, such as malicious codepaths dependent on semi-random events (e.g., stealing funds if wallclock time % 1000 == 0). We could argue that it’s the user’s responsibility to verify the code they interact with, but that weakens Solana’s value proposition, as EVM users, for example, don’t face similar concerns when purchasing tokens on Uniswap.\nThus, it appears necessary to introduce a mediator between the client and implementor, preventing the client’s signed account from leaking through. I would appreciate your thoughts on this matter.\n\n[joncinque]: These concerns are definitely all valid, and could be a model that’s used on top of interfaces, a sort of “safe-interface-wrapper” program that’ll enforce all of the correct signer / writable flags on accounts, downgrade signers, and CPI to the next program.\nWith the model you’re proposing, since you don’t want a signature to propagate down, then you’ll have to also provide some signed PDA from the interface wrapper program to ensure that this is a “valid” call to the program, which may be a bit restrictive.\nRather than restricting program design, I’d prefer to make the interfaces well-designed, the wallets to catch potentially dangerous situations, and for everyone to make heavy use of token delegates.\nFor example, you should never send an instruction that requires your wallet to be signer and writable, along with the system program. This is the current pattern with PDA creation, but it stinks! The program should only allocate + assign the PDA, and your wallet can do a direct system transfer of the required lamports to the PDA at the top-level of the transaction, so only the system program gets your wallet as a signer.\nIf an interface needs to create a PDA from a wallet, it’s very risky for the reasons that you’ve mentioned, and should not be used.\nFor tokens, the best option is to use the CPI guard extension on token-2022 solana-program-library/instruction.rs at 8f9c33b3a04250938a573809cd9dfdb698025972 · solana-labs/solana-program-library · GitHub\nBut otherwise, wallets / clients should always use delegates when transferring tokens. A client should never sign a transaction containing an instruction to a program that requires an owner’s signature, their token account, and the token program. Unless that’s the token program, of course.\nIf an interface requires these things, it should also be changed. And wallets can catch if there’s a potentially risky set of accounts in an instruction.\nOr we can consider expanding the runtime / transaction format to “scope” signatures so they can’t go past a few levels. Bad actors can abuse the privilege extension feature for Cross-Program Invocations via system_instruction::transfer, spl_token::instruction::approve, spl_token::instruction::transfer · Issue #17762 · solana-labs/solana · GitHub has some interesting ideas on that.\n\n[metaproph3t]: Thank you for your thoughtful response, sire. I am also not yet convinced that the solution I’ve presented here is the best one.\n@ngundotra maybe want to offer your counsel as well, given your commendable work on sRFCs 2, 3, and 10? @joec as well, given your leadership of Nautilus and your involvement in the interfaces project?\nFor the purposes of clarity, I am going to enumerate the options that have emerged from this discussion thus far.\nOption 1: Status quo\nIn the status quo option, users would continue passing in signed accounts directly to programs. For example, if someone wanted to submit an order to a CLOB, they would need to directly pass their signed token account, which the CLOB would in turn pass to a token program so that the CLOB may claim the user’s funds.\nCLOB-status-quo699×131 7.02 KB\nHere, a user needs to trust every program that they pass their signed account into. Hence, in order to remain secure, the user would need to do due diligence on programs.\nPrograms, on the other hand, could adopt a more relaxed posture, since a PDA generally only has control over 1 asset (e.g., a quote_vault of a CLOB holding only quote tokens, so that even a malicious token program wouldn’t be able to steal their other assets). Still, programs would need to ensure that their PDAs are not drained of lamports, which could be done like so:\nlet pda_balance_before = pda.balance;\nsolana_program::invoke_signed(/* CPI here */);\nassert!(pda.balance == pda_balance_before);\nOption 2: Discourage passing in signed accounts\nAnother option is to discourage passing signed accounts into programs. In the CLOB example, a user would first delegate an amount to the CLOB before their trade, and then the CLOB could pull the funds even without the user’s signed account. This is analogous to the EVM approve and transferFrom combination, although this could be done in a single transaction because Solana transactions can contain multiple instructions.\nTo create accounts, the program would Allocate and Assign the account, and the user would Transfer lamports to the account in a separate instruction.\nOf course, the user would still need to trust the token program they interact with and any other programs that require an account to authorize itself (e.g., an NFT program).\nOption 3: Proxy\nThis is what I originally proposed in this sRFC. Interfaced programs would sit behind a proxy that allocates accounts on behalf of the user and signs on behalf of the user.\nproof of concept code\nOption 4: Runtime changes\nAllow users to sandbox their program invocations.\nSome ideas here include:\nAllow users to call programs in a way that the program can see the user’s signed account, but it can’t pass it along (this is analagous to msg.sender in the EVM)\nLimit how many lamports can be transferred out of an account in an instruction\nFurther ideas discussed here: Bad actors can abuse the privilege extension feature for Cross-Program Invocations via system_instruction::transfer, spl_token::instruction::approve, spl_token::instruction::transfer · Issue #17762 · solana-labs/solana · GitHub\n\n[metaproph3t]: I personally am unconvinced that any of these approaches is ideal.\nOption #1 is possibly the worst one, as it requires users to completely trust programs that they interact with. As a result, users will naturally limit their interactions to a small set of programs.\nI consider option #2 a step-function improvement over option #1, but insufficient by itself. In the CLOB example, one must still trust the token program, even if one may not need to trust the CLOB program. In the EVM, on the other hand, you needn’t place such a high degree of trust in token programs.\nAt first, I considered option #3 estimable (hence the sRFC ), but it also seems insufficient. In the CLOB example, one would still need to pass one’s signed account into the CLOB, even if it will never reach the token program. Doing every CPI through a proxy would also double the number of CPIs, which is likely to remain expensive until runtime v2.\nOption #4 seems the least poor choice. However, I am not well-versed enough in the validator codebase to determine what can be introduced without adding performance degradation.\n\n[ngundotra]: metaproph3t:\nHowever, my primary concern with the interface approach, unless mediated by a program like the one I’ve developed, lies in security. How can we ensure implementors don’t misuse signed inputs? This becomes crucial, especially considering implementors can request additional accounts.\nThe solution to this is to open-source programs, and verify that the code compiles to the program executable that we see on-chain. There’s on going work funded by Foundation to make this an easy-to-use tool.\nRealistically, I think we’ll end up in 2 different worlds with some amount of interoperability.\n1 - Tokenkeg + Token22 programs\n2 - ERC 721 world where each program is its own token with innovative or malicious rules around how it requests accounts and transfers balance\nWith Token22’s being the bridge between the two worlds (cc @joncinque).\n metaproph3t:\nA modular token program\nThe advocated approach in this sRFC is the adoption of a modular token program. This program would allow anyone to create a token handler program and register it with the main token program. The token program would then pass along all calls to the relevant handler (this is defined at the mint), taking care to never pass along signed user accounts. It would do so through two means:\nI think this is a great idea, and would love to see more research here. @austbot came up with a similar program structure he called Digital Asset Spec that has the same spirit: shared state, modular programs control specific logic (royalty payout, transfer rules, trait swaps, etc), and a slim program that defines this lifecycle.\nIf you continue to build this out, I’d recommend pursuing 2 checkpoints:\nCan you build a marketplace that swaps / lists assets built with your modular token interface?\nCan you index assets appropriately given your token interface?\nThanks for your research so far @metaproph3t !\n\n",
            "comment_count": 6,
            "original_poster": "metaproph3t",
            "activity": "2023-06-07T15:34:07.370Z"
        },
        {
            "id": 268,
            "title": "sRFC 00016: Generalized Ownable Indexable Assets",
            "url": "https://forum.solana.com/t/srfc-00016-generalized-ownable-indexable-assets/268",
            "created_at": "2023-05-24T14:48:18.095Z",
            "posts_count": 3,
            "views": 548,
            "reply_count": 1,
            "last_posted_at": "2023-05-29T14:35:49.036Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Generalized Ownable Indexable Assets\nSummary\nThis is a standard protocol to enables RPCs to track assets that programs to create, replace, update, and delete for users. Indexers will be able to interpret “owned” program data by executing a pre-determined simulated view function on the program.\nMotivation\nWe want to support forking of commonly used protocols, so that big enterprises can have full control and maintainability over their contracts, while also retaining the ability to have their program’s data indexed &amp; shown in wallets.\nCentralized control of commonly used protocols leads to catastrophic failure scenarios when the trusted operator is compromised. Short term work-arounds in previous upheavals, e.g. the genesis of OpenBook, have failed to produce meaningful abstraction patterns on Solana. As developers on Solana begin to realize the risks of centralized protocols, they may want to fork even the SPL Token program.\nHowever they will quickly run into issues getting adoption into marketplaces, dApps, and wallets. This is both a social and technical problem. We hope that this standard for indexing will solve the technical problem, and thus reduce the tension in the social problem of adoption.\nSpecification\nRequire the usage of CPI events. See sRFC #00013.\nWe propose that programs control ownable assets for wallets using the following 4 CRUD event structs.\nThe payload of these events is used to manage an Asset which has an ID of type Pubkey, and consists of an ordered array of Pubkeys.\n// Inform indexers that a new Asset Group was created for an authority\npub struct CrudCreate {\n asset_id: Pubkey,\n authority: Pubkey,\n pubkeys: Vec&lt;Pubkey&gt;,\n data: Vec&lt;u8&gt;\n}\n// Inform indexers to change both authority &amp; pubkeys\npub struct CrudReplaceKeys {\n asset_id: Pubkey,\n authority: Pubkey,\n pubkeys: Vec&lt;Pubkey&gt;\n}\n// Inform indexers to update the bytes for an asset group\npub struct CrudUpdateBytees {\n asset_id: Pubkey,\n owner: Pubkey\n}\n// Inform indexers to delete the asset \npub struct CrudDestroy {\n asset_id: Pubkey\n}\nIndexers will store assets issued by programs, so that you will always be able to query /getAssetsForOwner { program_id, wallet } and have it return a list of Asset { id: Pubkey, pubkeys: Vec&lt;Pubkey&gt;, data: Vec&lt;u8&gt;}.\nOptionally, indexers can ask the program for a human readable interpretation of the Asset’s data by simulating view functions on the program that it belongs to. This is done by sending the getAssetData anchor instruction with no arguments, and deserializing the return_value as a JSON.\nPrograms that comply with this spec will have similar implementation as below:\n#[program]\npub mod my_program {\n ...\n pub fn get_asset_data(ctx: Context&lt;GetAssetData&gt;, data: Vec&lt;u8&gt;) -&gt; Ok(()) {\n let asset_id_account = &amp;ctx.accounts.asset_id.to_account_info();\n let asset_accounts = &amp;ctx.remaining_accounts.to_vec();\n // interpret asset data\n let my_json_bytes = // serialize asset data here\n set_return_data(my_json_bytes)\n Ok(())\n }\n}\n#[derive(Accounts)]\npub struct GetAssetData&lt;'info&gt; {\n /// CHECK:\n pub asset_id: AccountInfo&lt;'info&gt;,\n /// CHECK:\n pub authority: AccountInfo&lt;'info&gt;,\n}\nThis will allow indexers to serve JSON data from each Asset that the program has issued.\nImplementation:\nTBD",
            "comments": "[nickfrosty]: Does this concept require indexers to all be running an implementation of the Digital Asset Standard API (set forth by Metaplex)?\nOr would there be a more generalized implementation of this endpoint/view function calling getAssetsForOwner and getAssetData that gets baked into RPC nodes/indexers?\n\n[ngundotra]: There would be a more generalized version of the indexer that supports those two RPC calls (getAssetsForOwner and getAssetData).\n\n",
            "comment_count": 2,
            "original_poster": "ngundotra",
            "activity": "2023-05-29T14:35:49.036Z"
        },
        {
            "id": 51,
            "title": "sRFC 00006: Writing SVG Images as PDAs of a Solana Program to implement On-chain images for NFTs",
            "url": "https://forum.solana.com/t/srfc-00006-writing-svg-images-as-pdas-of-a-solana-program-to-implement-on-chain-images-for-nfts/51",
            "created_at": "2023-03-21T15:07:05.314Z",
            "posts_count": 9,
            "views": 985,
            "reply_count": 6,
            "last_posted_at": "2023-04-08T16:42:08.534Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "This proposal discusses storing image data in base64 SVG format directly to a Solana address, using a data URL header to make it natively readable by browsers. The image storage account is a PDA generated with the NFT token pubkey as a seed, making it easy to retrieve the image knowing just the programID and the NFT token pubkey.\nShowcased below is the code used to implement Blockrons on-chain images, open sourced by me under CC1 (just give a thanks/attribution or something)\n1. Main program - Lib.rs (written in solpg)\nuse anchor_lang::prelude::*;\ndeclare_id!(\"BLKRNygNc7mWMpxQPXs4UoVPyApvyqy4v8uK9F3iJmqS\");\n#[program]\n// Smart contract functions\npub mod imager {\n use super::*;\n // creates the image storage account\n pub fn create_imager(ctx: Context&lt;CreateImager&gt;,token: Pubkey) -&gt; Result&lt;()&gt; {\n let imager = &amp;mut ctx.accounts.imager;\n imager.authority = ctx.accounts.authority.key();\n imager.token = token;\n imager.img = (\"\").to_string();\n Ok(())\n }\n // puts data inside the image storage account by concatenating strings &lt;- client-side to fit the solana txn limit\n pub fn update_imager(ctx: Context&lt;UpdateImager&gt;,token: Pubkey, image: String) -&gt; Result&lt;()&gt; { \n let imager = &amp;mut ctx.accounts.imager;\n imager.authority = ctx.accounts.authority.key();\n imager.token = token;\n imager.img = format!(\"{}{}\", imager.img, image); \n Ok(())\n }\n}\n// Data validators\n//token input (nft token addy) + programID are the main seeds for PDA\n#[derive(Accounts)]\n#[instruction(token: Pubkey)]\npub struct CreateImager&lt;'info&gt; {\n#[account(mut)]\n authority: Signer&lt;'info&gt;,\n #[account(\n init_if_needed,\n seeds = [token.as_ref()],\n bump,\n payer = authority,\n space = 10000\n )]\n imager: Account&lt;'info, Imager&gt;,\n system_program: Program&lt;'info, System&gt;,\n}\n#[derive(Accounts)]\npub struct UpdateImager&lt;'info&gt; {\n authority: Signer&lt;'info&gt;,\n #[account(mut, has_one = authority)]\n imager: Account&lt;'info, Imager&gt;,\n}\n// Data structures\n#[account]\npub struct Imager {\n authority: Pubkey,\n token: Pubkey,\n img: String,\n}\n^^On-chain program is simple and only uses 2 functions - 1. create the PDA and 2. store data to the PDA thru string concatenation with multiple txns (to bypass solana’s byte txn limit)\n2. Client side implementation - also deployed using solpg:\nconst systemProgram = anchor.web3.SystemProgram;\n// EDIT THIS: token = public nft account addy you want to tie your on-chain image to\nconst token = new web3.PublicKey(\"NFT PUBKEY HERE\");\n// EDIT THIS: dataS = string of the base64 info\nconst dataS = new String(\"BASE64 SVG DATA HERE\");\n// do not touch, automatically divide dataS into strings to split transactions\nvar string1 = new String(dataS.slice(0,800));\nvar string2 = new String(dataS.slice(800,1600));\nvar string3 = new String(dataS.slice(1600,2400));\nvar string4 = new String(dataS.slice(2400,3200));\nvar string5 = new String(dataS.slice(3200,4000));\nvar string6 = new String(dataS.slice(4000,4800));\nvar string7 = new String(dataS.slice(4800,5600));\n// program logic\n const [imagerPubkey, _] = await anchor.web3.PublicKey.findProgramAddress(\n [token.toBytes()],\n pg.program.programId\n );\n console.log(\"Your imager address\", imagerPubkey.toString());\n// create image storage account\n const [imager, _imagerBump] =\n await anchor.web3.PublicKey.findProgramAddress(\n [token.toBytes()],\n pg.program.programId\n );\n const tx = await pg.program.methods\n .createImager(token)\n .accounts({\n authority: pg.wallet.publicKey,\n imager: imager,\n systemProgram: systemProgram.programId,\n })\n .rpc();\n// transact all 7 string parts \n const tx1 = await pg.program.methods\n .updateImager(token,string1.toString())\n .accounts({\n imager: imagerPubkey,\n })\n .rpc(); \n const tx2 = await pg.program.methods\n .updateImager(token,string2.toString())\n .accounts({\n imager: imagerPubkey,\n })\n .rpc();\n const tx3 = await pg.program.methods\n .updateImager(token,string3.toString())\n .accounts({\n imager: imagerPubkey,\n })\n .rpc();\n const tx4 = await pg.program.methods\n .updateImager(token,string4.toString())\n .accounts({\n imager: imagerPubkey,\n })\n .rpc();\n const tx5 = await pg.program.methods\n .updateImager(token,string5.toString())\n .accounts({\n imager: imagerPubkey,\n })\n .rpc();\n const tx6 = await pg.program.methods\n .updateImager(token,string6.toString())\n .accounts({\n imager: imagerPubkey,\n })\n .rpc();\n const tx7 = await pg.program.methods\n .updateImager(token,string7.toString())\n .accounts({\n imager: imagerPubkey,\n })\n .rpc();\n \n console.log(\"Done!\"); \n// displays current data \n// console.log(\"Your imager\", imager);\n^^Client side program is a little bit more complex as it is built to breakdown a long string into 7 diff txns of 800 bytes each - to fit solana’s txn limit. Currently handles for strings upto 5600 bytes but of course this can be extended until the PDA max of 10,000 bytes.\nTypically, each Blockron image is 32x32 pixels and takes up 3-4kb of space.",
            "comments": "[Hamster]: Example of a data stored inside the string, inside the variable dataS:\ndata:image/svg+xml;base64,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\nThis is an SVG file in base 64 format, once you copy this entire string and paste it into a new browser window, it will automatically resolve into an image.\n\n[blockiosaurus]: I like this! And I think a base64 image data image string nested in a JSON file would be the way to go, or at least the easiest to implement. As far as I can tell this doesn’t need to be a contract change. If we just check the protocol format (https:// or solana://) and use a getAccount instead of a fetch in the Metaplex JS SDK then it should work perfectly. Both already return buffers so interpretation would work the same way.\n\n[Hamster]: Appreciate the comment, what would the JSON file look like? It will be directly stored in a solana account as a json variable?\nThen inside the JSON, can we put another solana address for the image link? or should that be the image string in data URL format directly?\nThat way we can reduce the complexity to 1 solana account but limit the image size further.\n\n[blockiosaurus]: It would just be the standard JSON metadata. All Metaplex NFTs have an on-chain URI which points to the JSON file, which points to the image URI. We’d just be replacing the image URI with a data:image string instead of a pointer to another account. You don’t even need a special program or anything, you just encode the JSON bytes directly in the Solana account.\n\n[Hamster]: blockiosaurus:\nyou just encode the JSON bytes directly in the Solana account.\nHow do you do this? Have not seen anything in solana documents all the tutorials I can find are how to put a counter variable on-chain lmao. It took a lot of testing to even get a string working\n\n[blockiosaurus]: I guess you’d still have to write a program to do the actual write, but strings are just series of bytes so when you write the account (assuming non-anchor) you shove them directly into the account info data.\n\n[Hamster]: Okay doing some testing to store the json object, on-chain. First, we create a suitable JSON that has no off-chain or exterior links.\n{\"name\":\"Blockrons 020\",\"symbol\":\"BLKRNS\",\"description\":\"Each Blockrons NFT has its art asset stored directly on-chain in an image container address. \\n\\nBlockrons 020 - \\\"Nomad\\\" \\n\\nImage container address: 4kHmoHh6FYp4ae8a3Nj5N64hbs2hB2AGfuhraCaNrGg7\",\"seller_fee_basis_points\":0,\"image\":\"data:image/svg+xml;base64,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\",\"attributes\":[],\"properties\":{\"creators\":[{\"address\":\"F1QyW2RiabaUTHYYMZs6kVQmjw3QzhRWtAJNUp6ifWAe\",\"share\":100}]}}\nAbove is the json file used by the Blockrons 020 nft, solscan here: Solscan. We can successfully use url-data formatted base64 svg code as an image instead of a file link - it is read normally by browsers and wallets.\nNext step is to put this json on-chain and make it readable. Lemme try by storing it as a basic string if that works, should still be readable on buffer\n\n[Hamster]: Okay stored the whole JSON as a string into this pda account:\n4kHmoHh6FYp4ae8a3Nj5N64hbs2hB2AGfuhraCaNrGg7\nOn typescript clients, if you use connection.getAccountInfo(key), it should be able to show up in the read buffer data.\nWould this work with:\n blockiosaurus:\n(https:// or solana://) and use a getAccount\n?\n\n",
            "comment_count": 8,
            "original_poster": "Hamster",
            "activity": "2023-04-08T16:42:08.534Z"
        },
        {
            "id": 206,
            "title": "sRFC 00012: Wallet Delegation Standard for Secure Proof of Ownership",
            "url": "https://forum.solana.com/t/srfc-00012-wallet-delegation-standard-for-secure-proof-of-ownership/206",
            "created_at": "2023-05-07T11:06:05.612Z",
            "posts_count": 4,
            "views": 1405,
            "reply_count": 1,
            "last_posted_at": "2023-05-08T20:41:29.963Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Wallet Delegation Standard\nSummary\nThis RFC introduces a delegation standard to mitigate loss, theft, and unauthorized access to funds stored in cold wallets. The standard maintains the offline nature of cold wallets, allowing them to delegate ownership of assets to dedicated hot wallets, thus allowing secure and controlled proof of ownership of assets.\nMotivation: Proof of Ownership\nProof of ownership is a critical yet highly neglected problem in the Solana ecosystem. In its current form, it’s highly insecure\nusers must connect their cold wallets to unverified websites, or\ntransfer their valuable assets to warm wallets\nGoals\nThis standard aims to remedy this by providing a secure way to delegate assets from cold to hot wallets. This allows users to prove ownership of their assets without risking them by connecting to insecure websites or moving them around.\nBackground\nDelegation\nDelegating an asset from a cold wallet → hot wallet: The hot wallet is shadowing as the owner of the asset, and the hot wallet does NOT own any write/transfer rights over this asset, nor does it have the ability to transfer these rights to anyone else. The cold wallet remains the true owner of this asset on-chain.\nActions\nTwo main actions can be performed using the proposed protocol:\nDelegation\nRevoking Delegation\nDelegation Types\nFull Wallet Delegation: The entire wallet gets delegated, including all assets that are owned by the wallet.\nToken Delegation: Tokens having the same mint address can be delegated. This includes fungibles, semi-fungibles, and non-fungibles.\nProposed Implementation\nPDAs can be used to derive delegation accounts for full wallet or individual token Delegation.\nDelegationAccount\nTo delegate wallet A to another wallet B, a DelegationAccount can be derived using the address of wallet A.\n#[account]\npub struct DelegateAccount {\n // Cold Wallet\n pub authority: Pubkey,\n pub hot_wallet: Pubkey,\n}\nwhere seeds for DelegateAccount:\n[\n DelegateAccount::PREFIX.as_bytes(),\n authority.key().as_ref(),\n]\nTokenDelegationAccount\nIn the case of token Delegation, i.e., delegating a token in wallet A with ATA (associated token account) T to wallet B, a TokenDelegationAccount can be derived using the address of wallet A and the ATA T.\n#[account]\npub struct DelegateTokenAccount {\n pub authority: Pubkey,\n pub hot_wallet: Pubkey,\n pub token_account: Pubkey,\n}\nwhere seeds for DelegateTokenAccount:\n[\n DelegateTokenAccount::PREFIX.as_bytes(),\n authority.key().as_ref(),\n token_account.key().as_ref(),\n]\nCustos Delegatation Protocol1434×1116 54.2 KB\nRevoking Delegations\nThe authority of the Delegation can revoke delegations by closing the PDA.\n#[derive(Accounts)]\npub struct RevokeDelegate&lt;'info&gt; {\n #[account(mut)]\n pub authority: Signer&lt;'info&gt;,\n #[account(mut, has_one=authority, close=authority)]\n pub delegate_account: Account&lt;'info, DelegateAccount&gt;,\n}\n#[derive(Accounts)]\npub struct RevokeTokenDelegate&lt;'info&gt; {\n pub authority: Signer&lt;'info&gt;,\n #[account(mut, has_one=authority, close=authority)]\n pub delegate_token_account: Account&lt;'info, DelegateTokenAccount&gt;,\n}\nFetching Delegations\nThis standard would require an SDK for client-side fetching delegated wallets. When a user connects their wallet, the SDK can fetch all the assets delegated to the given (connected) hot wallet. Here is a reference implementation of the SDK:\ngetUserDelegates = async (hotWallet: PublicKey) =&gt; {\n const data = await this.delegationProgram.account.DelegateAccount.all([\n {\n memcmp: {\n offset: 8 + 32,\n bytes: hotWallet.toBase58(),\n },\n },\n ]);\n return data;\n};\ngetTokenDelegates = async (hotWallet: PublicKey) =&gt; {\n const data = await this.delegationProgram.account.DelegateAccount.all([\n {\n memcmp: {\n offset: 8 + 32,\n bytes: hotWallet.toBase58(),\n },\n },\n ]);\n return data;\n};\nPotential Use-Cases\nClaiming Airdrops\nClaiming Allowlists\nToken-gated mints\nToken-gated games\nDAO Governance\nOn-chain reputation\nVerifiable Wallet aggregation\nOngoing Investigation\nSupporting Programmable Wallets: Smart contract wallets like Multisigs are not system accounts; hence, the current implementation for the standard can be changed to support PDAs that represent programmable wallets.\nAllowing tightly scoped rights to the delegated hot wallet for making transactions on behalf of the cold wallet.\nDelegation Expiration: Current implementation requires users to manually revoke a delegation if they no longer want a given hot wallet to be the Delegation for a cold wallet. Automatic expiration could be introduced to let users set a dedicated interval, after which the Delegation/TokenDelegationAccount will be closed.\nCall For Action\nReaders: Requesting readers to provide feedback, audit reference implementation, and contribute to the codebase.\nDapps: Requesting dapps to explore the SDK reference implementation and provide feedback for making integration of this standard easy.\nWallets: Requesting wallets to explore the SDK reference implementation to provide users an easy way to perform the Delegation inside the wallets themselves - connecting cold wallets to any website (even one that is created specifically for this standard) goes against the very core principle behind this standard.\nReference Implementation\nDelegation Program: GitHub - custosprotocol/custos: Custos Protocol\nJS SDK: GitHub - custosprotocol/custos-js-sdk: JavaScript SDK for Custos Protocol\nCitation\nAnvit Mangal &lt;@0xprof_lupin&gt;, Pratik Saria &lt;@PratikSaria&gt;. “sRFC 00012: Wallet Delegation Standard for Secure Proof of Ownership,” https://forum.solana.com/t/srfc-00012-wallet-delegation-standard-for-secure-proof-of-ownership, May 2023.",
            "comments": "[silo]: Why limit delegation to a single delegate?\nThere’s definitely a case to be made for having multiple delegates for a single cold wallet, especially in this mobile era.\n\n[anvit]: One to many delegation was designed for a specific reason: in use cases where a user needs to be airdropped a single token / whitelisted only once per token/wallet, many to many delegation would fail as it the delegate fetcher wont know which wallet to whitelist/airdrop/allowed to enter a token-gated website.\nBut as you said, there are use cases where many to many mappings would be beneficial.\n\n[AdeleneJennifer]: Is a single delegate to limit delegation?\n\n",
            "comment_count": 3,
            "original_poster": "anvit",
            "activity": "2023-05-08T20:41:29.963Z"
        },
        {
            "id": 122,
            "title": "sRFC 00010: Program Trait - Transfer Spec",
            "url": "https://forum.solana.com/t/srfc-00010-program-trait-transfer-spec/122",
            "created_at": "2023-04-22T22:38:28.494Z",
            "posts_count": 3,
            "views": 1069,
            "reply_count": 0,
            "last_posted_at": "2023-04-27T16:13:14.965Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Code: GitHub - ngundotra/additional-accounts-request-transfer-spec: Proof of Concept for Additional Accounts Request for Transfers\nSRFC 00010 - Program Trait - Transfer Spec\nThis spec is currently alpha and subject to change\nSummary\nA standard protocol to enable on-chain and client communication with Solana programs to “transfer” assets that allows target programs to require additional accounts.\nMotivation\nA standard protocol for enabling programs to support \"transfer\"ring assets while also allowing a flexible number of accounts into the program allows for a better user experience across apps and wallets in the Solana ecosystem.\nBy defining a protocol to resolve additional accounts required for programs to adhere to the same instruction interface, developers can build applications that are compatible with a wide range of programs.\nCalling programs should ensure that called programs are using the additional accounts appropriately, or otherwise fail instruction execution.\nDevelopers implementing this specification should be prepared to chew glass.\nBy standardizing a simple approach to solving program abstraction, we ensure basic compatibility of programs and clients so developers can focus on higher level abstractions.\nSpecification: Program Trait - Transfer\nExecuting a “transfer” instruction against a program that implements ProgramTraitTransferV1 requires two CPIs from the caller program to the callee program.\nThe first CPI from the caller to the callee is to determine which (if any) additional accounts are require for the 2nd CPI.\nThe second CPI from the caller to the callee is with the same list of accounts from the 1st call, but also passes the list of accounts requested by the first CPI.\nThe ProgramTraitTransferV1 trait requires that programs implement two instructions, described below.\nuse anchor_lang::prelude::*;\n/// Accounts required by ProgramTraitTransferV1\n#[derive(Accounts)]\npub struct ITransfer&lt;'info&gt; {\n /// CHECK:\n pub owner: AccountInfo&lt;'info&gt;,\n /// CHECK:\n pub to: AccountInfo&lt;'info&gt;,\n pub authority: Signer&lt;'info&gt;,\n /// CHECK:\n pub mint: AccountInfo&lt;'info&gt;,\n}\n#[derive(Accounts)]\npub struct MyProgramTransfer {\n /// CHECK:\n pub owner: AccountInfo&lt;'info&gt;,\n /// CHECK:\n pub to: AccountInfo&lt;'info&gt;,\n pub authority: Signer&lt;'info&gt;,\n /// CHECK:\n pub mint: AccountInfo&lt;'info&gt;,\n \n // Additional optional accounts follow here\n pub my_special_account: AccountInfo&lt;'info&gt;,\n // etc\n}\n#[program]\npub mod MyProgram {\n pub fn preflight_transfer(ctx: Context&lt;ITransfer&gt;, amount: u64) -&gt; Result&lt;()&gt; {\n // Your code goes here \n set_return_data(\n &amp;PreflightPayload {\n accounts: vec![\n IAccountMeta {\n pubkey: *my_special_account_key,\n // You cannot request additional signer accounts\n signer: false, \n // You may however request additional writable or readonly accounts\n writable: true,\n },\n ]\n }.try_to_vec()?\n )?;\n Ok(())\n }\n pub fn transfer(ctx: Context&lt;MyTransfer&gt;, amount: u64) -&gt; Result&lt;()&gt; {\n // Your code goes here\n Ok(())\n }\n}\nenum MyProgramInstruction {\n ...,\n PreflightTransfer(u64)=solana_program::hash::hash(\"global:preflight_transfer\")[..8],\n Transfer(u64)=solana_program::hash::hash(\"global:transfer\")[..8]\n}\nExecuting “transfer” against a conforming program is interactive because optional accounts may be sent in the 2nd CPI. The optional accounts are derived from the 1st CPI by Borsh deserializing return data as Vec&lt;AccountMeta&gt;.\nAccounts\nThe accounts list required for adhering to ProgramTraitTransferV1 is simply a list of account metas, that have no direct relationship to each other.\nWe overlay semantic descriptions to give advice on how this should be used, but ultimately we expect that there will be program implementations that abuse the\nsemantic descriptions.\nOwner\nisSigner: false\nisWritable: false\nThis is the owner of the asset to be transferred.\nTo\nisSigner: false\nisWritable: false\nThis is the intended recipient of the transferred asset.\nAuthority\nisSigner: true\nisWritable: false\nThis is the account that has the authority to transfer from owner to the recipient. For example, this may be the same pubkey as owner.\nMint\nisSigner: false\nisWritable: false\nThis account was included for Token* compatability.\nThis account is meant to be your implementing program’s program id, so calling programs know which program to execute.\nOr, it can be used as a token* Mint account, which allows programs to decide if they need to execute a token* CPI or a ProgramTraitTransferV1.\nInstructions\nThe instructions formats are described below\nAmount\nBoth instructions have a single parameter amount which must be serialized &amp; deserialized as a little-endian u64.\npreflight_transfer\nThis instruction’s data has an 8 byte discriminantor: [0x9d, 0x84, 0xf5, 0x5a, 0x61, 0xea, 0x7b, 0xe2], followed by u64 serialized in little-endian format.\nAnd no other bytes.\nThe accounts to this instruction are:\nvec![\n // owner\n AccountMeta {\n pubkey: owner,\n isSigner: false,\n isWritable: false,\n }\n // to\n AccountMeta {\n pubkey: to,\n isSigner: \n isWritable:\n }\n // authority\n AccountMeta {\n pubkey: authority,\n isSigner: true,\n isWritable: false,\n }\n // mint\n AccountMeta {\n pubkey: mint\n isSigner: false,\n isWritable: false\n }\n]\nReturn data for this instruction is a vector of AccountMetas, serialized as ReturnData.\n#[derive(BorshSerialize, BorshDeserialize)]\npub struct IAccountMeta {\n pub pubkey: Pubkey,\n pub signer: bool,\n pub writable: bool,\n}\npub type ReturnData = Vec&lt;IAccountMeta&gt;;\ntransfer\nThis instruction’s data has an 8 byte discriminantor: [0xa3, 0x34, 0xc8, 0xe7, 0x8c, 0x03, 0x45, 0xba], followed by u64 serialized in little-endian format.\nAnd no other bytes.\nThe accounts to this instruction are:\nvec![\n // owner\n AccountMeta {\n pubkey: owner,\n isSigner: false,\n isWritable: false,\n }\n // to\n AccountMeta {\n pubkey: to,\n isSigner: \n isWritable:\n }\n // authority\n AccountMeta {\n pubkey: authority,\n isSigner: true,\n isWritable: false,\n }\n // mint\n AccountMeta {\n pubkey: mint\n isSigner: false,\n isWritable: false\n },\n]\nAdditional account metas returned from the previous call to preflight_transfer must be appended to the list of accounts, in the order they were deserialized.\nOff-Chain Usage\nIn order to craft a transfer TransactionInstruction to a program that adheres to ProgramTraitTransferV1, you can simulate the\npreflight_transfer instruction with the required accounts, in order to get the list of additional AccountMetas.\nThen you can append those AccountMetas to the remaining accounts.\nReference code is provided below, written using @coral-xyz/anchor.\nimport * as anchor from '@coral-xyz/anchor';\nasync function resolveRemainingAccounts&lt;I extends anchor.Idl&gt;(\n program: anchor.Program&lt;I&gt;,\n simulationResult: RpcResponseAndContext&lt;SimulatedTransactionResponse&gt;\n): Promise&lt;AccountMeta[]&gt; {\n let coder = program.coder.types;\n let returnDataTuple = simulationResult.value.returnData;\n let [b64Data, encoding] = returnDataTuple[\"data\"];\n if (encoding !== \"base64\") {\n throw new Error(\"Unsupported encoding: \" + encoding);\n }\n let data = base64.decode(b64Data);\n // We start deserializing the Vec&lt;IAccountMeta&gt; from the 5th byte\n // The first 4 bytes are u32 for the Vec of the return data\n let numBytes = data.slice(0, 4);\n let numMetas = new anchor.BN(numBytes, null, \"le\");\n let offset = 4;\n let realAccountMetas: AccountMeta[] = [];\n const metaSize = 34;\n for (let i = 0; i &lt; numMetas.toNumber(); i += 1) {\n const start = offset + i * metaSize;\n const end = start + metaSize;\n let meta = coder.decode(\"ExternalIAccountMeta\", data.slice(start, end));\n realAccountMetas.push({\n pubkey: meta.pubkey,\n isWritable: meta.writable,\n isSigner: meta.signer,\n });\n }\n return realAccountMetas;\n}\nThis is used like so:\n// Simulate the `preflight_transfer` instruction\nconst preflightInstruction = await wrapper.methods\n .preflightTransfer(new anchor.BN(1))\n .accounts({\n to: destination,\n owner: wallet,\n authority: wallet,\n mint: iProgram.programId,\n })\n .remainingAccounts([])\n .instruction();\nlet message = MessageV0.compile({\n payerKey: wallet,\n instructions: [preflightInstruction],\n recentBlockhash: (\n await wrapper.provider.connection.getRecentBlockhash()\n ).blockhash,\n});\nlet transaction = new VersionedTransaction(message);\n// Deserialize the `AccountMeta`s from the return data\n// We have to use VersionedTransactions to get `returnData`\n// back from simulated transactions \nlet keys = await resolveRemainingAccounts(\n wrapper,\n await wrapper.provider.connection.simulateTransaction(transaction)\n);\n// Send the actual `transfer` instruction with the required additional\n// accounts\nconst tx = await wrapper.methods\n .transfer(new anchor.BN(1))\n .accounts({\n owner: wallet,\n to: destination,\n authority: wallet,\n mint: iProgram.programId,\n })\n .remainingAccounts(keys)\n .rpc({ skipPreflight: true });\nconsole.log(\"Transferred with tx:\", tx);\nCompatability: SPL Token\nSPL tokens are compatible with this format.\nThere is a provided program programs/token-wrapper that shows how to “wrap” tokenkeg to make it compatible with ProgramTraitTransferV1.\nLimitations\nWhen returning a vector of account metas in the preflight_transfer instruction, additional account metas must have isSigner: false.\nRequiring additional signer account metas must come in the form of a new ProgramTrait specification.\nReference\nThere is a reference implementation of a program adhering to ProgramTraitTransferV1 under programs/token-program of a program that records which pubkey owns how much of a token in a singleton address.\nCalling transfer on this program will change decrement the owner’s stored balance by amount and increment the recipient’s balance by amount.\nTests\nTo run a test against this program, run anchor test.",
            "comments": "[joec]: What’re your thoughts on adding the version of the implementing program you’re hitting?\nFor example, you could include the version of the program in the return data, so that PreflightPayload will capture the version of the implementing program.\nThis way, if developers introduce breaking changes to the program, you can validate the version upon preflight.\nie:\n#[derive(Debug, Clone, AnchorDeserialize, AnchorSerialize)]\npub struct PreflightVersionPayload {\n major: u8,\n minor: u8,\n}\n#[derive(Debug, Clone, AnchorDeserialize, AnchorSerialize)]\npub struct PreflightPayload {\n pub version: PreflightVersionPayload\n pub accounts: Vec&lt;IAccountMeta&gt;,\n}\n\n[joncinque]: This looks really great. It addresses the most important use case for interfaces in a very approachable way.\nIn this example, it comes with the assumption that all information must be present between the 4 accounts provided and the program logic in order to derive the additional accounts.\nThis is fine for transfers that CPI once into a well-known program, but for transfer programs that CPI to another interface during transfer, it doesn’t address passing additional accounts in the preflight to derive the next level of additional accounts.\nFor example, if my transfer program CPIs into a “pausable” interface, and the pausable interface requires another account that gives the “paused” info, the preflight is insufficient. TLV structures will help, but if the “pausable” program ends up requiring a sysvar account, we’re kinda screwed.\nThis might be addressable by saying that the preflight gets the required accounts PLUS all “remaining accounts” in Anchor speak. What do you think?\nA nit: the term owner is overloaded, why not just from or source or bag (j/k)? Especially for people already comfortable with Solana development, it’s confusing to change its meaning from “the address that can fully authorize transfers and delegations” to “the address of the token account”.\nAlso also: owner and to must be writable, correct?\n\n",
            "comment_count": 2,
            "original_poster": "ngundotra",
            "activity": "2023-04-27T16:13:14.965Z"
        },
        {
            "id": 123,
            "title": "Standard for supporting multiple SVM chains on Solana wallets",
            "url": "https://forum.solana.com/t/standard-for-supporting-multiple-svm-chains-on-solana-wallets/123",
            "created_at": "2023-04-23T19:41:55.578Z",
            "posts_count": 1,
            "views": 949,
            "reply_count": 0,
            "last_posted_at": "2023-04-23T19:41:55.658Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "sRFC 00011: A standard for implementing multi-chain SVM support in Solana Wallets\nMotivation\nThis sRFC proposes a standard for implementing multi-chain SVM(eg: Eclipse chains) support in Solana Wallets, allowing Solana applications (“dapps”) to suggest chains to be added to the user’s wallet application. This enables convenient interaction with dapps on testnets, canary chains, or other SVM chains without requiring complex manual custom RPC changes. The standard aims to streamline the process and prevent inconsistencies across multiple standards and one-off implementations.\nInspiration\nInspired by EIP-3085, this design allows Solana applications to suggest chains to be added to the user’s wallet application. The wallet application may arbitrarily refuse or accept the request. We have received a request for this feature from many dapps.\nThe problem\nUnlike Ethereum, Solana doesn’t have chain ids that are used to prevent replay attacks. Here is the solution we propose to have multichain support while not introducing breaking changes to Solana.\nSolution A: Whitelist Registry\neg:- On Keplr, when you first create a wallet, you get 10 of the most popular cosmos chains added by default; it would also make sense to do this for Solana wallets with SVM chains.\nThe workflow for adding new chains should be like this: User clicks add chain → Wallet checks if the RPC is whitelisted → New chain is added.\nQuestion: Where would we want to store the registry of the whitelisted RPCs?\nA. We could ship it with the wallet if it’s a low number(&lt;50).\nB. If it’s larger, put it on a database like Postgres or something decentralized like Arweave, IPFS, etc.\nMore technical details\nImplementation\nSolana Wallet Standard: At first glance there are some changes needed to the Solana wallet standard. There are places where the wallet is required to return all chains that it supports, so we would need some interface for the dapp to pass back a new chain to be added: wallet-standard/account.ts at master · solana-labs/wallet-standard · GitHub This change seems like it could be made immediately if it is done in a non-breaking way. The wallet should not be required to add a chain even if the dapp requests it.\nWallet Adapter: We would need to make some change to the wallet adapter so the dapp can specify the desired RPC network. This would need to be rolled out after the Wallet Standard change.\nDapp Support: The dapp updates their Wallet Adapter library and specify their desired RPC network. If no RPC network is specified, by default this change has no effect - the behavior is exactly the same as SVM wallets function right now (default to Solana mainnet). This is the last change to occur.\nMore info.\nGithub discussion: https:// github. com/solana-labs/wallet-standard/issues/9\nBackpack PR: https:// github. com/coral-xyz/backpack/pull/3730",
            "comments": "",
            "comment_count": 0,
            "original_poster": "PrasoonPratham",
            "activity": "2023-04-23T19:41:55.658Z"
        },
        {
            "id": 31,
            "title": "sRFC 00003: On-chain interface account resolution",
            "url": "https://forum.solana.com/t/srfc-00003-on-chain-interface-account-resolution/31",
            "created_at": "2023-03-15T15:19:36.966Z",
            "posts_count": 2,
            "views": 1315,
            "reply_count": 0,
            "last_posted_at": "2023-04-04T21:40:41.500Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "On-chain interface account resolution\nSummary\nAs the Solana program ecosystem matures, program interfaces will become the main means of building the composable future. Developers will still be able to innovate with their protocols to do anything, but by having their programs also adhere to interfaces, they can get immediate support with the rest of the ecosystem.\nFor example, as long as a program implements instruction processors for all of the possible “spl-token” instructions, and their structures conform to the Mint and TokenAccount types, then any marketplace or trading program can also use that program without any additional work.\nThis approach currently works, but it’s very limited. For example, if a program that implements the “spl-token” interface needs one more account to properly process a transfer (for example, the instructions sysvar), then both the client and on-chain programs need to figure out how to resolve the required accounts. SRFC 00002 solves the problem during transaction creation, but programs must also be able to construct CPI instructions on-chain. To put it differently, given a list of accounts, a program must be able to construct instructions in order to perform a CPI, without the program knowing everything about the target program.\nLet’s walk through a concrete example.\nPermissioned transfer for two different token types\nLet’s say there’s a token marketplace program with some form of bids and asks on tokens. It doesn’t matter how exactly the program works, but at some point, it needs to transfer two different token types in one instruction, which we’ll call A and B. A and B could belong to different token programs, that all implement the “spl-token” interface.\nAs part of the “spl-token” transfer interface, a token program may also CPI into another program, which adheres to a “permission-transfer-check” interface. This nested “permissioned-transfer-check” interface requires at least the program, the token mint account, a PDA derived from the mint, and any number of additional accounts to validate the transfer.\nLet’s assume that the client has properly constructed the transaction, so that all necessary accounts are available to the program. How does the program figure out which accounts are needed to construct the CPI instruction to transfer token A?\nPossible solution\nThe “spl-token” transfer interface specifies certain “guaranteed” accounts: source token account, destination token account, mint, and authority. Also, the accounts in the program (the token account and mint) must conform to a certain structural definition.\nAfter that, any other required account must be derivable from those required four. Additional account addresses may be derived as new program-derived addresses or read from account data. These additional accounts must also be provided after all of the “guaranteed” accounts in the marketplace interface.\nFor example, in Rust pseudo-code, that could be:\nMarketplaceSwap {\n token_a_source: TokenAccount,\n token_a_destination: TokenAccount,\n token_a_mint: Mint,\n token_a_authority: Signer,\n token_b_source: TokenAccount,\n token_b_destination: TokenAccount,\n token_b_mint: Mint,\n token_b_authority: Signer,\n additional_accounts: &amp;[AccountInfo]\n}\nTo create an instruction to transfer token A, the token interface exposes an instruction creator:\nfn create_transfer_instruction(\n program_id: &amp;Pubkey,\n source: &amp;TokenAccount,\n destination: &amp;TokenAccount,\n mint: &amp;Mint,\n authority: &amp;AccountInfo,\n additional_accounts: &amp;[AccountInfo]\n) -&gt; Instruction;\nThe interface instruction creator looks inside the mint, finds that it needs a CPI into the “permission-transfer-check” interface, finds the program in additional_accounts, along with a required PDA, and passes it down to one more nested function to extract the next level of additional accounts:\nfn get_additional_accounts_for_permission_transfer_check(\n program_id: &amp;Pubkey,\n mint: &amp;Mint,\n additional_accounts: &amp;[AccountInfo],\n) -&gt; [AccountMeta];\nGiven all of these, the marketplace program can construct the full instruction with all of the required accounts, and finally pass them all to the token program.\nOne level deeper, the token program will need to perform just one round on-chain account resolution to get the “permission-transfer-check” instruction:\nfn create_permission_transfer_check_instruction(\n program_id: &amp;Pubkey,\n mint: &amp;Mint,\n additional_accounts: &amp;[AccountInfo],\n) -&gt; Instruction;\nThis function is very similar to get_additional_accounts_for_permission_transfer_check, but instead it actually gives the full instruction, not just the additional account metas.\nConclusion\nWhile this is just one approach to dynamic on-chain instruction creation / account resolution, with well-defined interfaces, this recursive approach allows programs to construct even the most complicated instructions while on-chain, and without additional CPIs into other programs. Everything must be derivable from the interface, the program, and the required accounts, or the whole model fails.\nImplementation: still WIP, will update when it’s ready!",
            "comments": "[ngundotra]: Hey Jon!\nHere’s a proof of concept that does an on-chain “round trip” for account resolution written in Anchor, but doesn’t require introspecting Mint data.\n \n github.com\n \n \n ngundotra/solana-interface-permissioned-tfer/blob/c87b8fe66c29a59e4b0f9770cc87732d21a9f883/interface/src/lib.rs#L63-L93\n \n \n pub fn call&lt;\n 'info,\n C1: ToAccountInfos&lt;'info&gt; + ToAccountMetas + ToTargetProgram&lt;'info, TargetCtx&lt;'info&gt; = C2&gt;,\n C2: ToAccountInfos&lt;'info&gt; + ToAccountMetas,\n &gt;(\n ix_name: String,\n ctx: CpiContext&lt;'_, '_, '_, 'info, C1&gt;,\n log_info: bool,\n ) -&gt; Result&lt;()&gt; {\n msg!(\"Preflight\");\n // preflight\n call_preflight_interface_function(ix_name.clone(), &amp;ctx)?;\n \n \n msg!(\"Parse return data\");\n // parse cpi return data\n let additional_interface_accounts = get_interface_accounts(&amp;ctx.accounts.to_target_program())?;\n \n \n // execute\n msg!(\"Convert into target context\");\n let cpi_ctx: CpiContext&lt;C2&gt; = ctx\n \n \n This file has been truncated. show original\n \n \n \n \n \n \nI think this is a super cool paradigm that can be extended beyond Token implementations.\nFor those interested typescript tests to drive this on-chain account resolution:\ntest with program A:\n \n github.com\n \n \n ngundotra/solana-interface-permissioned-tfer/blob/main/tests/caller.ts#L155-L181\n \n \n it(\"Can lock user token account\", async () =&gt; {\n tokenRecord = PublicKey.findProgramAddressSync(\n [tokenAccount.toBuffer(), Buffer.from(\"token_record\")],\n program.programId\n )[0];\n \n \n const lockCtx: LockContext = {\n token: tokenAccount,\n mint,\n delegate: program.provider.publicKey!,\n payer: program.provider.publicKey!,\n tokenProgram: TOKEN_PROGRAM_ID,\n permProgram: program.programId,\n };\n \n \n const builder = caller.methods.lock().accounts(lockCtx);\n let keys = await builder.pubkeys();\n let { accounts: remainingAccounts } = await resolveRemainingAccounts(\n program.provider,\n \"lock\",\n \n \n This file has been truncated. show original\n \n \n \n \n \n \ntest with program B:\n \n github.com\n \n \n ngundotra/solana-interface-permissioned-tfer/blob/main/tests/caller.ts#L316-L350\n \n \n it(\"Can lock user token account\", async () =&gt; {\n tokenRecord = PublicKey.findProgramAddressSync(\n [tokenAccount.toBuffer(), Buffer.from(\"token_record\")],\n program.programId\n )[0];\n \n \n let lockCtx: LockContext = {\n token: tokenAccount,\n mint,\n delegate: program.provider.publicKey!,\n payer: program.provider.publicKey!,\n tokenProgram: TOKEN_PROGRAM_ID,\n permProgram: program.programId,\n };\n \n \n const builder = caller.methods.lock().accounts(lockCtx);\n let keys = await builder.pubkeys();\n let { accounts: remainingAccounts } = await resolveRemainingAccounts(\n program.provider,\n \"lock\",\n \n \n This file has been truncated. show original\n\n",
            "comment_count": 1,
            "original_poster": "joncinque",
            "activity": "2023-04-04T21:40:41.500Z"
        },
        {
            "id": 25,
            "title": "sRFC 00002: Off-Chain Instruction Account Resolution",
            "url": "https://forum.solana.com/t/srfc-00002-off-chain-instruction-account-resolution/25",
            "created_at": "2023-02-24T22:18:40.146Z",
            "posts_count": 4,
            "views": 764,
            "reply_count": 2,
            "last_posted_at": "2023-03-25T21:13:02.558Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Off-Chain Accounts Resolution for Transaction Creation\nSummary\nThis RFC proposes a solution for collecting all the Accounts needed for a Solana transaction before the transaction is sent to the network. By introducing a new field to the Anchor IDL called invocations and providing a mechanism to insert accounts directly from an on-chain account, we aim to enable clients to resolve all required accounts for a transaction with a single round-trip call to RPC operators for on-chain information.\nGoals\nEnable clients to resolve all required accounts for a transaction using a single round-trip call to RPC operators.\nProvide a deterministic account ordering during transaction creation, allowing programs on-chain to unpack accounts deterministically.\nBackground\nCurrently, Solana transactions require all accounts used during execution to be passed in when the transaction is created by the client. Programs can expose information about how to deserialize and serialize their state information and program instructions through their IDL (interface description language).\nProposed Implementation\nWe propose adding a new field to each instruction description in the Anchor IDL called invocations. This field is an ordered array of program address, instruction data, and accounts. Clients can then recursively traverse this list to determine the accounts each invoked program needs to complete execution.\nAnchor IDL with invocations field\n{\n \"name\": \"transfer\",\n \"accounts\": [...],\n \"args\": [...],\n \"invocations\": [\n {\n \"program_address\": \"pubkey\",\n \"instruction_data\": \"data\",\n \"accounts\": [...]\n },\n ...\n ]\n}\nTo handle cases where accounts have to be hardcoded, we should provide a mechanism that allows clients to insert accounts into a transaction directly from an on-chain account. This can be helpful but may also present a potential vector for crafting malicious transactions.\nWith these two tools, it should be possible to symbolically define the invocations ordered list so that clients can determine the full list of accounts that a program uses in a single RPC call. The full symbolic language will need further specification and design, focusing on conditionals based on instruction data and account names.\nExample IDL Instruction for NFT Program’s “transfer”\n{\n \"name\": \"transfer\",\n \"accounts\": [\n ...\n ],\n \"args\": [\n ...\n ],\n \"invocations\": [\n {\n \"program_address\": \"pubkey\",\n \"instruction_data\": \"data\",\n \"accounts\": [...]\n },\n ...\n ]\n}\nBy implementing the proposed solution, we aim to improve the process of resolving accounts required for a Solana transaction and ensure deterministic ordering for unpacking accounts by programs on-chain.\nImplementation\nTBD",
            "comments": "[blockiosaurus]: Random suggestion, but what if instead of manually recording invocations we add a more automated approach that collated retrieved accounts at simulation time? It’s a more involved approach, but something I’ve been mulling over and something I think would be scalable and require less work by users in the long run.\nAdd compilation to WASM for instructions/processors so Solana programs can be called from a client.\nAdd a mock interface for all Solana specific tasks so basically NOOP so the program can be executed on the client. This is the most work but probably has use cases beyond account resolution.\nAdd extra code to the #[account] attribute that adds an expression to the deserialization step on simulation. When the data is retrieved from the “account” during simulation, the account address that is retrieved from will be stored in a Set. Therefore any account read from, whether it’s a dynamically determined PDA or static account, will be tracked and added to the set.\nAfter this simulation step, a Set is returned with a complete list of accounts used by the instruction.\nLimitations: Account addresses that depend on non-deterministic data (random numbers, slot time, or derivation from on-chain state that is subject to change) may not function with this method.\n\n[ngundotra]: blockiosaurus:\nwe add a more automated approach that collated retrieved accounts at simulation time? It’s a more involved approach, but something I’ve been mulling over and something I think would be scalable and require less work by users in the long run.\nI think this would work technically, but I have 2 concerns:\nBrute forcing transaction simulation to figure out missing accounts breaks the account design constraints that all Solana programs are built with. Accounts are a first-class constraint across the entirety of the Solana runtime. All program development implicitly requires knowledge of the accounts needed during instruction execution. Changing the runtime to support a method of execution to resolve accounts via simulation feels like a direct violation of this primary design constraint for programs.\nFiguring out a format for programs to emit missing accounts during simulation will end up being the same as putting required accounts into the IDL. At the point where we start adding branching logic to which accounts are required for any given instruction, then we might as well go all the way and design a language to express which accounts are needed, conditioned on other account data or instruction data. In my head, this ends up being the same approach I have described above.\n blockiosaurus:\nAdd extra code to the #[account] attribute that adds an expression to the deserialization step on simulation. When the data is retrieved from the “account” during simulation, the account address that is retrieved from will be stored in a Set. Therefore any account read from, whether it’s a dynamically determined PDA or static account, will be tracked and added to the set.\nI think this is really smart, and very doable. However, I think this approach adds compute cost to programs that implement this approach, and is otherwise untenable for frozen programs. For compute-restricted programs, like DeFi, this will probably be unusable.\nBut I think if you hack away at this, you may discover solutions around this \nQuick note on the original sRFC feasibility:\nI think implementing this sRFC could literally be as simple as adding #[idl_annotation(cpi(program, args))] over instruction enums, like shank does. This would not require modifying on-chain code to generate an updated IDL, but would automatically generate the corresponding invocation graph in the IDL. ¯_(ツ)_/¯ hope this helps add some concrete detail until I get around to working on an implementation\n\n[blockiosaurus]: I think I overloaded the term “simulation” in my original post . My meaning was that all IX/Processors would be compiled to WASM and executable on the client without using RPC or on-chain simulation. Simulation would be done on the client.The #[account] attribute code that keeps track of needed accounts would only be added when compiled for WASM.\nBut I agree that the original sRFC makes the most sense and would hugely beneficial.\n\n",
            "comment_count": 3,
            "original_poster": "ngundotra",
            "activity": "2023-03-25T21:13:02.558Z"
        },
        {
            "id": 32,
            "title": "sRFC 00004: Native Events Program",
            "url": "https://forum.solana.com/t/srfc-00004-native-events-program/32",
            "created_at": "2023-03-15T15:52:01.989Z",
            "posts_count": 5,
            "views": 1298,
            "reply_count": 1,
            "last_posted_at": "2023-03-25T00:00:39.830Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Solana Events for Programs\nSummary\nThis RFC proposes a native “Event” program built into the Solana runtime to address the current limitations in log storage and validation. By implementing a standardized event interface, we aim to achieve the following goals:\nLogs should be available as long as Solana blocks are available.\nThere must be a way to validate that logs (or events) returned by the RPC operators are stored in the block and are truthful.\nThere must be a clear pricing curve for permanent log storage on Solana, either implicit or explicit.\nProblem Description\nCurrently, Solana logs are truncated to 10kb max per transaction, and there is no process to find “untruncated” logs stored in blocks or to validate the logs returned by RPC nodes. Solana applications work around log-truncation by calling another program (like a “no operation” program) and sending the desired log as instruction data. Such invocations are reliably returned by RPC operators, but they could maliciously (or accidentally) drop history of invocations, and nobody would be able to verify without replaying the historical transaction.\nIt’s worth noting that EVM chains index their programs only through events, since program data layout is theoretically unspecified. Indexing solana programs only through logs is currently not feasible until there is a reliable way to retrieve them.\nGoals\nDefine an “Event” interface for Solana programs that allows logs to be stored for as long as Solana blocks are available.\nImplement a mechanism to validate that the logs returned by RPC operators are accurate and stored in the block.\nEstablish a clear pricing curve for log storage on Solana.\nPossible Implementation\nHigh level summary\nExpose a new program built-in to the runtime, that merkelizes logs emitted over the course of a block and writes the merkle tree root to a read-only account via PDA (e.g. [block_number.to_le_bytes()]) at the end of the block. These are referred to as EventStorage accounts. This should allow dApps to reliably use this built-in program’s logs for app-critical needs such as indexing.\nEvent Instruction and Storage\nWe can define an event program for Solana that stores event data into a Solana block, hashes the event data in the whole block into a 32 byte hash, and stores the hash into an EventStorage account. This will allow logs to be recovered from block information, and allow downstream clients to reliably verify RPC responses.\n/// Event program instruction\nenum EventProgramInstruction {\n /// `LogEvent` instruction that takes an `Event` as input \n /// and logs the event to the on-chain storage.\n LogEvent(Vec&lt;u8&gt;)\n}\n/// EventStorage account\npub struct EventStorage {\n pub block_id: u64,\n pub events_hash: u32\n}\n/// Event struct returned by RPC operators\nstruct Event {\n pub block_index: u16, \n pub event_data: Vec&lt;u8&gt;\n}\nLog Validation\nTo validate that the logs returned by RPC operators are accurate and stored in the block, we can ask RPC nodes to return the Merkle proof along with the requested logs, allowing clients to verify the logs’ authenticity against the Merkle root stored in the EventStorage account.\nPricing Curve\nTo establish a clear pricing curve for log storage on Solana, we can implement the following:\nModify the LogEvent instruction to charge a storage fee, paid in native tokens, based on the size of the event data to the feePayer of the executing transaction.\nIntroduce a StorageFee struct that contains the base storage fee and the additional fee per byte of event data that can be modified via SIMD proposal.\nAlternative Solutions\nWe could instead just add a pricing mechanism for existing logs emitted via sol_log_data that requires RPC operates to store them permanently. This could be as simple as increasing the CU for log syscalls to reflect an explicit “price per byte” of a transaction. If transaction logs are usually capped at 10 KiB, and a single transaction has a flat cost of 5000 lamports (at minimum, for a single signer), then each byte of transaction execution record is implicitly priced at 0.5 lamports per byte.\nCurrently, this low cost of adding to the Solana ledger incentivizes applications to run their own validator to retrieve logs their programs have emitted. This can be both a time-expensive and financially-expensive operation for an application to run their own validator to simply index &amp; debug their own program.\nPerhaps by introducing a separate fee schedule for logging that has price per byte &gt; 0.5 lamports/byte, we can discourage the volume of logging per transaction, to less than 10 KiB. This would allow most currently existing RPC operators to comply with an SLA to consistently deliver complete logs for each transaction.\nRecommended Implementation\nWe recommend implementing the built-in event program and storage as described above. This approach would address the current limitations in log storage and validation while also providing clear pricing for writing data to the Solana ledger.\nThere are 2 issues with the alternative approach of simply adding explicit transaction fees for logging.\nOne, existing programs that perform logging will now have increased fees, which may cause unfair competitive disadvantage between DeFi dApps. Two, the only mechanism to validate transaction logs returned by RPC operators is to run your own validator &amp; compare transaction logs after transaction replay.\nIn comparison, the built-in program also provides a new mechanism to define log storage, but provides a superior method to validate logs that reduces burden on dApps. Thus we recommend the built-in event program solution, and encourage further implementation research.",
            "comments": "[joec]: Cool!\nSo, just for clarity: you basically take all of the emitted logs for a block, hash them into a merkle tree, store the root in the account for that blockhash, and can verify all logs emitted during that block are valid via the merkle proof?\nSo with this implementation, one could get their program logs from their RPC provider, and the RPC provider can also provide the merkle proof to validate that those logs are represented in the stored account?\naka, “proof of logging” \nSince the suggested program is proposed to be embedded in the runtime, I’m curious how we could possibly expand/leverage this proposal to solve for the issue of truncating the logs as well.\nThinking about something like persistent logging or a logging retention pipeline, if an enterprise-scale large program wanted to retain lots of logs for a period of time and also have the validation proposed here that those logs are accurate and represented within the block, how can we possibly integrate un-truncating logs?\n\n[ngundotra]: joec:\nyou basically take all of the emitted logs for a block, hash them into a merkle tree, store the root in the account for that blockhash, and can verify all logs emitted during that block are valid via the merkle proof?\nYes and instead of logging, they’ll be executing a syscall to the event program, which does the hashing at the end of the block, and posts the merkle root to an EventStorage account on-chain. This should also increase the fee charged to the transaction feePayer, proportional to the amount of data that was emitted to the event program.\n joec:\nThinking about something like persistent logging or a logging retention pipeline, if an enterprise-scale large program wanted to retain lots of logs for a period of time and also have the validation proposed here that those logs are accurate and represented within the block, how can we possibly integrate un-truncating logs?\nIf an enterprise-scale large programs want to store “logs” (aka sol_log_data logs) then they should run their own validator &amp; only trust the logs that are emitted from replaying their programs own transactions. This could potentially be a good business for RPC operators.\nI think it’s fine to make current “logs” a very transient &amp; unreliable source of program metadata, and then encourage RPC operators to charge customers to serve program logs, since they are additional burden on RPC operation.\n\n[santhosh]: I think it would be great if there was a reliable way to associate events with programs that emitted them!\n\n[ngundotra]: Here’s a concrete example of program control flow that is uninterpretable without access to full logs for transaction history.\nThese two transaction look like they both have the same control flow, but the second inner instruction is actually different.\n \n \n explorer.solana.com\n \n \n \nExplorer | Solana\n \nLook up transactions and accounts on the various Solana clusters\n \n \n \n \n \n \nProgram A → Program B\nProgram A → Program C\n \n \n explorer.solana.com\n \n \n \nExplorer | Solana\n \nLook up transactions and accounts on the various Solana clusters\n \n \n \n \n \n \nProgram A → Program B\nProgram B → Program C\nWith full logs, we would be able to index the 2nd inner instruction properly, but when we only have access to truncated logs, like in the 2nd transaction, we cannot tell which program invoked program C.\nJust wanted to add this example to the discussion to emphasize the importance of this primitive for indexing programs. Personally I don’t think there are any RPC providers that offer “full logs” as a service, but having a few different ones offering that service would solve this problem entirely.\nCode for the repro transactions here: GitHub - ngundotra/solana-event-logging-repro: indexing is hard\n\n",
            "comment_count": 4,
            "original_poster": "ngundotra",
            "activity": "2023-03-25T00:00:39.830Z"
        },
        {
            "id": 44,
            "title": "sRFC 00005: An implementation of On-chain image storage to the current metaplex nft standard",
            "url": "https://forum.solana.com/t/srfc-00005-an-implementation-of-on-chain-image-storage-to-the-current-metaplex-nft-standard/44",
            "created_at": "2023-03-19T05:00:01.151Z",
            "posts_count": 3,
            "views": 1163,
            "reply_count": 1,
            "last_posted_at": "2023-03-21T14:58:02.737Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "An implementation of On-chain image storage to the current metaplex nft standard \nThis standard utilizes the existing fields of the metaplex nft program to be able to store an on-chain image\nImplementation: \nCreating this topic to discuss how I stored images on-chain on solana with the NFT project known as Blockrons as a proof of concept.\nIntro:\nBlockrons was built in december 2022, before there were much interest on on-chain solana nfts. The goal of the project is 2 fold, implement a true on-chain nft as well as educate the degen/twitter audience on why/how on-chain nfts are done on solana.\nImage Data:\nThe image data is stored as a string directly to a solana address. The format is base 64 SVG, which is a great format to store pixel art. The idea was not to complicate things with advanced struct or json’s but rather simply store the data in an address in such a way that a simple data retrieval call on the solana address will return the image data without needing additional decoding or struct deserialization.\nThe string also uses a data URL header, this allows browsers to native read the string data into an image. Therefore all strings start with: “data:image/svg+xml;base64,(data here)”\nConnection to the NFT token:\nTo connect this PDA image storage account to an existing metaplex NFT, we use 3 methods.\nMethod 1 is write the image storage account on the NFT description, however since the description itself is in a json stored off-chain, this is not a good implementation.\nMethod 2 is the fact that the image storage account is a PDA generated with the NFT token pubkey as a seed (the other half of the seed is the programID). This makes it easy to retrieve the image storage account knowing just the programID and the nft token pubkey.\nMethod 3 is writing the image storage account directly to the URI field. This is done by appending the solana account pubkey after the arweave (off-chain) link. Wallets and marketplace still resolve to the arweave link, we can keep the image storage account written there for manual image retrieval.\nScreenshot 2023-03-19 at 11-55-24 Solscan - The most intuitive Solana explorer2474×1492 349 KB\nYou can see the highlighted green area above to show how the image storage account was added to the URI field.\nTesting:\nIf you would like to test how images are stored, visit this link: https://onchain-datagrabber-developerhamster.vercel.app/\nThis website was built to showcase to the audience how the images are stored and retrieved in a step by step process. If I were to automatically retrieve the image on-chain, then how can you illustrate/educate to the audience that the images are on-chain?\nThis proposal aims to introduce the “/?onchain=” append to the metaplex URI field as a standard way to store on-chain image accounts.",
            "comments": "[ngundotra]: Hey Hamster!\nThank you for your proposal on writing Solana NFT images into Solana accounts. After reviewing your RFC, I think to highlight that it actually comprises two separate RFCs.\n1. Writing &amp; Reading SVG Images as PDAs of a Solana Program\nThis portion of the proposal discusses storing image data in base64 SVG format directly to a Solana address, using a data URL header to make it natively readable by browsers. The image storage account is a PDA generated with the NFT token pubkey as a seed, making it easy to retrieve the image knowing just the programID and the NFT token pubkey.\nIt’s very important that you attach a Proof of Concept implementation, or an existing program’s implementation to define this RFC.\n2. URI Structure of Metaplex NFTs with Optional Fetching of Image Data\nThis part of the proposal outlines three methods to connect the PDA image storage account to an existing Metaplex NFT. The suggested approach is to append the Solana account pubkey after the Arweave (off-chain) link in the URI field, and introduce the “/?onchain=” append to the Metaplex URI field as a standard way to store on-chain image accounts.\nI think it’d be awesome if you could rewrite #1 as a separate RFC, and #2 is fine as an RFC here, but I think it would need to be submitted to Metaplex separately as well, so they have some awareness.\nLooking forward to hearing from you soon!\n\n[Hamster]: I appreciate the response. It seems like metaplex will be going their own direction with this as they are in full control of the standard currently.\nI will still continue my sRFC proposal as they might be of use to any future developer looking to learn more or tackle this subject.\nFor RFC #2: URI Structure of Metaplex NFTs with Optional Fetching of Image Data\nI used a solana account that directly contains an image but its better to follow the current standard of first pointing to solana account containing a .json struct, then inside that .json we can have another solana account containing the image.\nSo I propose we use 2 solana accounts, one for the json and one for the image, similar to how arweave is used now.\n\n",
            "comment_count": 2,
            "original_poster": "Hamster",
            "activity": "2023-03-21T14:58:02.737Z"
        },
        {
            "id": 13,
            "title": "About the sRFC category",
            "url": "https://forum.solana.com/t/about-the-srfc-category/13",
            "created_at": "2023-02-23T01:34:03.721Z",
            "posts_count": 2,
            "views": 432,
            "reply_count": 0,
            "last_posted_at": "2023-02-24T05:13:14.541Z",
            "category_id": 6,
            "category_name": "sRFC",
            "description": "Solana Request For Comments category is geared towards discussions between Solana developers about application standards.",
            "comments": "[jacobcreech]: \n\n",
            "comment_count": 1,
            "original_poster": "jacobcreech",
            "activity": "2023-02-24T05:13:14.541Z"
        }
    ],
    "RFP": [
        {
            "id": 348,
            "title": "About the RFP category",
            "url": "https://forum.solana.com/t/about-the-rfp-category/348",
            "created_at": "2023-06-30T01:45:20.491Z",
            "posts_count": 1,
            "views": 538,
            "reply_count": 0,
            "last_posted_at": null,
            "category_id": 10,
            "category_name": "RFP",
            "description": "RFP, or Request for Proposal, outlines a project that the author is interested in funding. This category is for discussing RFPs created by Solana Foundation.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "jacobcreech",
            "activity": null
        },
        {
            "id": 2249,
            "title": "Solana Historical State Verification Tool",
            "url": "https://forum.solana.com/t/solana-historical-state-verification-tool/2249",
            "created_at": "2024-10-14T04:28:11.883Z",
            "posts_count": 3,
            "views": 522,
            "reply_count": 0,
            "last_posted_at": "2025-01-16T12:10:04.472Z",
            "category_id": 10,
            "category_name": "RFP",
            "description": "Solana Historical State Verification Tool\nContext\nThe Solana ecosystem currently lacks a comprehensive tool for verifying historical state transitions. As we’ve evolved from the Anza Google Bucket instance to more diverse and efficient archival solutions, there’s a growing need for end users to verify the accuracy of data received from various archive providers. While the existing solana-ledger-tool can reproduce transaction execution results, it doesn’t account for historical file formats, previous SVM versions, or past feature sets, leading to potential inaccuracies when verifying older data.\nPlease see the following RFP that outlines a request to create an open-source historical state verification tool for Solana. The Solana Foundation has proposed a set of solutions, but the specific implementation details will be finalized during grant negotiation between the grantee and the Solana Foundation.\nLogistics\nTake note of the application deadline (11/15/2024). The maximum grant amount is currently earmarked at $275k in USD-equivalent locked SOL. The final grantee will work with the Solana Foundation to decide on the final terms of the agreement, including negotiation of rigorous but attainable milestones.\nGround Rules\nThis thread can be used for comments, questions, praise, and/or criticism, and is intended to be an open forum for any prospective responders. This thread is also an experiment in increasing the transparency through which RFPs are fielded by the Solana ecosystem, so please be mindful that we’re all here to learn and grow.\nResponses to this RFP are not required to be public (but recommended), but if it is helpful to share notes or combine forces, then please use this thread for such purposes.",
            "comments": "[SE7EN]: Hi @laura,\nWe’ve submitted our application but haven’t heard back yet. Do you have a timeline for when you’ll be responding to the proposals?\n\n[luke_mlabs]: MLabs (www.mlabs.city) also submitted a bid - was there ever a result announced?\n\n",
            "comment_count": 2,
            "original_poster": "laura",
            "activity": "2025-01-16T12:10:04.472Z"
        },
        {
            "id": 2059,
            "title": "Indexer tooling",
            "url": "https://forum.solana.com/t/indexer-tooling/2059",
            "created_at": "2024-09-05T10:57:11.925Z",
            "posts_count": 10,
            "views": 1138,
            "reply_count": 3,
            "last_posted_at": "2024-11-04T20:52:48.857Z",
            "category_id": 10,
            "category_name": "RFP",
            "description": "Context\nGeyser, while a great tool, does not provide an ergonomic end-to-end solution for developers to get a full picture of the data created from their programs or desired accounts. While useful for certain use cases, developers in the Solana ecosystem need a simple, yet malleable tool for retrieving live and historical data.\nPlease see the following RFP that outlines a request to create an open source indexer framework. The Solana Foundation lays out a list of proposed solutions, but the technology used to build the indexer will be decided during grant negotiation by the grantee and the Solana Foundation.\nLogistics\nTake note of the application deadline (10/04/2024). The maximum grant amount is currently earmarked to $100k in USD-equivalent locked SOL. The final grantee will work with the Solana Foundation to decide on the final terms of the agreement, including negotiation of rigorous but attainable milestones.\nGround Rules\nThis thread can be used for comments, questions, praise, and / or criticism, and is intended to be an open forum for any prospective responders. This thread is also an experiment in increasing the transparency through which RFPs are fielded by the Solana ecosystem too, so please be mindful that we’re all here to learn and grow.\nResponses to this RFP are not required to be public (but recommended), but if it is helpful to share notes or combine forces, then please use this thread for such purposes.\nLink: Airtable - Solana Foundation Active RFPs",
            "comments": "[codewithmide]: pkxro:\nThis thread is also an experiment in increasing the transparency through which RFPs are fielded by the Solana ecosystem too, so please be mindful that we’re all here to learn and grow.\nI would love to team up on this. Anyone interested?\n\n[dmozhevitin]: Hi!\nThe RFP itself doesn’t mention the data sources where the proposed tool should fetch the data from. For me it seems that it should allow integration with existing indexers (or just node RPC) to fetch the data from there, and allow exporting the data about specific accounts directly to developer’s database. Could you please tell if I got the idea of this RFP right, or elaborate a bit more if this is not the case?\nThank you in advance!\n\n[abba0110]: Have a related question on system design. Is the goal for this tool to interface with the validator via remote procedure call, or parse the state in /accounts/run and /rocksdb?\nSecond pass at the requirements.\n\n[pkxro]: The RFP itself doesn’t mention the data sources where the proposed tool should fetch the data from. For me it seems that it should allow integration with existing indexers (or just node RPC) to fetch the data from there, and allow exporting the data about specific accounts directly to developer’s database. Could you please tell if I got the idea of this RFP right, or elaborate a bit more if this is not the case?\nThis is up to the RFP applicant, but ideally will be a connection to a geyser stream service that RPC providers already offer. If you can do RPC link or geyser, that would be bonus points.\n\n[pkxro]: Have a related question on system design. Is the goal for this tool to interface with the validator via remote procedure call , or parse the state in /accounts/run and /rocksdb?\nThe goal of this RFP is to offer a stack for developers who do not run fullnodes to get realtime (and historical) updates for their program. It is reasonable to say that the access to the data will come from geyser or RCP links that come from RPC providers.\n\n[meowmeowmoew]: Hi,\nQuick question about the grant funds repartition here.\nIs the maximum grant of 100k a fund to build the project in the ~ 4 months delay and thats all, or should applicant consider including the maintenance cost for 2 years part of the grant ?\nwarmest,\nmeowmeowmeow\n\n[Dsrdrk11a]: Hello! We have aplied for this RFP on the 3rd of Oct, but still didn’t get any reply or info on evaluation on the email, mentioned in the application. In order to understand the workload for the coming months, could someone, please, inform us about the decision-making timeline regarding this RFP?\n\n[luke_mlabs]: Hello,\nWe at MLabs are quite interested in this project and are considering applying. As part of our due diligence, we have a few clarifying questions:\nHow is the SOL payment determined and unlocked? For instance, upon application acceptance, the spot equivalent of 100K USD in SOL is locked and then unlocked upon project completion? We’re trying to understand better where the currency volatility risk lies.\nWhile we view this as a minimal risk, could you clarify what happens if the project timeline is exceeded? Is the project considered a failure, is payment reduced, or are there other contingencies?\nWhat are the expectations regarding progress reports, stakeholder check-ins, and general communication throughout the project duration?\nThank you for the clarity!\n\n[idontknowhowtospell]: i will start an experiment, I will launch a coin on pump.fun through this forum to see if it gets traction Its gonna contain the Ticker $Apple and will use this TG: Telegram: Contact @apple_on_sol\nno more infos\nHave fun\n\n",
            "comment_count": 9,
            "original_poster": "pkxro",
            "activity": "2024-11-04T20:52:48.857Z"
        },
        {
            "id": 2060,
            "title": "Discriminator Database",
            "url": "https://forum.solana.com/t/discriminator-database/2060",
            "created_at": "2024-09-05T10:59:08.815Z",
            "posts_count": 7,
            "views": 502,
            "reply_count": 5,
            "last_posted_at": "2024-09-24T11:16:10.051Z",
            "category_id": 10,
            "category_name": "RFP",
            "description": "Context\nTeams in the Solana ecosystem regularly face challenges interacting with unknown deployed contracts and parsing unknown instructions. A community discriminator dataset would lead to a public good grouping of IDL discriminators that any developer can pull from when needed, which would lead to an incremental increase in developer productivity and speed of interaction of Solana tooling.\nPlease see the following RFP that outlines a request to create an open source discriminator database. The Solana Foundation lays out a list of proposed solutions, but the technology used to build the database will be decided during grant negotiation by the grantee and the Solana Foundation.\nLogistics\nTake note of the application deadline (10/04/2024). The maximum grant amount is currently earmarked to $60k in USD-equivalent locked SOL. The final grantee will work with the Solana Foundation to decide on the final terms of the agreement, including negotiation of rigorous but attainable milestones.\nGround Rules\nThis thread can be used for comments, questions, praise, and / or criticism, and is intended to be an open forum for any prospective responders. This thread is also an experiment in increasing the transparency through which RFPs are fielded by the Solana ecosystem too, so please be mindful that we’re all here to learn and grow.\nResponses to this RFP are not required to be public (but recommended), but if it is helpful to share notes or combine forces, then please use this thread for such purposes.\nLink: Airtable - Solana Foundation Active RFPs",
            "comments": "[dmozhevitin]: Hi! Thank you for publishing this RFP, sounds like a really valuable improvement!\nCould you please help with the following questions?\nWhat kind of collisions/conflicts/duplications is mentioned in the RFP description?\nShould the frontend interface for uploading IDLs require the signature of program authority to upload the IDL for it? For the contract verification tools it isn’t necessary because their main purpose is comparing the uploaded source code with the bytecode deployed to the blockchain, but in this case some 3rd party actor can upload incorrect IDL for a given program intentionally, so it sounds like that this feature is necessary, unless I’m missing something.\nWhat does “resolve an input given a hash” mean in the RFP description?\nThank you in advance!\nUPD: as far as I understood from the discussions in Anchor discord, the apr.dev registry is discontinued and no longer working. Is it right that the RFP part about Anchor registry watcher is no longer relevant?\n\n[ludovic]: Hi @pkxro,\nWe’re a team of 3 seasoned blockchain engineers, actively working full time on the problem that this spec is describing and we are super excited to see a RFP for this!\nThe specs makes sense to us, I have a few questions regarding some specific points:\nA Community dataset that allows developers to upload discriminators and their associated inputs with metadata about program relevance with collision detection\nI’d like to check my understanding here, is the idea helping developers checking the validity of their payloads pre transaction signing?\nCommunity dataset hosted with chunked and compressed parquet files that developers can download on some regular cadence\nWould it make sense to partition this dataset by protocol author / teams?\nCould you unpack the intention / need driving this requirement?\nThank you for your help!\n\n[pkxro]: @ludovic\nI’d like to check my understanding here, is the idea helping developers checking the validity of their payloads pre transaction signing?\nYes exactly that – you should be able to upload an ABI and parse out all of the discriminators, or upload them individually with the sha256 of the 8bytes – note that there is some new yet-to-be upstreamed work that makes discriminators work for arbitrary lengths Support custom discriminators · Issue #3097 · coral-xyz/anchor · GitHub\nWould it make sense to partition this dataset by protocol author / teams?\nCould you unpack the intention / need driving this requirement?\nGenerally speaking, it is quite hard for data and monitoring teams to parse instructions cleanly. The goal if this RFP is to offer a UI, an API, and a rate-limited ability to get a full dump of the dataset (apache parquet is just what the data ecosystem converged on). Partitioning is ultimately up to you, but seeing as the output of this is just a set of discriminators/idls and not every parsed instruction ever, this should ideally not be too cumbersome because the footprint is quite small\n\n[pkxro]: @dmozhevitin\nWhat kind of collisions/conflicts/duplications is mentioned in the RFP description?\nAs a result of Support custom discriminators · Issue #3097 · coral-xyz/anchor · GitHub the footprint for collisions increases. You ultimately move from 2^256 to much lower parameters.\nShould the frontend interface for uploading IDLs require the signature of program authority to upload the IDL for it? For the contract verification tools it isn’t necessary because their main purpose is comparing the uploaded source code with the bytecode deployed to the blockchain, but in this case some 3rd party actor can upload incorrect IDL for a given program intentionally, so it sounds like that this feature is necessary, unless I’m missing something.\nIdeally this does not require the program authority to upload a discriminator or IDL. The idea here is to put the dataset in the hands of developers. If someone has previously found an IDL or someone has a discriminator and knows the args, they should be able to upload it. The key here is that the dataset does not attribute an IDL to a program. You can combine this user-generated dataset and scrape all of the anchor PDAs that hold IDLs and create a very comprehensive list that also includes authorized IDLs (we’ve already found indexed all of the anchor IDLs and are happy to share the dataset)\nWhat does “resolve an input given a hash” mean in the RFP description?\nReverse mapping of hash → function if the function is already in the dataset\n\n[ludovic]: @pkxro thank you for the details + pointer to 3097!\nI would love to show you what we’re building, will you be Breakpoint?\n\n[dmozhevitin]: Hi @pkxro!\nThank you for your answers!\nI have one more question regarding this RFP: the RFP mentions the ability to upload/search discriminators/IDL for a given program. I wonder what’s the point of providing the ability to upload the discriminator separately, since uploading the IDL covers all the program discriminators. Does it make sense to make the frontend/API to upload only IDLs?\nAs a follow-up question, I also wonder what does the “upload discriminator” mean? It seems that upload the discriminator itself doesn’t make much sense, is it right that “uploading the discriminator” also includes uploading the associated account/instruction data (i.e. the part of the IDL)?\nThank you in advance!\n\n",
            "comment_count": 6,
            "original_poster": "pkxro",
            "activity": "2024-09-24T11:16:10.051Z"
        },
        {
            "id": 939,
            "title": "Rustls support for raw public keys",
            "url": "https://forum.solana.com/t/rustls-support-for-raw-public-keys/939",
            "created_at": "2024-01-11T05:21:03.995Z",
            "posts_count": 2,
            "views": 749,
            "reply_count": 0,
            "last_posted_at": "2024-08-29T14:30:48.903Z",
            "category_id": 10,
            "category_name": "RFP",
            "description": "Proposal\nSponsor the authors of rustls to add support for the RFC 7250 standard.\n \n \n IETF Datatracker\n \n \n \nRFC 7250: Using Raw Public Keys in Transport Layer Security (TLS) and...\n \nThis document specifies a new certificate type and two TLS extensions for exchanging raw public keys in Transport Layer Security (TLS) and Datagram Transport Layer Security (DTLS). The new certificate type allows raw public keys to be used for...\n \n \n \n \n \n \nRelated GitHub issue: Support RawPublicKey (non-X509) certificates, e.g. for P2P · Issue #423 · rustls/rustls · GitHub\nThe maintainers of rustls are (according to the README):\nJoe Birr-Pixton (@ctz, Project Founder - full-time funded by Prossimo)\nDirkjan Ochtman (@djc, Co-maintainer)\nDaniel McCarney (@cpu, Co-maintainer - full-time funded by Prossimo)\nSponsorship links:\nRustls - Prossimo\nSponsor @djc on GitHub Sponsors · GitHub\nMotivation\nThe Solana Labs client uses the rustls open-source library to establish secure peer-to-peer connections.\nConsiderable tech debt has been caused by lack of support for “raw public keys”, a TLS extension that simplifies authentication.\nrustls only supports heavy web PKI/X.509 authentication, which is primarily designed for the web. The Solana Labs client has had to resort to hacks to get X.509 to work in peer-to-peer networks.\nIn Firedancer, 10255 lines of code are currently dedicated to supporting these X.509 mock certificates.\nSupport for RFC 7250 will improve network security and reduce code footprint by tens of thousands of lines of code across Solana peer-to-peer libraries.\nSee the following related forum posts:\nQUIC-TLS in Firedancer (fd_tls) - #4 by ripatel-jump\nDeprecate X.509 certs for P2P connections",
            "comments": "[aochagavia]: I’m working on this right now (see this PR) and have a question: would it be all right to implement this only for TLS 1.3, or do we also need it for TLS 1.2? The maintainers mentioned they’d prefer a TLS1.3 -only implementation, that’s why I’m asking.\n\n",
            "comment_count": 1,
            "original_poster": "ripatel-jump",
            "activity": "2024-08-29T14:30:48.903Z"
        },
        {
            "id": 667,
            "title": "Generalized State Compression",
            "url": "https://forum.solana.com/t/generalized-state-compression/667",
            "created_at": "2023-11-07T16:28:52.022Z",
            "posts_count": 4,
            "views": 1076,
            "reply_count": 0,
            "last_posted_at": "2024-08-02T10:58:39.615Z",
            "category_id": 10,
            "category_name": "RFP",
            "description": "Context\nState Compression is a technical primitive for verifying data secured in the Solana ledger, allowing that data to be used in smart contracts and drastically reducing the cost of state on the Solana network.\nState Compression is most prominently used by Metaplex’s Bubblegum program, which powers Compressed NFTs. NFTs have stable data structures and thus are easily parsed by multiple parties. However, generalized state compression would allow the same cost savings being used for NFTs to work for any piece of state on Solana.\nThere’s an assortment of standards, specifications, tools, and workflows needed to make this a reality on Solana. This RFP is intended to be completed by one (or many) parties, and prospective submissions can address one or many of the milestones.\nPlease apply for the RFP here for consideration.\nLogistics\nTake note of the end date (1/1/2024) and be sure to make sure all criteria is met prior to sending in an application.\nGround Rules\nThis thread can be used for comments, questions, praise, and / or criticism, and is intended to be an open forum for any prospective responders. This thread is also an experiment in increasing the transparency through which RFPs are fielded by the Solana ecosystem too, so please be mindful that we’re all here to learn and grow.\nResponses to this RFP are not required to be public, but if it is helpful to share notes or combine forces, then please use this thread for such purposes.",
            "comments": "[jnwng]: note—the submission deadline has been extended to 12/1/2023 due to slippage (me procrastinating due to Breakpoint)\nalso a note on milestones: since the milestones are somewhat inter-related but not entirely dependent, submissions for the 1st and 2nd milestones will be reviewed after the submission deadline, and submissions for the 3rd milestone may be accepted on a rolling basis because that milestone is broadly applicable to compressed NFTs, not just generalized state compression.\n\n[jnwng]: this RFP has been completed, and all teams with submissions have been (personally) evaluated as well as notified regarding their status. we’re looking forward to the continued development of technologies to help scale Solana to new heights- more to come as work proceeds.\n\n[Gabynto]: I would keep my hands crossed in anticipation to more of such RFPs.\n\n",
            "comment_count": 3,
            "original_poster": "jnwng",
            "activity": "2024-08-02T10:58:39.615Z"
        },
        {
            "id": 1031,
            "title": "Post-Deployment Monitoring Tooling",
            "url": "https://forum.solana.com/t/post-deployment-monitoring-tooling/1031",
            "created_at": "2024-02-07T17:41:18.825Z",
            "posts_count": 9,
            "views": 2039,
            "reply_count": 4,
            "last_posted_at": "2024-07-31T10:41:41.498Z",
            "category_id": 10,
            "category_name": "RFP",
            "description": "Context\nThere exists a lack of post-deployment monitoring tooling that allow for program developers to extract realtime and actionable insights. Developers should look to use continuous monitoring or observability tools to manage active threats and achieve actionable data regarding their programs. Most insights require a real time data filtering or analysis workflows and will need to maintain a firehouse of program-specific updates, assessors and instruction calls.\nPlease see the following RFP that outlines a request to create program monitoring tooling. The Solana Foundation lays out a list of proposed solutions, but the technology used to monitor (or verify) is at the behest of the applicant.\nLogistics\nTake note of the application deadline (2/29/2024). The maximum grant amount is not included within the request as different monitoring applications will have varying cost factors. The resulting finalist(s) will work with the Solana Foundation to receive an appropriate grant issued in USD-equivalent locked SOL with approachable, but rigorous milestones.\nGround Rules\nThis thread can be used for comments, questions, praise, and / or criticism, and is intended to be an open forum for any prospective responders. This thread is also an experiment in increasing the transparency through which RFPs are fielded by the Solana ecosystem too, so please be mindful that we’re all here to learn and grow.\nResponses to this RFP are not required to be public (but recommended), but if it is helpful to share notes or combine forces, then please use this thread for such purposes\nLink: Airtable - Solana Foundation Public RFP Database",
            "comments": "[0xmulch]: Hey @pkxro,\nThanks for this post and getting the word out. I had this realization a few days ago- there is a huge market gap for tools that monitor contracts post-deployment. I think it is something that has been overlooked to date since it’s not something that a startup really thinks like, and lack of metrics on contracts will be a huge barrier to entry for a lot of tradfi/corps eyeing launching an on-chain product.\nI started working on something that I intend to open source after I get it in working order, and I would love help if anyone is interested!\n\n[SendBlocks]: Hi all, I’m curios to learn what you believe are the most pressing gaps? The RFP Mentions Observability Tools, Live Fuzzing Tools, and Off-chain Alerting and Dispatcher Tools. Prioritization / more specific needs will enable us to bring the best offering to the table.\n\n[pkxro]: Hey @SendBlocks,\nAll of these are gaps. This was a deliberate choice to not be prescriptive in the types of applications we were looking for. RFP applicants should define how their tooling best helps the ecosystem.\n\n[pkxro]: Hey @0xmulch if you’re not applying to the RFP track, please reach out! Would love to see where you’re at here. Telegram: Contact @pkxro or dm me directly on the forum\n\n[SendBlocks]: Thank you for your response! We were prioritizing to fit into the requested schedule. We look forward to discussing our proposition further.\n\n[0xmulch]: Hey, would love to chat, I am not familiar with RFP, so if this is something that is grantable/ fundable I would quit my job today to work on this. Let me know how serious you are, I was considering going and trying to pitch it in the Colosseum, but still working on the MVP, value prop, TAM etc\n\n[0xmulch]: Just finished the Colosseum hackathon up and am contemplating which thing I would like to prioritize next, this is still one of my higher priority ideas, @pkxro did you ultimately cancel the RFP or were some submissions accepted? I can produce some preliminary arch docs if this is still on the table\nUpdate: Disreg-- just located the RFP DB-- glad you guys got something going. Thanks!\n\n[Gabynto]: This RFP is a great opportunity to develop program monitoring tools for real-time insights and threat management. Make sure to review the details and deadlines carefully, and consider collaborating or sharing notes on this thread to strengthen your proposal.\n\n",
            "comment_count": 8,
            "original_poster": "pkxro",
            "activity": "2024-07-31T10:41:41.498Z"
        },
        {
            "id": 1030,
            "title": "Pre-Deployment Program Analysis",
            "url": "https://forum.solana.com/t/pre-deployment-program-analysis/1030",
            "created_at": "2024-02-07T17:34:19.129Z",
            "posts_count": 3,
            "views": 1190,
            "reply_count": 0,
            "last_posted_at": "2024-02-29T22:01:35.917Z",
            "category_id": 10,
            "category_name": "RFP",
            "description": "Context\nThere exists a lack of Open Source security tooling that allow for Solana developers to alleviate their programs from a wide range of vulnerabilities. Open Source Formal Verification and Symbolic Analysis tooling, Fuzzing Frameworks and other technologies can better assist program developers in the journey of securing their programs before deployment or upgrades.\nPlease see the following RFP that outlines a request to create repeatable program analysis tooling. The Solana Foundation lays out a list of proposed solutions, but the technology used to secure programs is at the behest of the applicant.\nLogistics\nTake note of the application deadline (2/29/2024). The maximum grant amount is not included within the request as different security applications will have varying cost factors. The resulting finalist(s) will work with the Solana Foundation to receive an appropriate grant issued in USD-equivalent locked SOL with approachable, but rigorous milestones.\nGround Rules\nThis thread can be used for comments, questions, praise, and / or criticism, and is intended to be an open forum for any prospective responders. This thread is also an experiment in increasing the transparency through which RFPs are fielded by the Solana ecosystem too, so please be mindful that we’re all here to learn and grow.\nResponses to this RFP are not required to be public (but recommended), but if it is helpful to share notes or combine forces, then please use this thread for such purposes\nLink: Airtable - Solana Foundation Public RFP Database",
            "comments": "[Coinfabrik]: Bringing Scout to Solana\nDear Solana community,\nWe’re excited to present our proposal for this RFP, Scout, our open-source vulnerability detection tool. Whether you’re an entry-level developer or an expert, Scout is the perfect tool to improve the secure development lifecycle of your smart contract projects. Designed with ease of use in mind, Scout offers a seamless installation process, allowing you to focus on what matters most: creating innovative and secure smart contracts.\nWe are CoinFabrik, a leading research, development, and security auditing company specializing in Web3 technologies. This year marks our 10-year anniversary, and for the past 3 years, we’ve added value to the Solana ecosystem. CoinFabrik’s technical team has performed development and auditing for projects like Codigo.AI, Genopets, SmartChain, and Fitchin. Furthermore, we co-hosted the first Solana Hackathon in Argentina (Summer Sol Sessions Buenos Aires) and also had our booth at Breakpoint Amsterdam in 2023, presenting our smart contract testing tool SolBricks. Our commitment is to continue contributing to Solana’s developer growth and retention, and to foster the entry of new talent to help maintain and improve the network.\nOur team has an academic background in computer science and mathematics, adding up to decades of experience in cybersecurity and software development, including academic publications, patents turned into products, and conference presentations. Furthermore, we have an ongoing collaboration on knowledge transfer and open-source projects with the University of Buenos Aires.\nTool Overview\nScout is an open-source bug detection tool designed to assist developers and auditors in identifying potential security threats and applying best practices to smart contracts. It enhances contract security by detecting issues and suggesting remediations during development, thus ensuring the security of contracts before deployment.\nScout is a static analyzer equipped with specialized lints or detectors that pinpoint specific vulnerabilities. These lints are designed for easy integration, enabling contributors to add new detectors seamlessly. Scout includes a command-line interface (CLI) offering various output formats, along with a VSCode extension that highlights vulnerable code segments and provides explanations and remediation suggestions.\nAs a security companion, Scout’s comprehensive documentation and open-source approach encourage community contributions, elevating ecosystem security standards and best practices.\nHelp us bring Scout to Solana!\nWe want to hear from you! We look forward to any feedback the Solana developer community wants to share concerning our proposal to bring Scout into the ecosystem.\nWhich types of Solana vulnerabilities would you like our bug detection tool to focus on identifying? Your suggestions will help us refine our tool’s capabilities to better meet the community’s requirements and improve the network’s security.\n\n[maddavid]: The Whale Suite\nMad Shield is the Premiere Solana auditing and security solutions, providing clients with in-depth code review, design improvement and vulnerability analysis and security tooling. As seasoned security experts in the blockchain industry with a focus on the Solana ecosystem, we are excited to offer a pre-deployment testing tool for Solana smart contracts that help teams and developers to uncover potential vulnerabilities that are hard to detect and uncover through manual code review.\nOur goal is to empower the developers with a comprehensive tool that exhausts most of the categorical Solana vulnerabilities. In addition, our tool is to be used to exhibit emergent exploits that have not been discovered previously and thus potentially revealing new categories of attacks guided by educated guesses and business-logic related guidelines that the auditors suspect to cause critical deviation from program’s functionality.\nThe testing tool is meant to be used to monitor new program releases or upgrades before deployment to main-net/production, consistently checking the trust boundary and security guarantees between the incremental development cycles. This is significantly important as many of the programs providing infrastructure in the ecosystem such as SPL/MPL libraries have been extensively supporting user requested features that are honeypots for irregularity within the code to arise.\nMad Shield team is excited to bring this tool as a primitive for developers and smart contract designers alike to build better and higher quality code to help with the technical intricacies of the Solana smart programming model.\n\n",
            "comment_count": 2,
            "original_poster": "pkxro",
            "activity": "2024-02-29T22:01:35.917Z"
        },
        {
            "id": 1032,
            "title": "Program Verification Tooling",
            "url": "https://forum.solana.com/t/program-verification-tooling/1032",
            "created_at": "2024-02-07T17:49:35.234Z",
            "posts_count": 3,
            "views": 893,
            "reply_count": 1,
            "last_posted_at": "2024-02-29T14:10:26.511Z",
            "category_id": 10,
            "category_name": "RFP",
            "description": "Context\nSolana users and developers should have better insights into the security posture of programs they’re using or invoking. There should exist verification tooling, program scoring and APIs that let wallets, explorers and developers fetch security details about programs they’re interacting with. Additionally, auditors and program developers should be able to post attestations about the incremental audits of programs for ingestion into the tooling requested above.\nPlease see the following RFP that outlines a request to create program verification tooling. The Solana Foundation lays out a list of proposed solutions, but the technology used to verify programs is at the behest of the applicant.\nLogistics\nTake note of the application deadline (2/29/2024). The maximum grant amount is not included within the request as different security applications will have varying cost factors. The resulting finalist(s) will work with the Solana Foundation to receive an appropriate grant issued in USD-equivalent locked SOL with approachable, but rigorous milestones.\nGround Rules\nThis thread can be used for comments, questions, praise, and / or criticism, and is intended to be an open forum for any prospective responders. This thread is also an experiment in increasing the transparency through which RFPs are fielded by the Solana ecosystem too, so please be mindful that we’re all here to learn and grow.\nResponses to this RFP are not required to be public (but recommended), but if it is helpful to share notes or combine forces, then please use this thread for such purposes\nLink: Airtable - Solana Foundation Public RFP Database",
            "comments": "[0xGeorgii]: Hello, Solana community,\nI’m excited to see that Solana Foundations is interested in formal method application development in this RFP. Before submitting my proposal, I seek clarity on whether it aligns with the Foundations’ expectations for this RFP.\nI represent Inferara, a research group I lead that is focused on automated reasoning. We’re exploring the feasibility of formalizing specifications for blockchain systems within a proof system (e.g., Coq). We aim to create a mathematically grounded framework to aid developers in writing formal specifications and proving the properties of their code.\nOur Team’s Strengths:\nExtensive software engineering experience, including blockchain security and formal language analysis.\nAcademic expertise in mathematics, game theory, and distributed systems.\nProject Overview:\nOur research necessitates thorough preliminary studies, adopting an academic approach. For insight into our methodology, consider our article on Program Verification: background and notation.\nThe project is phased, starting with theoretical research milestones we are currently progressing in.\nInquiry to Solana Foundations:\nWould the Foundations be interested in detailed, academically styled papers as deliverables? Additionally, could code examples in Coq and Rust, illustrating the discussed concepts, be valuable?\nOur ultimate goal is to evolve this framework into a practical implementation. This would facilitate formal specifications and proofs for code within both Solana’s core and on-chain DApps.\nI am looking forward to hearing feedback and hoping our proposal can contribute to the Solana ecosystem.\n\n[pkxro]: Hi @0xGeorgii apologies for the late reply here.\nThis would better fit under our research initiative, as these security RFPs are focused on production use cases. Please reach out to me at Telegram: Contact @pkxro\n\n",
            "comment_count": 2,
            "original_poster": "pkxro",
            "activity": "2024-02-29T14:10:26.511Z"
        },
        {
            "id": 389,
            "title": "Alternative Archival Storage Technologies",
            "url": "https://forum.solana.com/t/alternative-archival-storage-technologies/389",
            "created_at": "2023-07-17T19:24:10.400Z",
            "posts_count": 9,
            "views": 1752,
            "reply_count": 4,
            "last_posted_at": "2023-12-18T17:34:38.433Z",
            "category_id": 10,
            "category_name": "RFP",
            "description": "Context\nArchival storage for Solana has historically expensive and centralized from the technology perspective; at the moment, BigTable tends to be the only reasonable choice for RPCs to store historical information back to genesis.\nSee this RFP on the development of technologies to enable alternative storage technologies and providers to provide low-cost, high-efficiency access to Solana archival data\nLogistics\nTake note of the end date (8/13) and be sure to make sure all criteria is met prior to sending in an application. The listed grant amount is a maximum allocation and is issued in USD-equivalent locked SOL and gated behind delivery milestones.\nGround Rules\nThis thread can be used for comments, questions, praise, and / or criticism, and is intended to be an open forum for any prospective responders. This thread is also an experiment in increasing the transparency through which RFPs are fielded by the Solana ecosystem too, so please be mindful that we’re all here to learn and grow.\nResponses to this RFP are not required to be public, but if it is helpful to share notes or combine forces, then please use this thread for such purposes.",
            "comments": "[JoakimEQ]: Is there some publicly available documentation of the often cited method of using Filecoin for this storage? I see it consistently mentioned by aeyakovenko:\n \n twitter.com\n \n \n \ntoly 🇺🇸\n@aeyakovenko\n \n \n \n @0xMert_ @0xrwu chain is archived to file coin, and probably eventually to arweave. We just need an easy way for indexers to recover a solana:// object from the archives\n 3:58 AM - 20 Jul 2023\n \n \n \n \n 11\n \n \n \n \n \n 1\n \n \n \n \n \n \n \nWould love to see what the pros and cons of this approach have been so far. Based on my knowledge of Filecoin, the cost might be quite prohibitive.\n\n[anjor]: I am working on this from the filecoin side along with folks from Triton. Triton just released https://old-faithful.net/ which has more details on how data is being onboarded to filecoin. Happy to answer any follow up questions!\nTo the point about cost being prohibitive, Filecoin is actually the cheapest option today. See this from messari:\n \n twitter.com\n \n \n \nMessari\n@MessariCrypto\n Decentralized storage networks range from 70% (@Storj) to 99% (@Filecoin) cheaper than @Amazon S3 🤯\n 12:00 AM - 17 Jan 2023\n \n \n \n \n 310\n \n \n \n \n \n 66\n\n[sven]: Can you elaborate on these requirements?\nSolution should provide relevant connection logic for the Solana RPC client\nSolution must prove equivalence to the Solana ledger as determined by random-sampling of RPC calls\nDoes this mean that the solution must include a separate RPC that runs on a subset of the data? (eg. an epoch)\nand\nA complete security audit must be completed prior to production launch.\nwho is responsible for this? if the submitter, should this be factored as a cost? (problematic as it’s an unknown)\n\n[jnwng]: Does this mean that the solution must include a separate RPC that runs on a subset of the data? (eg. an epoch)\nno, you can use the existing Solana RPC code. today, that RPC will pack things into BigTable / serve archival requests out of BigTable. the proposed solution needs to plug into that existing code to serve as a suitable replacement, and needs to store data from genesis to tip.\nwho is responsible for this? if the submitter, should this be factored as a cost? (problematic as it’s an unknown)\ngood point. don’t have a perfect answer for you since this component isn’t security-critical enough to require an audit; probably okay to waive this concern for the time being\n\n[matta]: Hello,\nWas an applicant accepted for this grant or did it just expire?\nI am unable to view the RFP at this time but I am interested in working on this.\nThe solution I have mind uses Apache Parquet archived to commodity object storage such as Amazon S3. I am confident that this approach would reduce costs while providing fast RPC access.\nCompared to the Old Faithful approach this would not be decentralized or verifiable but should be a more “plug and play” replacement for RPCs. Parquet has a lot of benefits: great compression, efficient remote queries, and high quality Rust crates, but it’s not a deterministic format. That being said, the lower operational costs would make building your own verified archives from the ledgers much more accessible.\n\n[pkxro]: Hey Matta,\nWe closed this RFP about a month ago and are starting the implementation process with the final participants. We had more than 13 applicants across a wide spectrum of tooling choices and will be able to give more details once the details are finalized.\nWe will likely have follow-up RFPs as the landscape for archival and the state of RPCs is ever evolving. We’ll be sure to post any new details on the forum.\n\n[ripatel-jump]: The solution I have mind uses Apache Parquet archived to commodity object storage such as Amazon S3. I am confident that this approach would reduce costs while providing fast RPC access.\nSorry for the late reply – I’m very interested in the Apache Parquet solution. We can make it verifiable for sure. The community needs a ledger data format that is language-agnostic and compact so we can replay Solana Labs data in Firedancer and vice versa. Have you started any work on this? A Parquet format would be easier to work with than Filecoin/CAR (which solves a different problem).\n\n[matta]: Hey there,\nWe are doing some work adjacent to this but aren’t working on Parquet specifically at the moment. I’m actually in the process of backporting your patches for Geyser support to old ledger tool versions at the moment \nI did do some initial research into using Parquet and the most straightforward encoding of the ledger data is not actually that compact. The data Parquet compresses well (slot, block time, etc.) is not what takes up the majority of the space. Here are my notes from when I was researching this: https://gist.github.com/matt-allan/851499ed79ffdd48af3c4949270866fc\nI would love to talk more and see if we can collaborate on something. I will send you a message. If anyone else on this thread is interested in collaborating too please let me know!\n\n",
            "comment_count": 8,
            "original_poster": "jnwng",
            "activity": "2023-12-18T17:34:38.433Z"
        },
        {
            "id": 634,
            "title": "Test Validator Plugin Framework",
            "url": "https://forum.solana.com/t/test-validator-plugin-framework/634",
            "created_at": "2023-10-24T03:18:58.316Z",
            "posts_count": 15,
            "views": 1813,
            "reply_count": 7,
            "last_posted_at": "2023-12-17T17:37:13.259Z",
            "category_id": 10,
            "category_name": "RFP",
            "description": "Context\nWhen developers are building locally using local-test-validator, they often want to build on top of protocols already live on mainnet-beta. They can load each individual program and required account with CLI flags, but it is tedious and takes a lot of time for each developer. Not only that, but some programs require a level of traditional infrastructure to be working properly, which the developer will also be required to learn just to build locally.\nSee the RFP outlining a framework that can:\nLoad programs from mainnet-beta on start\nLoad any account from mainnet-beta on start\nUpdate an account’s data on start\nRun traditional infrastructure as needed to run the program\nLogistics\nTake note the end date (11/15) and be sure to make sure all criteria is met prior to sending in an application. The listed grant amount is a maximum allocation and is issued in USD-equivalent locked SOL and gated behind delivery milestones.\nGround Rules\nThis thread can be used for comments, questions, praise, and / or criticism, and is intended to be an open forum for any prospective responders. This thread is also an experiment in increasing the transparency through which RFPs are fielded by the Solana ecosystem too, so please be mindful that we’re all here to learn and grow.\nResponses to this RFP are not required to be public, but if it is helpful to share notes or combine forces, then please use this thread for such purposes.",
            "comments": "[jimii]: Here is my suggestion for implementing the plugin marketplace.\nInstead of building it from the ground up, I recommend looking into the feasibility of utilizing the APR (Anchor Program Repository) for retrieving programs and their corresponding versions.\nBy leveraging the existing APR infrastructure, we can expedite the development process and ensure a more efficient marketplace integration.\nAt the moment it is not up but talking to the anchor team should be helpful.\n\n[jacobcreech]: I want this to exist outside of Anchor - the plugins can be built for both Native and Anchor programs.\n\n[SE7EN]: Hi @jacobcreech,\nWe have a few questions regarding this RFP:\nAbility to run traditional infrastructure as needed to operate the program\nWhat do you mean by “traditional” infrastructure?\nThe solution must offer a means of discovering different plugins for each program\nAre you suggesting that users should be able to discover plugins by providing the program or account address, similar to docker images discovery or like crates.io homepage?\nOne plugin must include traditional infrastructure as part of the load\nCould you provide an example or clarify your specific requirements for this?\nDistribution model of plugins\nAre you looking for a separate application to facilitate the discovery of plugins, similar to something like npm?\n\n[jacobcreech]: What do you mean by “traditional” infrastructure?\nLike cranks. Not going as far to say something like postgres, but I want to make sure something basic like cranks can run. Maybe going as far to make sure something like a bot that can market make to create fake activity in a docker container as well.\nAre you suggesting that users should be able to discover plugins by providing the program or account address, similar to docker images discovery or like crates.io homepage?\nYes. There needs to be a way for people to both upload and retrieve different plugins.\nOne plugin must include traditional infrastructure as part of the load\nSomething like create an Openbook plugin and make sure the crank is running when the plugin is installed and local validator is started.\nAre you looking for a separate application to facilitate the discovery of plugins, similar to something like npm?\nDiscovery and upload.\n\n[metselder]: Hey @jacobcreech,\nI have couple of questions, I’d be grateful to understand more, so that i can thoroughly look into and send a proposal over for review\nWhen developers are building locally using local-test-validator, they often want to build on top of protocols already live on mainnet-beta. They can load each individual program and required account with CLI flags, but it is tedious and takes a lot of time for each developer\nIn order to comprehensively assess the necessary functionalities, particularly the “Ability to load programs from mainnet-beta on start” and “Ability to load any account from mainnet-beta on start,” providing precise examples of commands for loading individual programs becomes paramount. This would greatly assist in developing a proof of concept (POC) and exploring integration possibilities. Although seemingly straightforward, validation is essential due to the limited descriptive nature of the current documentation regarding the envisioned capabilities. A plausible example command could resemble the following:\nsolana program dump -u m 9xQeWvG816bUx9EPjHmaT23yvVM2ZWbrrpZb9PusVFin serum_dex_v3.so &amp;&amp; solana-test-validator --bpf-program 9xQeWvG816bUx9EPjHmaT23yvVM2ZWbrrpZb9PusVFin serum_dex_v3.so --reset\nSolution must support a configuration file for each plugin, denoting how to load each program and plugin - Could you elaborate on the envisioned configurability?\nSolution must have a way of packaging these plugins per program - Could you provide further details on the anticipated outcome? For instance, should a developer possess a configuration file for a program to be cloned from mainnet-beta and initiated with the validator? Subsequently, if the developer wishes to load another program, would the local-test-validator be capable of starting with a distinct configuration file for the latter program, specified as a path argument to the plugin?\n\n[GregoryLibert]: Hi,\nFor the RFP’s requirement of a configuration file for each plugin, could you clarify its function?\nIs it meant to:\nAct as a descriptive, recipe-like file for setting up the plugin, or\nServe as a means to input parameters into the plugin? Additionally, if it’s the latter, should there be an option to modify these parameters via the user interface?\nAlso, concerning the requirements for packaging, distributing, and discovering plugins, are you envisioning a system akin to an API marketplace, complete with a website and a CLI tool for searching, acquiring, and uploading plugins? If this is the case, are there any specific hosting constraints or requirements we should be aware of?\n\n[jacobcreech]: The commands you posted are correct. The goal of this is to hide or abstract all of the commands away though so it can load more gracefully. You find a lot of people that just want NFTs locally are rebuilding the same command, but what if they need 100+ accounts? Not scalable anymore.\nSolution must support a configuration file for each plugin, denoting how to load each program and plugin\nSure. Let’s say you have a config file for a plugin formatted something like this in yml:\nprograms:\n- programId1 or programName(as referenced in explorer)\n- programId2\naccounts:\n- account1\n- account2\noverrides:\n- accountAddress: address\n- accountOwner: address\nThis is just an example. Programs within the config will get me the full list of programs to load for that specific plugin, same with accounts. Overrides could potentially overwrite the data so that you can get more usefulness out of it locally. For example, you pull a USDC account from mainnet down to local but you’re not the owner, so you update it so you can transfer the USDC around.\nCould you provide further details on the anticipated outcome? For instance, should a developer possess a configuration file for a program to be cloned from mainnet-beta and initiated with the validator? Subsequently, if the developer wishes to load another program, would the local-test-validator be capable of starting with a distinct configuration file for the latter program, specified as a path argument to the plugin?\nLet’s say you have a directory that contains plugins on your local that your local validator plugin framework picks configs from. For the sake of discussion, structured as follows:\n/plugins\n /mango\n /drift\n /openbook\n /metaplex\nEach plugin would then have a config file like mentioned earlier in this post to give the information about what accounts to load to successfully run the program locally, plug potentially some additional accounts(like USDC) to make developing locally on these programs even easier. Framework would crawl through each directory, grab the account, load validator with the accounts loaded, override any accounts necessary, and then start.\nIdeally there’s an easy way with the distribution to also just “install” these plugins so that devs are not moving folders around as well. That’s in the separate milestone.\n\n[jacobcreech]: Configuration ideally like a recipe for each program that the framework then uses to load on test validator start. It’d be cool to have input parameters, but that would be increased scope of this current rfp.\nPackaging, distributing, discovering - Could be just a website that helps discovery + easy install of plugins. Something like how plugins are discovered and installed for something like minecraft or skyrim mods today.\n\n[SE7EN]: @jacobcreech\nHi,\nI have attempted to submit our proposal several times, but I consistently encounter the same error. How can I successfully submit our proposal?\nimage1392×760 84.9 KB\n\n[jacobcreech]: Hey @SE7EN, I just ran a test submission and it worked. Could you reload and try again?\n\n[SE7EN]: It worked, thank you\n\n[SE7EN]: Hi @jacobcreech,\nWe’ve submitted our proposal, but haven’t received any feedback yet. Could you please provide an update on the status of the RFP?\nThanks,\n\n[jacobcreech]: We should be reaching out to everyone this week. Apologies, holidays in the US got in the way.\n\n[ripatel-jump]: What is local-test-validator? Do you mean the solana-test-validator binary?\nThe solana-genesis command already supports a “primordial accounts file” which can be used to deploy programs and accounts from mainnet beta on start. This can then be used with a solana-validator. We do this regularly with Firedancer development.\nI don’t think there are any Solana monorepo changes required to support this, apart from a 100 line shell script maybe to glue things together.\n\n",
            "comment_count": 14,
            "original_poster": "jacobcreech",
            "activity": "2023-12-17T17:37:13.259Z"
        },
        {
            "id": 367,
            "title": "The Solana app for Ledger devices",
            "url": "https://forum.solana.com/t/the-solana-app-for-ledger-devices/367",
            "created_at": "2023-07-11T18:22:15.798Z",
            "posts_count": 4,
            "views": 1547,
            "reply_count": 1,
            "last_posted_at": "2023-11-07T16:29:29.423Z",
            "category_id": 10,
            "category_name": "RFP",
            "description": "Context\nThe Solana app on Ledger is a crucial part of the self-custody story within the Solana ecosystem. Until now, that app has been faithfully developed by contributors at Solana Labs but we are now looking for a new steward to continue to build out this public good.\nSee an RFP for further development on the Ledger app here, starting with:\nsupport for “off-chain message signing”\nthe “ComputeBudget” instruction\na discovery document for future development across the suite of Ledger devices\nAll code is subject to code review from the Solana Labs and Ledger firmware teams.\nLogistics\nTake note of the end date (7/31) and be sure to make sure all criteria is met prior to sending in an application. The listed grant amount is a maximum allocation and is issued in USD-equivalent locked SOL and gated behind delivery milestones. If for whatever reason this isn’t a workable solution, please let us know in the application or reach out to me directly.\nGround Rules\nThis thread can be used for comments, questions, praise, and / or criticism, and is intended to be an open forum for any prospective responders. This thread is also an experiment in increasing the transparency through which RFPs are fielded by the Solana ecosystem too, so please be mindful that we’re all here to learn and grow.\nSubmissions to this RFP are not required to be public, but if it is helpful to share notes or combine forces, then please use this thread for such purposes.",
            "comments": "[SE7EN]: Hey, I’m from BlockyDevs we have a few questions:\nOne of the deliverables is Off-chain message signing support for Ledger devices. However, we noticed that off-chain message signing is already implemented in the Solana Ledger app, specifically in the file handle_sign_offchain_message.c, available at Link. Could you please clarify what additional work should be done regarding this deliverable?\nRegarding the deliverable for ComputeBudget instruction support for Ledger devices, should we enable the user to set their own gas unit limit for each transaction?\nFor the Discovery document on memory, could you provide more details about what is expected? Should it be a summary for each Ledger device, or is it more like a research paper that includes analysis, measurements, chart, tests?\nConcerning the section on “proposal on future accommodation for well-known tokens/instructions, including the Token-2022,” are we expected to demonstrate that our example implementation meets all the security requirementsor only lacks memory? How should we interpret this requirement?\nFor Off-chain message signing, are we only considering supporting ED25519?\n\n[jnwng]: thank you for the questions!\n SE7EN:\nOne of the deliverables is Off-chain message signing support for Ledger devices. However, we noticed that off-chain message signing is already implemented in the Solana Ledger app, specifically in the file handle_sign_offchain_message.c, available at Link . Could you please clarify what additional work should be done regarding this deliverable?\noffchain messages must adhere to the finalized message signing specification laid out here. see in-progress work to be finalized here\nthe Solana CLI support for the same OCMSF needs to be updated with tests\n SE7EN:\nRegarding the deliverable for ComputeBudget instruction support for Ledger devices, should we enable the user to set their own gas unit limit for each transaction?\ngood question. probably unnecessary, i think just the display of the provided ComputeBudget is suitable.\n SE7EN:\nFor the Discovery document on memory, could you provide more details about what is expected? Should it be a summary for each Ledger device, or is it more like a research paper that includes analysis, measurements, chart, tests?\nthe discovery document is intended to scaffold future work to support additional Ledger devices in the future, as well as streamline the development pipeline needed to add support for other actions and tokens in the future. this is a “we don’t know what we don’t know” piece but critical to know if there’s low-hanging fruit to improve the application or if its maxxed out.\n SE7EN:\nConcerning the section on “proposal on future accommodation for well-known tokens/instructions, including the Token-2022,” are we expected to demonstrate that our example implementation meets all the security requirements or only lacks memory? How should we interpret this requirement?\ni’m not fully sure how to answer this, but if you’re asking what i think you’re asking, i really just want to make sure that any future revamp of the app accommodates for the addition of new instructions in the future, not just existing ones\n SE7EN:\nFor Off-chain message signing, are we only considering supporting ED25519?\nyes, i think that would be the only curve that makes sense. but let me know otherwise.\n\n[jnwng]: this RFP has been closed with two submissions, and one winner that has been selected has already begun development. our estimated time of delivery is by the end of the year.\n\n",
            "comment_count": 3,
            "original_poster": "jnwng",
            "activity": "2023-11-07T16:29:29.423Z"
        },
        {
            "id": 526,
            "title": "Unified Security Token/RWA Program",
            "url": "https://forum.solana.com/t/unified-security-token-rwa-program/526",
            "created_at": "2023-09-10T15:41:28.646Z",
            "posts_count": 6,
            "views": 1080,
            "reply_count": 1,
            "last_posted_at": "2023-10-06T13:44:13.369Z",
            "category_id": 10,
            "category_name": "RFP",
            "description": "Context\nAs discussed in the sRFC section of the forum sRFC 00020: RWA/Security Token Standard, this RFP is for a RWA program that can generalize to many RWA representations.\nRWA RFP application link\nLogistics\nApplication deadline is Sept 29th, 2023 and be sure to make sure all criteria is met prior to sending in an application. The listed grant amount is a maximum allocation and is issued in USD-equivalent locked SOL and gated behind delivery milestones. If for whatever reason this isn’t a workable solution, please let us know in the application or reach out to me directly.\nGround Rules\nThis thread can be used for comments, questions, praise, and / or criticism, and is intended to be an open forum for any prospective responders. This thread is also an experiment in increasing the transparency through which RFPs are fielded by the Solana ecosystem too, so please be mindful that we’re all here to learn and grow.\nResponses to this RFP are not required to be public, but if it is helpful to share notes or combine forces, then please use this thread for such purposes.",
            "comments": "[jimii]: Off-topic questions,\nWhen a project is decided on, will there be an announcement?\nLooking at the evaluation criteria, it says that it is ideal for the project to be open source, Does this apply when the team start working on the projects or at a later date after the team has executed its idea?\n\n[eliasi]: It’s not clear what are the expected deliverables.\n(assuming the standard itself doesn’t have a code)\nIs it an example of the standard? Something else?\nMay I ask for more details?\n\n[Ves]: Hey @Tamgros, trying to understand whether you are looking explicitly for the development of the Token or also a team to run the program.\n\n[Tamgros]: The deliverable is a Solana Program on mainnet. There are also some milestones including a spec and testing.\n\n[thisisambros]: Hi,\nWe have submitted our proposal today as Rubicon Studio SA.\nI am aware that we’re past the deadline, but it would be great if it could be considered\n\n",
            "comment_count": 5,
            "original_poster": "Tamgros",
            "activity": "2023-10-06T13:44:13.369Z"
        }
    ],
    "SIMD": [
        {
            "id": 15,
            "title": "Solana Improvement Documents Info",
            "url": "https://forum.solana.com/t/solana-improvement-documents-info/15",
            "created_at": "2023-02-23T02:18:15.856Z",
            "posts_count": 2,
            "views": 1352,
            "reply_count": 0,
            "last_posted_at": "2023-02-23T02:18:59.533Z",
            "category_id": 5,
            "category_name": "SIMD",
            "description": "General Information regarding SIMDs\nHow to submit a SIMD\nTutorial\nGather feedback on your SIMD idea either here or in the Solana Tech Discord under the core-technology channel.\nOnce you get enough discussion on the SIMD where you think it may have a good chance of getting accepted, write your SIMD using the SIMD template\nCreate a PR to solana-foundation/solana-improvement-documents\nSIMD maintainers will assign SIMD number to your SIMD\nNotes:\nDo not copy paste the SIMD itself in the forum. Just post an overview and a link to the SIMD itself.\nSIMD Guidelines\nFormal guidelines SIMD-1\nCore Community Call\nOnce a month there is a core community call between core developers on the Solana protocol.\nYou can propose a topic for the agenda by making a PR to the latest agenda in the core-community-call repository.\nThe call is open to the public for viewing! If you’re interested in attending you can add the public calendar to be notified.\nYou can find a playlist of all previous core community calls on Youtube.",
            "comments": "[jacobcreech]: \n\n",
            "comment_count": 1,
            "original_poster": "jacobcreech",
            "activity": "2023-02-23T02:18:59.533Z"
        },
        {
            "id": 2890,
            "title": "State growth problem - Accounts Lattice Hash",
            "url": "https://forum.solana.com/t/state-growth-problem-accounts-lattice-hash/2890",
            "created_at": "2025-01-09T22:24:12.538Z",
            "posts_count": 1,
            "views": 117,
            "reply_count": 0,
            "last_posted_at": "2025-01-09T22:24:12.585Z",
            "category_id": 5,
            "category_name": "SIMD",
            "description": "I couldn´t find that proposal here. Does anyone know more about it? New Solana Proposal Advocates Fundamental Scalability Change\nI’d like to understand it better. Specifically:\nHow exactly will the Accounts Lattice Hash improve scalability compared to the current system?\nAre there any trade-offs or potential risks associated with this approach?\nThank you, Solana to the moon!",
            "comments": "",
            "comment_count": 0,
            "original_poster": "solanabigguy12",
            "activity": "2025-01-09T22:24:12.585Z"
        },
        {
            "id": 2015,
            "title": "Add new Warning and Error fields to JSON RPC results",
            "url": "https://forum.solana.com/t/add-new-warning-and-error-fields-to-json-rpc-results/2015",
            "created_at": "2024-08-20T13:51:51.006Z",
            "posts_count": 1,
            "views": 187,
            "reply_count": 0,
            "last_posted_at": "2024-08-20T13:51:51.059Z",
            "category_id": 5,
            "category_name": "SIMD",
            "description": "The Solana developer ecosystem has evolved to the point where most dApps no longer directly calling JSON RPC endpoints directly. Instead these calls are often obfuscated behind frameworks or protocol-specific libraries that abstract away the raw calls.\nIn an ideal world these frameworks and libraries would monitor and manage deprecated RPC calls and update current and previous versions to propagate deprecated and removed methods up to the dApp developer. This would provide a clear signal that the dApp developer should update their library versions to remove deprecated calls.\nHowever, we all know this isn’t the case. There’s no guarantee that libraries or frameworks will be updated, that developers will pull in the latest patch version, or that libraries will even be maintained. Instead I would like to propose that the endpoint returns on Solana RPCs be updated to include Warning and Error fields to indicate a common interface for signaling deprecation or removed endpoints. This would still require current libraries and frameworks to update to process these fields, but going forward it would provide a simple schema for these to propagate deprecations upwards in the stack. This method would also operate more dynamically with runtime-level checks. Frameworks would be able to detect deprecation and removal as soon as the RPC updates, rather than requiring the framework to be updated with the correct deprecation notices.\nAs an example:\n&gt; curl https://api.mainnet-beta.solana.com -X POST -H \"Content-Type: application/json\" -d '\n { \"jsonrpc\":\"2.0\", \"id\": 1, \"method\":\"getFees\"}\n' | jq\n % Total % Received % Xferd Average Speed Time Time Time Current\n Dload Upload Total Spent Left Speed\n100 313 100 262 100 51 626 121 --:--:-- --:--:-- --:--:-- 747\n{\n \"jsonrpc\": \"2.0\",\n \"result\": {\n \"context\": {\n \"apiVersion\": \"1.18.18\",\n \"slot\": 320162120\n },\n \"value\": {\n \"blockhash\": \"5xuwdC9NJ5ur8DLXFXVHESnUb74SuwgxQP9wgv9XrJDK\",\n \"feeCalculator\": {\n \"lamportsPerSignature\": 5000\n },\n \"lastValidBlockHeight\": 308386678,\n \"lastValidSlot\": 320162420\n }\n },\n \"id\": 1\n}\nA call to the getFees endpoint provides no versioning or notice that the call has been deprecated and will soon be removed.\ncurl https://api.devnet.solana.com -X POST -H \"Content-Type: application/json\" -d ' \n { \"jsonrpc\":\"2.0\", \"id\": 1, \"method\":\"getFees\"}\n' | jq\n % Total % Received % Xferd Average Speed Time Time Time Current\n Dload Upload Total Spent Left Speed\n100 129 100 78 100 51 271 177 --:--:-- --:--:-- --:--:-- 449\n{\n \"jsonrpc\": \"2.0\",\n \"error\": {\n \"code\": -32601,\n \"message\": \"Method not found\"\n },\n \"id\": 1\n}\nAnd as of 2.0 the method is completely gone.\nUnder this proposal, the above call would look as follows with the new Warning/Error fields:\nWith deprecation\n&gt; curl https://api.mainnet-beta.solana.com -X POST -H \"Content-Type: application/json\" -d '\n { \"jsonrpc\":\"2.0\", \"id\": 1, \"method\":\"getFees\"}\n' | jq\n % Total % Received % Xferd Average Speed Time Time Time Current\n Dload Upload Total Spent Left Speed\n100 313 100 262 100 51 626 121 --:--:-- --:--:-- --:--:-- 747\n{\n \"jsonrpc\": \"2.0\",\n \"result\": {\n+ \"warning\": {\n+ \"message\": \"Deprecated as of 1.9. Removal scheduled for 2.0.\",\n+ \"replacement\": \"getFeeForMessage\"\n+ },\n \"context\": {\n \"apiVersion\": \"1.18.18\",\n \"slot\": 320162120\n },\n \"value\": {\n \"blockhash\": \"5xuwdC9NJ5ur8DLXFXVHESnUb74SuwgxQP9wgv9XrJDK\",\n \"feeCalculator\": {\n \"lamportsPerSignature\": 5000\n },\n \"lastValidBlockHeight\": 308386678,\n \"lastValidSlot\": 320162420\n }\n },\n \"id\": 1\n}\nAfter removal\ncurl https://api.devnet.solana.com -X POST -H \"Content-Type: application/json\" -d ' \n { \"jsonrpc\":\"2.0\", \"id\": 1, \"method\":\"getFees\"}\n' | jq\n % Total % Received % Xferd Average Speed Time Time Time Current\n Dload Upload Total Spent Left Speed\n100 129 100 78 100 51 271 177 --:--:-- --:--:-- --:--:-- 449\n{\n \"jsonrpc\": \"2.0\",\n \"error\": {\n \"code\": -32601,\n- \"message\": \"Method not found\",\n+ \"message\": \"Method **getFees** not found\",\n+ \"replacement\": \"getFeesForMessage\"\n },\n \"id\": 1\n}\nFrameworks and libraries could then detect these new fields and propagate the associated warning/error upwards to developers and point them to the latest compatible version.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "blockiosaurus",
            "activity": "2024-08-20T13:51:51.059Z"
        },
        {
            "id": 1956,
            "title": "Create a Cluster SysVar",
            "url": "https://forum.solana.com/t/create-a-cluster-sysvar/1956",
            "created_at": "2024-08-08T14:09:09.936Z",
            "posts_count": 1,
            "views": 155,
            "reply_count": 0,
            "last_posted_at": "2024-08-08T14:09:09.993Z",
            "category_id": 5,
            "category_name": "SIMD",
            "description": "Every once and a while the question comes up of detecting the Solana cluster (mainnet, testnet, devnet) from within a program. As far as I know, this is still impossible. This issue has compounded recently with the explosion of SVM L2s, rollups, and new clusters.\nTo prevent the need to fragment codebases, or require additional program configs to be created, I propose the addition of a Cluster System Variable to indicate the SVM blockchain and specific cluster being used. This will allow programs to dynamically determine their execution environment.\nMy suggested format would be something like the following:\n#[repr(C)]\npub enum Cluster: {\n mainnet,\n devnet,\n testnet,\n other(String),\n}\n#[repr(C)]\npub struct ClusterDetails {\n pub blockchain: String,\n pub cluster: Cluster,\n}",
            "comments": "",
            "comment_count": 0,
            "original_poster": "blockiosaurus",
            "activity": "2024-08-08T14:09:09.993Z"
        },
        {
            "id": 1111,
            "title": "SIMD-0033 Test Results",
            "url": "https://forum.solana.com/t/simd-0033-test-results/1111",
            "created_at": "2024-02-26T22:15:11.618Z",
            "posts_count": 1,
            "views": 370,
            "reply_count": 0,
            "last_posted_at": "2024-02-26T22:15:11.670Z",
            "category_id": 5,
            "category_name": "SIMD",
            "description": "Hi, I’ve created a write-up of the resuts of the testing that was performed for the Timely Vote Credits implementation (which came out of SIMD-0033). Comments welcome!\n \n \n HackMD\n \n \n \nSolana Timely Vote Credits Feature Test Results - HackMD\n \nThe Timely Vote Credits (TVC) feature is enabled in two stages by two separate features:",
            "comments": "",
            "comment_count": 0,
            "original_poster": "zantetsu",
            "activity": "2024-02-26T22:15:11.670Z"
        },
        {
            "id": 914,
            "title": "SIMD-48: Secp256r1 Precompile",
            "url": "https://forum.solana.com/t/simd-48-secp256r1-precompile/914",
            "created_at": "2024-01-05T19:26:35.303Z",
            "posts_count": 2,
            "views": 643,
            "reply_count": 0,
            "last_posted_at": "2024-01-05T19:50:54.553Z",
            "category_id": 5,
            "category_name": "SIMD",
            "description": "Summary\nSIMD-48 outlines an implementation of the secp256r1 ECDSA verification routine as a precompile in the Solana runtime.\nA current testing repo for SIMD-48 can be found here\nSince Firedancer is being developed concurrently to the Solana Labs runtime, considerations towards the implementation of the precompile in C need to be made. Specifically towards the reproducibility of the verification operation. Any potential discrepancy would lead to serious security risks as well as a chain fork.\nThe OpenSSL implementation of secp256r1 should serve as the underlying reference point, as it’s one of the most well maintained and scrutinised cryptography implementations. As its written in C it can additionally serve as a reference point for the development of the Firedancer implementation.\nCurrently the test repo includes programatic analysis of test vector results of both the SIMD-48 implementation as well as the OpenSSL implementation.\nThis forum serves as a place to discuss the methodology behind ensuring a safe and reproducible implementation of SIMD-48.",
            "comments": "[BasedOrion]: An updated spec is currently under review here\n\n",
            "comment_count": 1,
            "original_poster": "BasedOrion",
            "activity": "2024-01-05T19:50:54.553Z"
        },
        {
            "id": 297,
            "title": "SIMD-0052: Add Transaction Proof and Block Merkle for Light Clients",
            "url": "https://forum.solana.com/t/simd-0052-add-transaction-proof-and-block-merkle-for-light-clients/297",
            "created_at": "2023-06-08T07:27:38.555Z",
            "posts_count": 2,
            "views": 574,
            "reply_count": 0,
            "last_posted_at": "2023-06-08T07:28:33.562Z",
            "category_id": 5,
            "category_name": "SIMD",
            "description": "Summary\nWe proposed a SIMD that modifies the blockhash to be computed as a Merkle Tree and include transaction statuses , this was originally part of the SPV proposal but wasn’t implemented and is an open issue.\nWe would also need to include transaction logs in the Merkle to verify certain state changes.\nOther changes include an RPC call that provides an inclusion proof from the transaction of interest to the bankhash.\nProposal\nSIMD-0052: https://github.com/solana-foundation/solana-improvement-documents/pull/52",
            "comments": "[Anoushk]: Here’s the original SPV proposal, adding it here because I wasn’t allowed more than 2 links\n\n",
            "comment_count": 1,
            "original_poster": "Anoushk",
            "activity": "2023-06-08T07:28:33.562Z"
        },
        {
            "id": 48,
            "title": "Bidirectional QUIC communication channel",
            "url": "https://forum.solana.com/t/bidirectional-quic-communication-channel/48",
            "created_at": "2023-03-20T16:28:05.659Z",
            "posts_count": 1,
            "views": 532,
            "reply_count": 0,
            "last_posted_at": "2023-03-20T16:28:05.729Z",
            "category_id": 5,
            "category_name": "SIMD",
            "description": "Bidirectional QUIC channel to track transaction lifecycle\nSummary\nCurrently, it is very hard to understand what happens to an individual transaction in the cluster.\nAs Solana is a distributed cluster each transaction can be forwarded to different nodes, may be rejected\nwithout any warning in the sig-verify stage, the connection could be dropped, and much more. As a client\nwhen I send a transaction to the cluster the only feedback that I have is whether the transaction is\nin the blocks or not. This feedback is insufficient when there is congestion in the network and the\ntransactions are dropped without any information about where this process is happening and which\npart of the code the cluster is struggling to keep up.\nAs Solana is being developed fast and we have multiple development roadmaps to improve the scheduling\nstage of the cluster. I propose that we add a new notification mechanism to the Solana cluster\nto understand more about the transaction lifecycle.\nThis will make it easier to develop, integrate, scale, and understand the Solana cluster.\nImplementation:\nFull proposal : Proposal for quic bidirectional reply by godmodegalactus · Pull Request #30161 · solana-labs/solana · GitHub\nPOC: Gmg/bidirectional quic replies (v1.14) by godmodegalactus · Pull Request #29954 · solana-labs/solana · GitHub",
            "comments": "",
            "comment_count": 0,
            "original_poster": "gmgalactus",
            "activity": "2023-03-20T16:28:05.729Z"
        },
        {
            "id": 12,
            "title": "About the SIMD category",
            "url": "https://forum.solana.com/t/about-the-simd-category/12",
            "created_at": "2023-02-23T01:33:35.510Z",
            "posts_count": 2,
            "views": 432,
            "reply_count": 0,
            "last_posted_at": "2023-02-24T05:13:03.800Z",
            "category_id": 5,
            "category_name": "SIMD",
            "description": "Discussions about specific SIMDs (Solana Improvement Documents), and general proposals that may become SIMDs. If applicable, specify the SIMD issue # in the topic title.",
            "comments": "[jacobcreech]: \n\n",
            "comment_count": 1,
            "original_poster": "jacobcreech",
            "activity": "2023-02-24T05:13:03.800Z"
        }
    ],
    "Releases": [
        {
            "id": 14,
            "title": "About the Releases category",
            "url": "https://forum.solana.com/t/about-the-releases-category/14",
            "created_at": "2023-02-23T01:46:31.428Z",
            "posts_count": 1,
            "views": 436,
            "reply_count": 0,
            "last_posted_at": null,
            "category_id": 7,
            "category_name": "Releases",
            "description": "News on upcoming Solana releases and their changelogs",
            "comments": "",
            "comment_count": 0,
            "original_poster": "jacobcreech",
            "activity": null
        },
        {
            "id": 175,
            "title": "Version 1.14.17 Release Summary",
            "url": "https://forum.solana.com/t/version-1-14-17-release-summary/175",
            "created_at": "2023-05-02T23:39:37.910Z",
            "posts_count": 1,
            "views": 4089,
            "reply_count": 5,
            "last_posted_at": "2023-05-02T23:39:37.973Z",
            "category_id": 7,
            "category_name": "Releases",
            "description": "With the recent trepidation around v1.14 of the Solana Labs validator client, the following document aims to outline the major changes landing in v1.14.17, release timelines, and testing. This release version includes significant changes and improvements to the Solana Labs validator client, many of which have been eagerly awaited for some time.\nThe tentative upgrade schedule is as follows:\n• 2023-05-08 - Ask for volunteers to upgrade 10% of mainnet-beta stake to v1.14.17.\n• 2023-05-15 - Ask for volunteers to upgrade 25% of mainnet-beta stake to v1.14.17.\n• 2023-05-22 - Recommend v1.14.17 for all mainnet-beta validators and RPC nodes.\nThe Solana Labs client version 1.14.17 has undergone a rigorous release engineering testing regime in preparation for the upcoming release. Testnet has been used to simulate the exact upgrade processes that will be done on mainnet-beta during rollout of the release.\nFirst, testnet was downgraded and feature gates were reverted to match current mainnet-beta on v1.13.7. Following this, testnet was gradually upgraded to v1.14.17 over a period of 27 days. This was done gradually to identify any interoperability issues between the two versions. Then testnet was live downgraded from v1.14.17 to v1.13.7 to demonstrate that the cluster can downgrade versions while remaining online. From here, testnet was again upgraded to v1.14.17. This time it was done quickly, mostly as a way to get back to v1.14.17 for further testing, but it also provided engineers another period of testing mixed Solana Labs client versions live on testnet. As previously mentioned, on the first downgrade to v1.13.7 feature gates in v1.14.17 were reverted. The final part of the release engineering testing process was to re-enabled the feature gates to verify the full functionality of v1.14.17 on testnet. This also brought testnet back to the same state as before the February 25th incident.\nThroughout this process stress testing on testnet was ongoing. This includes standard benchmarking and DDOS tests, as well as the new test cases outlined in detail in the outage report below. No outstanding issues were identified. In addition to the testnet testing, Solana Labs has been running test nodes against mainnet-beta with an assortment of 1.14 versions near the tip of the release. This has been ongoing since the February 25th incident and the test nodes have yet to encounter any problems.\nIf you would like to learn more about v1.14.17 you can start reading the pull requests which are linked in the notes on the release page.\nIf you’d like to better understand what caused the last outage check out the outage report.\ntldr: v1.14 was not the culprit; v1.13.6 was also susceptible (both v1.13 and v1.14 have since been patched\nA comprehensive list of changes in v1.14 can be found by reading the pull requests linked above, however an index of some of the more significant changes is listed below:\nCompact Vote State*\nStake Program Changes*:\nMinimum Stake Delegation Vote\nGet Minimum Stake Delegation Instruction\nUndelegated Stakes Allowed Below Minimum\nDeactivate Delinquent Stake Instruction\nImprovements to Caching\nRPC Call to Get Estimated Priority Fees\nAccounts Index on Disk\nTurbine Improvements*\nErasure Batch Construction &amp; Transmission\nRemove Redundant Turbine Path\nIncreased TX Account Lock Limits*\n*These features will not go live immediately upon 1.14.17 release but at a later date. These features are feature gated and require cluster synchronization on activation through consensus. A comprehensive list of feature gates scheduled for 1.14 can be found here.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "ZenLlama",
            "activity": "2023-05-02T23:39:37.973Z"
        },
        {
            "id": 201,
            "title": "Feature: RPC Call to Get Estimated Priority Fees (1.14.17)",
            "url": "https://forum.solana.com/t/feature-rpc-call-to-get-estimated-priority-fees-1-14-17/201",
            "created_at": "2023-05-06T04:41:31.659Z",
            "posts_count": 1,
            "views": 1016,
            "reply_count": 0,
            "last_posted_at": "2023-05-06T04:41:31.729Z",
            "category_id": 7,
            "category_name": "Releases",
            "description": "Version 1.14.17 introduces the getRecentPrioritizationFees RPC API, referencing the “priority” fees requested and returned via the ComputeBudget::SetComputeUnitPrice instruction. The endpoint returns a list of priority fees over the last 150 blocks that was used to successfully land at least one transaction with matching input parameters. The endpoint takes the optional argument of an array of accounts. If no accounts are provided, it matches any transaction. If an array of accounts is provided, it matches against any transactions that specified all of the provided accounts as writable. Developers may read the associated API docs in full detail here.\nThis API is provided so clients can use the data to estimate with what percentage likelihood and what priority fee is currently needed to land a transaction, or a transaction with a particular writable account set. This is useful for wallets or dApps wishing to take full advantage of the localized fee markets that the introduction of priority fees made possible on Solana.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "ZenLlama",
            "activity": "2023-05-06T04:41:31.729Z"
        },
        {
            "id": 200,
            "title": "Feature: Turbine Improvements (1.14.17)",
            "url": "https://forum.solana.com/t/feature-turbine-improvements-1-14-17/200",
            "created_at": "2023-05-06T04:30:10.392Z",
            "posts_count": 1,
            "views": 1088,
            "reply_count": 0,
            "last_posted_at": "2023-05-06T04:30:10.475Z",
            "category_id": 7,
            "category_name": "Releases",
            "description": "Turbine is the name for Solana’s block propagation methodology. Version 1.14 brings with it two significant changes to Turbine. The first of these changes is in the construction and transmission of erasure batches during turbine broadcast and the second is a modification to the Turbine Tree topology.\nErasure Batch Construction &amp; Transmission\nWhen a leader needs to transmit their block to the rest of the network, they do so by breaking their block data into maximum transmissible unit (MTU) sized chunks called “shreds.” In order to increase the reliability and censorship resistance of the network, these data shreds are used to generate paired recovery shreds through a Reed-Solomon erasure coding scheme. The set of data shreds and their paired recovery shreds together is known as an erasure batch. The scheme is designed such that the entirety of the block can be reconstructed even if some portion of the erasure batch is missing. With the erasure batch in hand, the leader sends data and recovery shreds one-by-one. For each individual shred, a stake weighted shuffle of the staked nodes is performed in order to construct a tree-like topology to transmit over. This is colloquially referred to as the Turbine Tree.\nThe old method of constructing erasure batches required a set of 32 data shreds, out of which 32 recovery shreds would be generated, creating a 32:32 erasure batch. However, for a variety of reasons, receiving nodes in the Turbine Tree may not always receive enough data shreds to generate a 32:32 erasure batch by the time they are scheduled to retransmit their data over Turbine. As a result, nodes with less than 32 data shreds would send the shreds un-encoded when called upon to retransmit, but also hold onto the shreds in memory to retransmit again as a proper 32:32 erasure batch when the missing data shreds could be acquired.\nThis results in delayed reliability guarantees, provided by Reed-Solomon encoding, to lower staked nodes in the Turbine Tree who are more likely to be placed in lower levels. This is because reliable propagation to lower levels is now bottlenecked by higher levels getting all the data shreds in time. If the higher level node does not get all 32 data shreds required to generate a proper erasure batch in time, lower level nodes now suffer because they only get sent the raw data shreds and not a Reed-Solomon encoded erasure batch. They eventually will get the encoded erasure-batch, but only after the higher level nodes can obtain the missing data. This bottleneck does not reduce the overall probability of a node getting all the block data over Turbine, but it extends the time over which it takes to get this data.\nVersion 1.14 changes the erasure batch construction in order to eliminate this delay in reliable retransmission of data shreds. This is achieved because nodes in the Turbine Tree no longer require 32 data shreds to generate an erasure batch. Instead, an equivalently reliable erasure batch is constructed from how many ever data shreds the node has at the time of retransmission. While the Reed-Solomon coding scheme used before targeted 32:32 erasure batches, the new version is able to construct variable size erasure batches with the equivalent reliability of a 32:32 batch. For example, in the worst case of only having 1 data shred, a 1:544 is constructed and transmitted over Turbine. This data shred now has the same reliability as the data shreds in a 32:32 erasure batch. This change eliminates the bottleneck described above, meaning that lower staked nodes lower in the Turbine Tree do not have delayed reliability guarantees as compared to higher staked nodes.\nRemove Redundant Turbine Path\nIn order to understand the Turbine topology changes, it’s important to first define some relevant terms. Structurally, Turbine consists of “layers”, made up of “neighborhoods.” A layer’s number refers to the shortest number of hops the leader’s messages must make in order to reach a node in that layer. Therefore nodes in Layer 1 are one network hop from the leader, nodes in Layer 2 are two network hops from the leader, so on and so forth. Nodes in each layer are grouped into sub-groups called neighborhoods. A neighborhood consists of N distinct nodes, the first of which is referred to as the “anchor.” Turbine also defines a parameter known as the “fanout.” The fanout of the network specifies how many nodes each neighborhood node forwards data to in lower layers. For example, for a given fanout M, a node in Layer X forwards data to M other nodes in Layer X+1.\nPrior to activation of this feature, the number of neighborhoods in each layer is a function of the fanout such that each neighborhood in Layer X communicates with M other neighborhoods in Layer X+1. Therefore the number of neighborhoods in each layer is M^(X-1) where X is the layer number and M is the fanout. You may notice this matches the number of nodes each node in Layer X sends data to in Layer X+1. This is by design, as nodes in Layer X don’t just send data to M other nodes in Layer X+1, but each of these nodes is in a distinct neighborhood as well. Therefore, each node in position “k” in a neighborhood, sends its data to M other nodes in position k of a neighborhood in the layer below. Additionally, the anchor node of each neighborhood forwards data to every node in its neighborhood. This additional data path from the anchor to all nodes in the neighborhood is referred to as the “redundant turbine path.” It’s referred to as redundant because nodes are already receiving this data through the normal fanout path each node takes to distribute data to lower layers. Figure 1 below shows a visual description of the Turbine topology prior to this feature activation.\nturbine-legacy1718×600 47.2 KB\nFigure 1: Turbine Tree with Redundant Turbine Path\nActivation of this feature removes the redundant turbine path, effectively eliminating the concept of anchor nodes and neighborhoods. This can be easily visualized in Figure 2 below.\nturbine-fanout-3-new1002×697 122 KB\nFigure 2: Turbine Tree without Redundant Turbine Path",
            "comments": "",
            "comment_count": 0,
            "original_poster": "ZenLlama",
            "activity": "2023-05-06T04:30:10.475Z"
        },
        {
            "id": 190,
            "title": "Feature: Accounts Index on Disk (1.14.17)",
            "url": "https://forum.solana.com/t/feature-accounts-index-on-disk-1-14-17/190",
            "created_at": "2023-05-04T23:39:55.163Z",
            "posts_count": 1,
            "views": 1805,
            "reply_count": 0,
            "last_posted_at": "2023-05-04T23:39:55.247Z",
            "category_id": 7,
            "category_name": "Releases",
            "description": "Version 1.14.17 of the Solana Labs validator client introduces a command line interface (CLI) option --enable-accounts-disk-index that enables storage of the AccountsIndex on disk rather than in RAM. This option can be enabled on validator start-up and reduces the RAM usage of a mainnet-beta validator by about 40-50GB as of May 4th 2023. It reduces RAM usage by storing the Accounts Index on disk, and using an in-memory least recently used (LRU) cache to store the most actively accessed accounts.\nThis feature is not enabled by default. It is recommended only for validators with their Ledger folder backed by 2 performant NVMe SSDs. Validators with only one SSD or older style SSDs may encounter performance degradation evident by disk IO reads/writes increasing significantly. This is particularly evident during startup/catchup such that the validators do not catch up quickly.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "ZenLlama",
            "activity": "2023-05-04T23:39:55.247Z"
        },
        {
            "id": 189,
            "title": "Feature: Increased TX Account Lock Limits (1.14.17)",
            "url": "https://forum.solana.com/t/feature-increased-tx-account-lock-limits-1-14-17/189",
            "created_at": "2023-05-04T21:58:36.201Z",
            "posts_count": 1,
            "views": 1474,
            "reply_count": 0,
            "last_posted_at": "2023-05-04T21:58:36.273Z",
            "category_id": 7,
            "category_name": "Releases",
            "description": "This feature is quite straightforward and the title indeed does it justice. Version 1.14.17 increases the transaction account lock limit from 64 to 128. When a transaction on Solana is composed, it must specify all the writable accounts it wishes to access. These writable accounts require locks to prevent race conditions where multiple parties are trying to read and write to the same accounts which can result in incorrect state being returned to the caller. As such, writable accounts are locked. While an account is locked however, no other transactions that need to write to the same account can be executed. Thus, the more accounts a transaction locks, the other less transactions writing to that account can be parallelized. With this in mind, the increased account lock limits should be used judiciously so as to not unnecessarily consume excess cluster resources.\nNote that this feature is gated as explained in the v1.14.17 Release Summary. Therefore, it is included in the release but will not go live till the feature gate is activated. You can track activation of the feature through the associated GitHub issue.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "ZenLlama",
            "activity": "2023-05-04T21:58:36.273Z"
        },
        {
            "id": 174,
            "title": "Feature: Compact Vote State (1.14.17)",
            "url": "https://forum.solana.com/t/feature-compact-vote-state-1-14-17/174",
            "created_at": "2023-05-02T23:35:36.933Z",
            "posts_count": 1,
            "views": 810,
            "reply_count": 0,
            "last_posted_at": "2023-05-02T23:35:36.998Z",
            "category_id": 7,
            "category_name": "Releases",
            "description": "Currently on mainnet-beta on-chain votes are sent incrementally. While network packets are always subject to some packet loss, resulting in missing votes, another issue arises when votes are received out of order. This can lead to a situation where the receiving validator has an inaccurate view of the on-chain vote state and will have to wait for its local lockout to explore before votes start to land.\nWhile waiting for lockout to expire is standard behavior when a validator needs to switch forks. When a validator’s local vote state diverges from the on-chain vote state, there is potential for the validator to think its lockout is much higher than was observed by the rest of the cluster. This can therefore lead to a situation where the validator spins unnecessarily waiting for the lockout to expire, or simply continues voting with invalid votes and does not have its stake properly observed by the rest of the cluster. This can then cause a chain reaction where other validators on the network are slower to switch off of minority forks because they are delayed in observing the divergent validator’s stake. Therefore, the further a validator’s local vote state diverges from the on-chain vote state, the more potential for degraded fork choice performance within the cluster.\nIn order to address this potential issue, the Solana Labs validator client leverages two conjoined changes. Firstly, each validator no longer just sends the incremental vote, but instead sends its entire vote state, known as its vote tower. This reduces forking because the local vote-state of a validator is now more tightly in sync with the on-chain vote state. The code responsible for this part of the changes has been included in the Solana Labs validator client codebase since v1.10, however the feature gate to activate it has not been activated on mainnet-beta. It has however been live on testnet since epoch 365 and has received extensive testing as a result.\nThe second conjoined change comes in v1.14 and motivates the reason why the first part was not activated on mainnet-beta while being active on testnet for so long. While sending this extra information has the effect of improving fork choice performance, it causes significantly more block space to be used for votes. Testnet trials showed this change to increase the vote instruction size by around 4 times. This caused a noticeable increase in the number of broadcast shreds, but did not cause any consensus issues. In light of this, Solana Labs engineers opted to delay activation till further optimization could be achieved. This optimization comes as part of v1.14, and addresses the increase in memory and gives the change its namesake, “Compact Vote State.”\nThese vote states are now losslessly compressed by utilizing a few tricks. The first is that the vote trees index their slots by offsets from the root, allowing the use of smaller data types for indexing their position in the vote tower. Secondly, a special serialization method is employed that allows representation of the data during transmission to be further compacted. All in all, the resulting compact vote state now consumes comparable memory overhead to the original votes. Only increasing the memory footprint of votes by about 20%, while allowing that vote to contain the full vote tower of votes rather than a single sequential member.\nWhile this change improves fork choice performance of the network, it also enables the implementation of Solana Improvement Document (SIMD) titled “Timely Vote Credits.” Without this change the proposed changes are unable to proceed due to dependency on the vote tower.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "ZenLlama",
            "activity": "2023-05-02T23:35:36.998Z"
        },
        {
            "id": 181,
            "title": "Feature: Stake Program Changes (1.14.17)",
            "url": "https://forum.solana.com/t/feature-stake-program-changes-1-14-17/181",
            "created_at": "2023-05-03T18:36:04.032Z",
            "posts_count": 1,
            "views": 899,
            "reply_count": 0,
            "last_posted_at": "2023-05-03T18:36:04.104Z",
            "category_id": 7,
            "category_name": "Releases",
            "description": "Version 1.14 introduces a few changes to the native stake program. These changes improve the performance of the network when doing various stake related activities such as delegating stake, and paying out rewards. The changes in 1.14 set the groundwork for more changes slated for future releases, one of these such changes being SIMD-15, “Partitioned Epoch Rewards Distribution.\"\nVote: Minimum Stake Delegation\nOne of the stake program changes in v1.14 enables an on-chain vote to take place that, if approved by the validators, will increase the minimum required stake delegation to 1 SOL. Existing stakes below the minimum will be grandfathered in, but will be allowed to deactivate or merge with another stake that results in a stake whose delegation is above the minimum. Once the software release goes live, validators will be provided with a chance to vote on the enablement of this feature through the Feature Proposal Program. The vote window will last 2 weeks. When the vote starts, the program distributes a stake weighted amount of voting tokens to staked validator accounts. Validators then approve the vote by sending these tokens back to the account they were sent from. If at the end of the two week voting period the number of tokens in the account is greater than 67% of the total tokens distributed, the proposal passes and the program triggers a feature flag to enable the new minimum stake delegation in the next epoch.\nGet Minimum Stake Delegation Instruction\nIn light of the potential modification of the minimum stake delegation based on the aforementioned vote, a new instruction to get the minimum stake delegation has been added to the program. This is needed as it is no longer a statically defined constant. This instruction is exposed in v1.14 via the command line interface (CLI), Web3.js, and the RPC method getMinimumStakeDelegation.\nUndelegated Stakes Allowed Below Minimum\nThe stake program will now allow undelegated stake accounts to contain less than the minimum required stake delegation. This is needed now that the minimum stake delegation will change from 1 lamport if the minimum vote delegation vote passes.\nDeactivate Delinquent Stake Instruction\nA new instruction is added to the stake program that allows permissionless deactivation of delinquent stake on the network. Currently the only way to deactivate stake delegated to an abandoned vote account is to induce a hard fork or upon network restart. This new instruction can be invoked to force deactivate stake delegated to a vote account if the vote account has not voted in the last 5 epochs; about 15 days in real time.\nAbandoned vote accounts cause stake to become unproductive as it will never be available for consensus. Unfortunately however, it continues to be counted towards the total effective stake. As a result, a proportional amount of productive stake is now needed to overcome this lingering unproductive stake; effectively increasing the total percentage of productive stake that is needed for consensus. Furthermore, and arguably more impactful, is the consequence that delinquent validators with enough stake still get leader slots. However because they are delinquent, their blocks will always be skipped, degrading performance of the entire network.\nThis new instruction allows network participants to clean up stake delegated to abandoned vote accounts. When the instruction is invoked, a second vote account is also required to be passed to prove that the chain has been active over the last 5 epochs. This is a security measure to prevent a very unlikely attack vector where a malicious actor could force the destaking of accounts if they are able to warp the chain ahead by 5 epochs.\nImprovements to Caching\nVersion 1.14 also brings some general optimizations to reward distribution. The stake program now uses information stored in the bank as a stake cache to read and write to stake accounts. The stake cache allows reading and writing stake accounts to avoid the need to go all the way into the accounts database to access the stake accounts. This allows the program to avoid costly operations that access the hardware’s disk rather than their RAM. This optimization is showing a 30% reduction in reward distribution time.\nThe stake cache is updated at the same time a transaction updates the accounts database, so it remains synchronized with the accounts database at all times, even upon software update. Nevertheless, to avoid any potential issues with consensus due to a mismatch in the stakes calculation, the use of the stakes cache is feature gated.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "ZenLlama",
            "activity": "2023-05-03T18:36:04.104Z"
        }
    ],
    "Research": [
        {
            "id": 315,
            "title": "About the Research category",
            "url": "https://forum.solana.com/t/about-the-research-category/315",
            "created_at": "2023-06-19T14:47:08.194Z",
            "posts_count": 1,
            "views": 461,
            "reply_count": 0,
            "last_posted_at": null,
            "category_id": 9,
            "category_name": "Research",
            "description": "Research on potential future core protocol changes",
            "comments": "",
            "comment_count": 0,
            "original_poster": "jacobcreech",
            "activity": null
        },
        {
            "id": 762,
            "title": "Deprecate X.509 certs for P2P connections",
            "url": "https://forum.solana.com/t/deprecate-x-509-certs-for-p2p-connections/762",
            "created_at": "2023-12-01T14:55:46.400Z",
            "posts_count": 3,
            "views": 1691,
            "reply_count": 1,
            "last_posted_at": "2024-06-21T15:24:27.906Z",
            "category_id": 9,
            "category_name": "Research",
            "description": "In QUIC-TLS in Firedancer, I shared some specifics of the TLS setup in TPU/QUIC. I also proposed a number of protocol changes that sheds some unnecessary complexity.\nIn this post, I wanted to share a technique to almost entirely remove X.509 logic from uses of QUIC in Solana, without changing protocol logic. TL;DR It allows validator implementations to\nEliminate thousands of lines of third party code\nReduce validator identity key exposure and signing operations\nI’m looking forward to your feedback!\nBackground\nThe Solana Labs implementation of the Solana peer-to-peer protocols currently produce unnecessary signatures and import thousands of lines of unnecessary dependency code. In the interest of safety, it is worth critically reviewing any line of code exposed to untrusted users, as well as any line of code exposed to sensitive data (such as node private keys).\nMeet QUIC, the transport layer used in some P2P connections. QUIC comes with a great deal of complexity and feature creep.\nSomewhere deep within the connection establishment logic is the target of this post: X.509 certificates. X.509 is an incredibly complex protocol that is mostly useless to the peer-to-peer layer. The only reason it is used is historical: QUIC uses the TLS 1.3 handshake for authentication, and rustls (the TLS library used in Solana Labs) does not support any replacement for it.\nMutual authentication\nWe are actually only interested in one property of the certificate: It holds the peer’s supposed public key.\nWhen making a connection, peers exchange their public keys and then use a challenge-response mechanism each other to prove that they are in possession of the corresponding private keys. (TLS 1.3 CertificateVerify; RFC 8446, Section 4.4.3)\nTLS connections on the web would typically also use this X.509 certificate to associate an external identity, like a domain name (e.g. forum.solana.com), as well as a signature chain vouching for the certificate’s validity.\nSolana validators, however, are inherently identified by their identity public key. There is no need to associate this key with external information. Consequently, there is no need for these X.509 certificates any signature chain nor any other pieces of data other than the public key itself.\nNotably, validators also have the ability to treat peers as “anonymous” and ignore their identity. This works because the message content is often authenticated by itself, regardless who is the sender. (Such as a gossip message)\nParsing is useless\nA surprising amount of code is required to encode and decode X.509 certificates. Complex parsers are particularly susceptible to security issues, such invalid memory accesses, infinite loops, and unbounded heap allocations. Even memory safe languages are not enough to prevent these sort of bugs.\nBut the only winning move for this game is to not play. If we don’t do complex parsing, we significantly reduce attack surface for these vulnerability classes.\nA commendable effort by ANSSI-FR to write a verified parser weighs in at about 10000 lines of code: GitHub - ANSSI-FR/x509-parser: a RTE-free X.509 parser Unfortunately, I could not get the Frama-C verification tooling to run without errors myself though. No full X.509 parsing verification effort exists for Rust, but some projects have partial coverage through fuzzing.\nTo understand how to ditch parsing, let’s look at a hex dump of a minimal certificate that TLS libraries can decode. The ff ff ff ff strings are the public key (SubjectPublicKeyInfo) and the signature placeholders respectively.\n0000: 30 81 f1 30 81 a4 a0 03 02 01 02 02 08 01 01 01 0..0............\n0010: 01 01 01 01 01 30 05 06 03 2b 65 70 30 11 31 0f .....0...+ep0.1.\n0020: 30 0d 06 03 55 04 03 0c 06 53 6f 6c 61 6e 61 30 0...U....Solana0\n0030: 20 17 0d 37 30 30 31 30 31 30 30 30 30 30 30 5a ..700101000000Z\n0040: 18 0f 34 30 39 36 30 31 30 31 30 30 30 30 30 30 ..40960101000000\n0050: 5a 30 00 30 2a 30 05 06 03 2b 65 70 03 21 00 ff Z0.0*0...+ep.!..\n0060: ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ................\n0070: ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff a3 ................\n0080: 29 30 27 30 17 06 03 55 1d 11 01 01 ff 04 0d 30 )0'0...U.......0\n0090: 0b 82 09 6c 6f 63 61 6c 68 6f 73 74 30 0c 06 03 ...localhost0...\n00a0: 55 1d 13 01 01 ff 04 02 30 00 30 05 06 03 2b 65 U.......0.0...+e\n00b0: 70 03 41 00 ff ff ff ff ff ff ff ff ff ff ff ff p.A.............\n00c0: ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ................\n00d0: ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ................\n00e0: ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ................\n00f0: ff ff ff ff ....\nIt is sufficient to replace the bytes of the public key and signature placeholders with the validator’s actual values.\nSimilarly, to parse a certificate we could apply this template as a “mask” check that all bytes except for the placeholder match, and then trivially extract the public key. If the mask does not match, we simply consider the peer fully anonymous, or reject the connection.\nThis mechanism is also backwards compatible: Validators using a real parser will still be able to decode this template.\nIt is admittedly hacky, but also simple to reason about. Considering X.509 should have never been used in the Solana protocol, I find it worth deleting a large amount of pointless code.\nSelf-signing is useless\nWhile tinkering with this, I realized that the signature part of the X.509 certificate is also entirely pointless in the context of peer-to-peer connections. Peer authenticity is proven using a separate signature mechanism in the TLS 1.3 layer. The X.509 signature allows trusted third parties to sign (and thereby certify) someone’s certificate. But as mentioned eariler, we don’t need anyone to certify validator identity keys.\nBut don’t take my word for it. Validators do not verify the X.509 signature field, and you can put whatever you want in it.\nThe signature field is required nonetheless, so Solana Labs validators used the “self-signed certificate” pattern, by signing their certificate with their own validator identity key. This is more problematic than it sounds: It creates an instance of key reuse, where the same key is used to sign messages of different types.\nUsing the CBMC verification system, I was able to prove that this instance of X.509 signing is not ambiguous with regards to any other types. (See here and here). But yet again, it is preferable to not have this risk in the first place.\nAnother concern is the exposure of the private key itself. Although unlikely, a supply chain attack in third-party dependency code could compromise the private key.\nSo, let’s just simply put a bunch of one bits in the X.509 signature field.\nConclusion\nIn the Firedancer validator, we replaced ~10000 of lines of code with about a dozen.\nFD_IMPORT_BINARY( template, \"cert_template.der\" );\nvoid\ngenerate_cert( uchar cert_out[ static 0xf4 ],\n uchar const pubkey [ static 0x20 ] ) {\n memcpy( cert_out, template, 0xf4 );\n memcpy( cert_out+0x5f, pubkey, 0x20 );\n}\nuchar *\nextract_pubkey( uchar pubkey_out[ static 0x20 ],\n uchar const * cert,\n ulong cert_sz ) {\n uchar check[ 0xf4 ];\n if( cert_sz!=0xf4 ) return NULL;\n memcpy( pubkey_out, cert+0x5f, 0x20 );\n memcpy( check, cert, 0xf4 );\n memset( check+0x5f, 0xff, 0x20 );\n return 0==memcmp( check, template, 0xf4 ) ? pubkey_out : NULL;\n} \nA related patch is available for Solana Labs: don't sign X.509 certs by ripatel-fd · Pull Request #34202 · solana-labs/solana · GitHub\nA standard solution to address this problem exists and is also implemented in Firedancer: RFC 7250 - Using Raw Public Keys in Transport Layer Security (TLS) and Datagram Transport Layer Security (DTLS)",
            "comments": "[ripatel-jump]: Solana Labs v1.18, which is the majority of testnet, now runs with X.509 dummy certs\n\n[ripatel-jump]: 96% of mainnet validators adopted Solana Labs (now Agave) v1.18.\n10000 lines of Firedancer code perished.\nIt was a vendored library to be fair, but it’s still nice to see significant protocol simplifications.\n \n github.com/firedancer-io/firedancer\n \n \n \n \n \n \n \n \n x509: remove parser\n \n \n \n committed 03:25AM - 21 Jun 24 UTC\n \n \n \n \n riptl\n \n \n \n \n +60\n -10889\n\n",
            "comment_count": 2,
            "original_poster": "ripatel-jump",
            "activity": "2024-06-21T15:24:27.906Z"
        },
        {
            "id": 326,
            "title": "QUIC-TLS in Firedancer (fd_tls)",
            "url": "https://forum.solana.com/t/quic-tls-in-firedancer-fd-tls/326",
            "created_at": "2023-06-22T12:04:41.020Z",
            "posts_count": 4,
            "views": 1859,
            "reply_count": 1,
            "last_posted_at": "2023-08-26T22:33:35.976Z",
            "category_id": 9,
            "category_name": "Research",
            "description": "Opportunities to simplify TLS over peer-to-peer connections\nI wanted to share some progress on fd_tls and kick off general discussion about the use of TLS in the Solana protocol.\nDisclaimer: fd_tls is not an officially supported component of Firedancer.\nBackground\nSince the adoption of the QUIC protocol, Solana’s peer-to-peer layer depends on the TLS protocol for securing connections. Currently, the Solana Labs client uses the rustls library, and Firedancer uses quictls, a fork of OpenSSL.\nI started fd_tls as an experiment to replace third-party network dependencies in Firedancer, with the intention of making fd_quic entirely self-hosted. It aims to implement the minimum amount of components required to secure peer-to-peer connectivity, while staying compliant with TLS 1.3 (RFC 8446) and QUIC-TLS (RFC 9001).\nTLS is commonly seen as a complex standard due to its lengthy history of bugs and changes, all while maintaining backward-compatibility. Since the deployment of TLS in Solana has no such backwards-compatibility requirements, there is opportunity to shed some complexity and make the handshake logic of the QUIC protocol more robust against various types of attacks.\nThe development philosophy for Firedancer thus far has been to own the entire Solana validator stack from OSI Layer 2 upwards. This is a lot of work, but has the advantage of reducing the amount of unknowns. (Such as: “How would our QUIC library behave in a specific edge case?”). It also reveals opportunities for deep optimization. However, all of this new networking code presents additional attack surface and will have to get audited.\nConsidering the above, we strongly suggest minimizing code complexity and the amount of cryptographic algorithms in the Solana validator network.\nProtocol\nhttps://quic.xargs.org/ is a great resource explaining every step of the QUIC-TLS handshake. I will try to summarize it in my own words.\nQUIC-TLS in Solana is a combination of three separate protocols:\nThe TLS handshake layer (as the name implies, only active during the handshake)\nX.509 certificates (mostly unused)\nThe QUIC record layer, which specifies how QUIC packets get encrypted (comparable to the TLS or DTLS record layers for TLS connections over TCP or UDP)\nIn TLS version 1.3, the latest version at the time of writing, creating a connection involves the following high-level steps:\nNegotiate a suite of cryptographic algorithms\nEstablish a “handshake-level” symmetric encryption key using X25519, an Elliptic Curve Diffie-Hellman key exchange algorithm\nExchange and verify X.509 peer certificates containing Ed25519 signatures\nEstablish an “application-level” symmetric encryption key\nTLS versions\nAn obvious first step is to drop support for legacy TLS versions. TLS 1.3 is more secure and much simpler than older TLS versions. It finds almost ubiquitous support and is currently the default in Solana peer-to-peer connections.\n \n github.com/solana-labs/solana\n \n \n \n \n \n \n \n \n disable rustls tls12 feature\n \n \n solana-labs:master ← ripatel-fd:ripatel/no-tls12\n \n \n \n opened 12:43AM - 25 Nov 22 UTC\n \n \n \n \n ripatel-fd\n \n \n \n \n +2\n -2\n \n \n \n \n \n #### Problem\nCurrently, Solana advertises support for legacy TLS 1.2 cipher s…uites (`0xc0??`).\nTLS 1.3 is widely supported at this point -- for security and simplicity, support for TLS 1.2 should be disabled.\n```\nCipher Suites (10 suites)\n Cipher Suite: TLS_AES_256_GCM_SHA384 (0x1302)\n Cipher Suite: TLS_AES_128_GCM_SHA256 (0x1301)\n Cipher Suite: TLS_CHACHA20_POLY1305_SHA256 (0x1303)\n Cipher Suite: TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 (0xc02c)\n Cipher Suite: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 (0xc02b)\n Cipher Suite: TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 (0xcca9)\n Cipher Suite: TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 (0xc030)\n Cipher Suite: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (0xc02f)\n Cipher Suite: TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 (0xcca8)\n Cipher Suite: TLS_EMPTY_RENEGOTIATION_INFO_SCSV (0x00ff)\n```\n#### Summary of Changes\nDisables the default `tls12` feature on the `rustls` dependency. \nSee https://github.com/solana-foundation/specs/pull/21\nFixes #\n \n \n \n \n \n \n \nCryptographic Algorithms\nTLS 1.3 incorporates a flexible mechanism for negotiating cryptographic algorithms. In early steps of the handshake, the client advertises a list of algorithms it supports. The server then picks a combination of them.\nThe main types of algorithms being negotiated are as follows:\nKey Exchange cryptography. Solana Labs validators support X25519, secp256r1, and secp384r1.\nCipher Suites: Solana Labs validators support the TLS 1.3 recommended Authenticated Encryption suites: AES-128-GCM-SHA256, AES-384-GCM-SHA256, and ChaCha20-Poly1305-SHA256. (Note: This implies HMAC-SHA256, not “pure” SHA)\nSignature Algorithms: Solana Labs validators support 9 signature hash algorithms, including EdDSA (Ed25519), 2x ECDSA-based schemes, and 6x RSA-based schemes.\nSome of the above cryptography is already in use in the Solana protocol.\nSHA-256 (almost everywhere in the Solana protocol)\nEd25519 (transaction signatures)\nby extension, Curve25519 used in X25519\nThe ChaCha20 block function (on-chain randomness)\nOther algorithms were newly introduced by adopting QUIC. Notably, RSA-based signature schemes are considerably slower than the elliptic curve alternatives.\nLuckily, to establish a TLS connection, only one cryptographic algorithm of each type is required. Therefore the first version of fd_tls will only support X25519 KEX, AES-128-GCM-SHA256 AEAD, and Ed25519 signatures. (Potentially also ChaCha20-Poly1305-SHA256)\nX.509\nAnother obvious opportunity for reducing complexity is eliminating the use of X.509 certificates. X.509 secures peer identity through a chain of trust, anchored in a set of root CAs. This model does not fit permissionless networks well, in which peers are inherently identified by their public keys, as opposed to a domain name (like the server of the forum.solana.com site you are currently reading).\nConsequently, the use of X.509 certificates in Solana is awkward: Nodes serve auto-generated certificates that are signed by themselves, and their peers verify this useless signature.\nFor each connection the validator makes, it then generates an additional “CertificateVerify” proof. It involves using the certificate’s key to sign a hash that is tied to the current connection. This proves that it is in possession of the key advertised by the certificate.\nFrom the perspective of the verifier, this means the following steps are involved when accepting a new QUIC connection:\nParse the X.509 certificate (DER serialization over various complex ASN.1 data structures)\nVerify the certificate chain (signature verification)\nExtract the Ed25519 public key of the peer\nVerify the “CertificateVerify” proof\nRaw Public Keys\nRFC 7250 introduces a second certificate type: Raw Public Keys (RPKs)\nRPKs consist of a minimal ASN.1/DER prefix followed by a copy of the serialized public key.\nThe new verifier steps then become:\nNegotiate RPK support via the CertificateType extension\nParse the RPK ASN.1 prefix\nVerify the “CertificateVerify” proof\nNot only is this mechanism much simpler; It also decreases the maximum byte count of a TLS handshake.\nUnfortunately, support for RPKs is sparse. It is currently not supported by stable releases of OpenSSL, GnuTLS, quictls, rustls, nor the Go standard library. OpenSSL and GnuTLS both provide experimental support. My attempt to use an Ed25519 RPK with GnuTLS failed for unknown reasons.\nIf time permits, I would like to contribute RFC 7250 support to the Go standard library and rustls. I would greatly appreciate any help with this task.\nfd_tls\nFinally, an update on fd_tls:\nfd_tls is currently able to correctly derive TLS 1.3 decryption keys up to the handshake level when speaking to OpenSSL. So far, my experience with implementing TLS 1.3 has been quite pleasant. There are no obvious blockers to completing self-hosted QUIC-TLS support; it is simply a matter of time. I am currently working on additional TLS extension types and the Certificate/CertificateVerify message types.\nWhether we’ll use fd_tls in production is unclear. Certainly, before attempting to do so, fd_tls needs to pass tlsfuzzer torture and various other conformance tests.\nI hope this post was informative and I’m looking forward to continue discussion on Solana’s network protocols.",
            "comments": "[leo]: ripatel-jump:\nAn obvious first step is to drop support for legacy TLS versions. TLS 1.3 is more secure and much simpler than older TLS versions. It finds almost ubiquitous support and is currently the default in Solana peer-to-peer connections.\ndisable rustls tls12 feature by ripatel-fd · Pull Request #28956 · solana-labs/solana · GitHub\n Not only is this much easier, it is also considered best practice. Same with restricting cipher suites to the small set of recommended modern suites.\n ripatel-jump:\nUnfortunately, support for RPKs is sparse. It is currently not supported by stable releases of OpenSSL, GnuTLS, quictls, rustls, nor the Go standard library. OpenSSL and GnuTLS both provide experimental support. My attempt to use an Ed25519 RPK with GnuTLS failed for unknown reasons.\nIf time permits, I would like to contribute RFC 7250 support to the Go standard library and rustls. I would greatly appreciate any help with this task.\nAgreed that removing ASN.1 and X509 parsing attack surface is highly desirable.\nBut, the big question would be interoperability - as you say, there’s barely any support in third party clients, and it would have to be plumbed all the way up from the TLS library to each QUIC library people might want to use.\nI’m not sure how feasible that is and nobody really knows what third party clients are currently in use (mostly traders and similar users).\nThe QUIC TPU is a public interface used by clients other than solana-validator and Firedancer, and there would have to be some period of time where both kinds of credentials would be accepted.\n\n[ripatel-jump]: Agree with all of the above.\nBut, the big question would be interoperability - as you say, there’s barely any support in third party clients, and it would have to be plumbed all the way up from the TLS library to each QUIC library people might want to use.\nOnce OpenSSL 3.2 and GnuTLS 3.8.0 get released, hopefully a critical mass is reached to drive further adoption of RFC 7250. I consider this an opportunity for the Solana community to introduce RPK support to other TLS libraries.\nMy random guess would be that widespread support for RPK takes at least 6 months.\nThe QUIC protocol itself is cleanly separated from peer authentication and other QUIC libraries I’ve looked at (quinn, ngtcp2) don’t require plumbing for this feature. Those take an arbitrary TLS config object. Once rustls supports RPKs, adding support to quinn is as easy as bumping the rustls dependency version number.\nThe logic to present an X.509 certificate is luckily minimal (a literal memcpy). It is more work to verify a cert, but fd_tls could still call down to the OpenSSL API. So it’s hardly a blocker.\nI’m not sure how feasible that is and nobody really knows what third party clients are currently in use (mostly traders and similar users).\nThe improvement document process will give maintainers of custom peer-to-peer clients an opportunity to share their concerns.\nThe QUIC TPU is a public interface used by clients other than solana-validator and Firedancer, and there would have to be some period of time where both kinds of credentials would be accepted.\nThis is the right way and is straightforward to implement: Negotiate between both certificate types using the CertificateType extension. If the other peer does not recognize the extension, it will fall back to X.509.\n\n[ripatel-jump]: image2794×1095 396 KB\nServer-side key schedule\n\n",
            "comment_count": 3,
            "original_poster": "ripatel-jump",
            "activity": "2023-08-26T22:33:35.976Z"
        },
        {
            "id": 927,
            "title": "Protocol TODOs 2024",
            "url": "https://forum.solana.com/t/protocol-todos-2024/927",
            "created_at": "2024-01-09T09:31:03.996Z",
            "posts_count": 1,
            "views": 1667,
            "reply_count": 0,
            "last_posted_at": "2024-01-09T09:31:04.056Z",
            "category_id": 9,
            "category_name": "Research",
            "description": "Here are three protocol changes that I consider important to improve reliability and performance of the Solana network and the Firedancer client. In other words: My Solana-land New Year’s resolution for 2024.\n(Software optimization is a game of whack-a-mole)\nExecutable Format Simplification\nPreparing an on-chain program for execution should be as simple as mapping bytecode and data into virtual machine memory. In reality, loading a typical user program often takes longer than actually executing it.\nThis is mostly owed to historical choices to allow the protocol to execute the outputs of clang / rustc directly. This was thought to feature better developer experience than requiring some post-processing step.\nWhat resulted was an error-prone and inefficient format.\nThe situation is best explained by Firedancer’s implementation:\nhttps://github.com/firedancer-io/firedancer/blob/main/src/ballet/sbpf/fd_sbpf_loader.c\nA 1200 line code footprint in this context is not only slow, but also complicates cross-client compatibility testing.\nDesigning a replacement binary format is a trivial exercise. It is mostly a social issue, as it will require approval from all contributors to the virtual machine and compiler toolchain.\nOnce the new format is implemented, the old format should be retired.\nMy preferred solution thus far is a one-time sweep over the account database that converts all existing deployed programs. Any VM bytecode would be kept strictly identical, such that immutability of program code is preserved.\nLimiting State Growth\nThe state size of the Solana network is gradually approaching a point where it is no longer practical to keep in DRAM. In fact, it is technically unbounded due to an overly simplistic storage cost model.\nOne obvious solution is to spilling over to accounts to cheaper storage such as NVMe. But unless done very carefully, would kill any prospect of greatly increasing network performance.\nSolana is DRAM-heavy for reasons that this Intel technical paper neatly summarizes: https://www.intel.com/content/www/us/en/developer/articles/technical/memory-performance-in-a-nutshell.html\nKeeping account state in-memory allows on-chain programs to access arbitrary account data with consistently low latency. More importantly, current generation x86 systems can achieve total memory bandwidth of hundreds of gigabytes per second.\nWhen operating in a storage environment with highly asymmetric latency and bandwidth, a blockchain runtime will certainly become limited by I/O capabilities unless fee models become aware of account locality.\nFor example, writing ten 1MB accounts 1 million times each is going to be significantly faster than writing (10^6)x10MB accounts once, even though both cases write 10TB of data. The former would operate at a bandwidth of ~hundreds GB/s via L3 cache, the latter at least 100x slower)\nWith the goal of staying in DRAM territory in mind, let’s instead take measures to reduce state growth rate.\nAn effective first step to improve cost modeling is to dynamically increase storage fees as free space decreases. Currently, storage fees are implemented via a “minimum account balance” that can be fully reclaimed.\nI don’t expect that it is possible to achieve negative state growth with this dynamic alone, though. At least, not until exceedingly high fees make the network unusable.\nWhich leads us to the next research item.\nGeneric Account Compression\nDid you know that 76% of Solana accounts have not been accessed in the last 6 months? (Credit to @andrewhong5297 for this data point)\nContinuing from the above, there is significant opportunity to reduce the in-memory set of accounts.\nHash trees have become the de-facto standard approach to provide large amounts of state to on-chain programs (commonly called “compression” within Solana). Compressed program data consists of just the root of a concurrently modifiable hash tree. The original data itself will no longer get replicated across the blockchain network and is stored separately by whoever chooses to (such as in a p2p torrent network). But it can be recovered as needed. Currently, each program would have to include logic for state compression separately.\n@toly has suggested to introduce compression generically by introducing a new storage class of compressed accounts. To reclaim DRAM, the runtime would periodically sweep the database for the oldest accounts and evict them if their balance drops below a dynamic minimum (determined using the aforementioned state growth limiting mechanic).\nTo avoid complexities with storage asymmetry, the transactions that attempt to access compressed accounts would fail. Users can decompress their accounts at any time be re-uploading all data fragments, along with a cryptographic proof that the data was not tampered with.\nThe main difficulties lie in client/wallet “RPC” infrastructure and choosing an appropriate storage solution. Failure to do so risks data loss. Storage networks like Filecoin and Arweave are of particular interest.\nClosing Thoughts\nPreliminary design work for all of the above is under way at the Solana Labs and Firedancer. I hope to publish detailed designs and technical proposals for all three items in the coming weeks. As always, I’d love to invite the wider community for discussion. I’m curious what you think.",
            "comments": "",
            "comment_count": 0,
            "original_poster": "ripatel-jump",
            "activity": "2024-01-09T09:31:04.056Z"
        },
        {
            "id": 829,
            "title": "BLAKE3 slower than SHA-256 for small inputs",
            "url": "https://forum.solana.com/t/blake3-slower-than-sha-256-for-small-inputs/829",
            "created_at": "2023-12-17T12:57:17.773Z",
            "posts_count": 3,
            "views": 1601,
            "reply_count": 0,
            "last_posted_at": "2023-12-28T01:14:58.569Z",
            "category_id": 9,
            "category_name": "Research",
            "description": "Summary\nAs of 2023-Dec, when comparing the fastest widely available BLAKE3 and SHA-256 implementations, BLAKE3 features worse throughput on x86_64 for message sizes up to ~4kB.\nThis is due to the fact that libraries for multi-message (batch) hashing exist for SHA-256, but not for BLAKE3. Multi-message hashing is possible on BLAKE3, but requires non-trivial scheduling of operations due to BLAKE3’s tree hash construction.\nTheory\nSHA-256 and BLAKE3 are hash functions used in the Solana protocol.\nBLAKE3 is used for account hashes, SHA-256 for virtually everything else.\nHash functions are commonly compared in single-message benchmarks, which are single-core tests in which one message is hashed at a time. The BLAKE3 paper claims to feature better throughput than SHA-256 for all input sizes in this setting. I could reproduce this result when comparing the BLAKE3 C reference implementation against SHA-256.\nSolana, however, uses both hash functions in a way that allows for a high degree of message-level SIMD and ILP parallelism, i.e. hashing multiple messages simultaneously on a single CPU core. In a multi-message benchmark, the hash implementation is given an asymptotically infinite number of messages that can be processed in parallel. We then measure the peak throughput of input bytes per second processed.\nMulti-message hashing is the fastest approach for both hash functions for small message sizes (up to ~4kB).\nPractice\nUsually, the fastest approach is to store each hash state in one or more SIMD registers. The multi-block function executes SIMD instructions that applies each step of the computation to all in-flight hash states.\nAscending to the next layer, the multi-message engine then schedules inputs to the multi-block function. This scheduler can get more complicated than it seems, especially with variable-length messages. It involves initialization of new hash states, appending pieces of message inputs, finalizing output values, and so on.\nThe relatively simple Merkle–Damgård construction powering SHA-256 makes scheduling manageable. The block function is invoked exactly once per 64 bytes of input data, plus optionally one additional time during finalization.\nIndeed, there exist open-source implementations of multi-message SHA-256: minio/sha256-simd in Go, and fd_sha256 in C. Both feature AVX2 and AVX512 backends, in the realm of ~10 Gbps (Zen 2, AVX2) and ~20 Gbps (Icelake Server, AVX512) per-core peak throughput respectively on recent x86_64 CPUs. Peak throughput is reached for message sizes (64*(n-1))+55.\nThe BLAKE3 hash function is implemented C, Rust, and Go. All three are highly optimized toward large message hashing. But currently, none support parallel hashing of small independent messages.\nThe following compares fd_sha256 vs BLAKE3-C throughput for multi-message hashing in AVX512 mode. The input size is (64*n)-9 for SHA-256 and 64*n for BLAKE3 (to account for SHA’s padding).\nn\nSHA-256 (Gbps)\nBLAKE3 (Gbps)\nDelta\n1\n7.6\n3.8\n-50%\n2\n8.9\n5.1\n-43%\n4\n9.7\n6.1\n-37%\n8\n10.1\n6.8\n-33%\n16\n10.3\n7.2\n-30%\n32\n10.5\n7.0\n-33%\n64\n10.5\n12.3\n+17%\n128\n10.5\n22.5\n+114%\nPlans\nBLAKE3 is a core part of the Solana runtime and this is unlikely to change soon.\nI will attempt to introduce multi-message hashing for BLAKE3 and will post findings in this thread. The scheduling required for multi-message mode is much more complicated in BLAKE3 due to its hash tree construction, so I don’t expect a linear speedup.",
            "comments": "[ripatel-jump]: Python PoC of multi-message parallel scheduling: blake3-parallel-scheduler/batch.py at main · ripatel-fd/blake3-parallel-scheduler · GitHub\n\n[ripatel-jump]: The work was worth it, it seems. First results show that fd_blake3 is 3.4x faster for the 1024 byte case.\nimage1842×1052 41 KB\n\n",
            "comment_count": 2,
            "original_poster": "ripatel-jump",
            "activity": "2023-12-28T01:14:58.569Z"
        }
    ],
    "Announcements": [
        {
            "id": 7,
            "title": "Welcome to the Solana Developer Forums",
            "url": "https://forum.solana.com/t/welcome-to-the-solana-developer-forums/7",
            "created_at": "2023-02-09T20:53:25.345Z",
            "posts_count": 4,
            "views": 1264,
            "reply_count": 0,
            "last_posted_at": "2023-02-24T17:39:57.075Z",
            "category_id": 4,
            "category_name": "Announcements",
            "description": "Welcome to the Solana Developer Forums\nThe Solana Developer forums is a place for the Solana developer community to discuss core protocol proposals(SIMDs), application standards(SRFC), and more. In other words, this forum is a place for developers that want to improve the Solana developer ecosystem.\nWhat to do on this forum?\nRead and discuss Solana\nForm a team\nAttend discussions\nShare prototypes, code, and ask for feedback",
            "comments": "[jacobcreech]: \n\n[jacobcreech]: \n\n[jacobcreech]: \n\n",
            "comment_count": 3,
            "original_poster": "system",
            "activity": "2023-02-24T17:39:57.075Z"
        },
        {
            "id": 3,
            "title": "About the Announcements category",
            "url": "https://forum.solana.com/t/about-the-announcements-category/3",
            "created_at": "2023-02-09T20:53:22.797Z",
            "posts_count": 1,
            "views": 432,
            "reply_count": 0,
            "last_posted_at": null,
            "category_id": 4,
            "category_name": "Announcements",
            "description": "Announcements from the Solana Foundation related to any of the following:\nSolana Foundation Delegation Program\nSolana Improvement Documents\nSolana Foundation Server Program",
            "comments": "",
            "comment_count": 0,
            "original_poster": "system",
            "activity": null
        },
        {
            "id": 772,
            "title": "Upcoming SFDP Changes",
            "url": "https://forum.solana.com/t/upcoming-sfdp-changes/772",
            "created_at": "2023-12-05T16:40:05.035Z",
            "posts_count": 15,
            "views": 7267,
            "reply_count": 6,
            "last_posted_at": "2024-07-31T10:28:44.119Z",
            "category_id": 4,
            "category_name": "Announcements",
            "description": "Goal: The Solana Foundation wants to provide additional support for validators that participate in the SFDP and help them become self-reliant and sustainable, with the broader goal of maximizing decentralization, network resiliency, and performance of the network.\nSituation: The current SFDP structure does not provide strong enough incentives for validators to sustainably grow stake from outside the program. Additionally, lax performance requirements allows poorly-performing validators to continue to receive stake.\nProposed solution: Improve Foundation staking to support validators at a higher level early on and less so over time as they grow, while maintaining a high performance requirement.\nWhat the Solana Foundation will do:\nCover voting costs for validators in the first year with a tapering amount over time. For the first three months 100% of vote costs will be covered, for the next three months 75%, for the third three months 50% and for the final three months 25%, with vote coverage ending after 12 months\nWhy? Start-up vote costs can be prohibitive to new, small validators. Time bounding this support encourages validators to achieve sustainable stake levels before support ends.\nExisting SFDP Participants: 1 year of tapering vote cost coverage starting when the new SFDP goes live. (see FAQ for timing)\nNew SFDP Participants: 1 year of tapering vote cost coverage starting from their mainnet onboarding.\nPerformance Requirements: Validators must meet the to be defined baseline requirements (see FAQ) for the epoch to be eligible for vote cost coverage.\nMatch outside stake 1:1, up to a cap of 100,000 SOL from the Foundation.\nWhy? The Foundation wants to assist in amplifying community stake decisions, encourage validators to attract outside stake, and engage with the broader Solana community.\nWhat is outside stake? Any stake that is not from the Foundation.\nExamples:\nIf an SFDP participant has 10,000 SOL stake outside of the Foundation, then they would get 10,000 SOL from the Foundation, for a total of 20,000 SOL plus the base delegation. (see base delegation below)\nIf an SFDP participant has 250,000 SOL stake outside of the Foundation, then they would get 100,000 from the matching portion of SFDP, for a total of 350,000 SOL plus the base delegation from the foundation (see base delegation below)\nPerformance Requirements: Baseline requirements + acceptable skip rate performance. The Solana Foundation will change how the skip rate is computed and will use averages over a longer period of time to reduce variance when a validator has a low count of leader slots in a given epoch.\nDelegate a base amount of the remaining SOL after the matching portion, divided evenly among program participants.\nWhy? Validators in the program need a minimum delegation to get on the leader schedule to be able to produce blocks\nHow much will each validator get as a base delegation? Initially the amount split up between all participants will result in roughly 40,000 SOL per participant. This amount will decrease over time as more stake is matched by other participants and more Foundation stake is deposited into stake pools.\nPerformance Requirements: Similar to existing baseline requirements outlined here with an increased requirements of vote credits.\nIncrease performance targets closer towards the cluster averages.\nWhy? High-quality, highly-reliable validators are paramount to a healthy network and the delegation program should only incentivize operators who can meet a high bar of performance\nWho will be impacted?: A vast majority of SFDP participants meet this high bar already, but there are a handful of operators who will have to change their operational strategy to achieve these higher performance standards.\nDeposit stake into stake pools.\nWhy? This helps support the liquid staking ecosystem and further assist in amplifying community network orientation while allowing additional entities to decide how stake should be distributed across the network.\nFAQ:\nDo I need to participate in the SFDP to run a validator on Solana? No, the Solana validator set is permissionless and anyone can start a validator at anytime with no minimum delegation.\nWhen will these changes go live? The target go-live time frame is late January / early February 2024.\nWhat specifically will the performance requirements be? The Solana Foundation will update solana.org with the new performance requirements in the weeks leading up to the program changes going live and will notify community members through the Discord and email.\nWill I get a delegation even if I have very little outside stake? Yes, participants that have just started out or have not had initial success attracting stake will still get the base delegation.\nWill testnet requirements stay the same? Yes, participants will still be required to run a well-performing node on testnet to be eligible for mainnet delegations.\nWill onboarding change? Onboarding will continue as usual both before and after the roll out of the new SFDP changes.",
            "comments": "[diman]: Hello Ben,\nThank you very much for this forward movement; it has been long awaited (as well as TVC). I only have a question about this particular phrase:\nand more Foundation stake is deposited into stake pools\nSo, my question is, will there be clear and transparent criteria announced regarding which pools will be eligible for this, the criteria for making it onto this list, exceptions (especially), frequency of reviewing the size of the stake from the Foundation, and so on?\nI am concerned about the following. I understand that pools are more flexible. However, at the same time, they might have closed delegation policies, and it seems from the foundation’s perspective, this would not be acceptable, as this distribution could entirely go to self-controlled validators (== some groups of validators that exist now might become pools, and then essentially nothing changes).\nAnother point is that there might be real parties interested in opening such pools as part of their business, and it seems that in such a case, it would be an excellent entry point for them.\nSo, it would be nice if such conditions are clear and transparent, and if the community could discuss them and help you in advance to model potential problems, similar to the issues with the delegation program, from which you are now trying to distance yourself (thank you very much for your hard work in doing so).\nUPD.\nPerhaps, to ensure that the current changes are effective immediately, it might be a good idea to simply remove the ‘and’, carefully reconsider it, and then add it back later.\nIt looks like a legal stuff - to leave a clause in the foundation’s program that hypothetically allows for opaque management of funds. Or, alternatively, make amendments in the future through a secondary document.\nMy only wish is to move forward without embedding old problems into new implementations.\nWith for Solana\nDim An\n\n[max.kaplan]: Hi Ben,\nThanks for posting the above changes. I think they are definitely net positive. I wanted to ask a few questions though:\nIs anything going to be done to make firedancer a valid client? Right now, I think many are probably afraid to run it, out of fear of losing stake, because of version differences between firedancer and the labs client. It would be good to get some clarity.\nThe voting cost changes you posted above I think assume that no one starts running a mainnet validator until they are onboarded through the delegation program, but I’ve seen people where this is not the case. For example, say Bob runs a testnet validator and signs up for delegation program. He could be earning TdS rewards, but before he is actually “onboarded” onto mainnet, he could still run a mainnet validator, just not receiving any stake from foundation. This could actually be viewed as a good thing because Bob is proactively looking for outside stake before getting some from the foundation. In this case, if he does that, can we get some clarity on how the voting costs would be subsidized? Would it be the first few months of voting costs AFTER he is onboarded to mainnet? Would it be the voting costs he incurred when he actually spun up the mainnet validator before he was onboarded to mainnet via the delegation program? etc.\nIf the foundation is going to be supplying stake pools with funds, I think it would be really great if the foundation can incentivize transparency around delegation of stake pools in terms of which validators they delegate to. For example, Marinade does a great job of this with their dashboard. Some stake pools have dashboards, but the formula / delegation strategy isn’t transparent. Some stake pools have no dashboards at all. The easiest way for new validators to get a good chunk of stake is to get delegation from a stake pool. If the foundation is going to be staking more with stake pools, transparency from stake pools is going to be even more important. Imagine how frustrating it would be for a new validator trying to get stake from a stake pool, but having no idea why they aren’t receiving any yet because it’s a black box. I think this would align incentives well.\nAnyway, thanks for posting this and this is definitely net positive IMO!\nMax\n\n[max.kaplan]: Also, If the foundation is going to try to deposit stake into stake pools, I hope there’s something that can be done to ensure that stake pools won’t just default to delegating mostly to validators running with 0% commission. That’s also the most common way to “build stake” for a validator. But if validators are on their own to go and get stake (which I do agree is a good thing), what will likely happen is validators will run at 0% commission while the stake pool makes 2%. Then the program isn’t really supporting validators anymore, it’s supporting stake pools. I don’t really have an answer unfortunately for how to solve this, but it’s something to think about. Ideally, it’s incentivizing stake pools to not just stake with the highest APY validators, rather to stake with well-run validators that are doing good for the network which aligns more with the original post here.\n\n[Ben.Hawkins]: Hello Dim An,\nThank you for your thoughtful feedback.\nRegarding your query about the foundation’s stake being deposited into stake pools, I want to assure you that we are deeply committed to the overall health of the Solana network. While we will be using our own discretion, our approach to selecting stake pools will be guided by a set of criteria designed to foster decentralization, network resilience, and the engagement of a diverse set of validators.\nHere are some key considerations we will use in our decision-making process:\nDecentralization: We prioritize stake pools that contribute to a more distributed and decentralized network. This includes evaluating the geographical distribution and the diversity of the validators in the pool.\nStaking Criteria Transparency: We expect stake pools to have clear, articulated criteria for staking. This includes their policies for selecting and onboarding validators, which should be openly accessible to the community.\nNumber of Validators: The inclusion of a broad range of validators is crucial. We will look favorably upon pools that support a larger number of validators, as this aligns with our goal of broadening participation across the network.\nWillingness to Add New Validators: Pools that demonstrate a commitment to continually adding new, capable validators will be considered more favorably. This is part of our effort to encourage growth and fresh talent within the Solana ecosystem.\nPerformance Standards: Consistent with the SFDP’s ethos, stake pools must adhere to high-performance standards. This includes maintaining a validator set with a good track record in terms of uptime, participation, and other key performance metrics.\nYour concerns about the potential for closed delegation policies and self-controlled validators are valid and have been taken into consideration. We will ensure that our process for selecting stake pools discourages these practices and promotes a more open and equitable ecosystem.\nFinally, we welcome ongoing community dialogue and input. Your suggestion to model potential problems and discuss them openly is well-received\n.\nWe are committed to revising and adapting our approach as necessary, keeping the community’s best interests at the forefront. Thank you once again for your dedication to Solana and for helping us refine our processes. Your support and constructive criticism are invaluable as we strive to improve and evolve the SFDP.\nWith gratitude,\nBen Hawkins\n\n[Ben.Hawkins]: Hello Max,\nThank you for your insightful questions and the positive feedback on the recent updates to the Solana Foundation Delegation Program (SFDP). I’m happy to provide some clarity on your queries.\nFiredancer Client Integration: We are actively working towards integrating Firedancer as a robust client in the Solana ecosystem. Once Firedancer is live on mainnet, running either the Firedancer or the Solana Labs client will not only be encouraged but also fully supported within the SFDP framework. We are making necessary adjustments to accommodate a multi-client landscape, ensuring a smooth and efficient operation for validators regardless of their chosen client.\nVoting Cost Subsidy for Early Mainnet Validators: We acknowledge and appreciate the proactive efforts of validators like the example you mentioned. It’s indeed beneficial for validators to begin attracting stake on mainnet as early as they can. Concerning the voting costs, the subsidy will commence from the time a validator is officially onboarded onto mainnet through the delegation program. This means that any voting costs incurred prior to formal onboarding will be the responsibility of the validator. Our aim here is to encourage validators to be self-reliant and proactive, while also providing support once they are part of the mainnet delegation program.\nStake Pool Transparency and Delegation: Your concerns regarding the transparency of stake pools are well-taken. As I mentioned in my response to Dim An, we are committed to ensuring that stake pools receiving funds from the Foundation operate with a high degree of transparency and fairness. This includes clear disclosure of their delegation strategies and decision-making processes. We believe that such transparency is crucial for the trust and effectiveness of the ecosystem, especially for new validators seeking stake from these pools. We will continue to work towards aligning incentives and ensuring clarity in how these stake pools operate.\nWe deeply value the community’s input and are dedicated to continuous improvement. The goal is to create a robust, fair, and thriving Solana network, and your feedback plays a vital role in this journey. Thank you again for your engagement and support.\nBest regards,\nBen Hawkins\n\n[max.kaplan]: All makes sense to me and sounds great. Thanks!\n\n[cfl0ws]: Thank you for this thoughtful approach, Ben. The suggested changes appear to me to be incremental and beneficial, taking initial steps in the right direction. I’ve personally found iterative approaches like this to be very effective.\nThe goal of helping validators become “self-reliant and sustainable” resonated strongly with me. The proposed changes align well with this goal.\nPerformance measurement is a key aspect of the plan and I’ll hold additional comments related to it until more details are published. Generally, I’d suggest that any performance measurements should be independent of stake amount. By this I mean a lower-staked validator should be able to achieve the same performance benchmark as a higher-staked validator, all other factors being equal.\nRegarding the matching program, is the match based on direct stake and does it exclude stake received from stake pools like Marinade?\nAnd to confirm an example -\n Ben.Hawkins:\nIf an SFDP participant has 10,000 SOL stake outside of the Foundation, then they would get 10,000 SOL from the Foundation, for a total of 20,000 SOL plus the base delegation. (see base delegation below)\nIn this example the total delegation received would be 20k (match) + 40k (base) = 60k (total), correct?\nSome additional comments -\nMaximum commission at 100% feels high to me and inconsistent with the goal of a validator becoming sustainable. I imagine it’s unlikely for a validator to attract additional stake at 100% commission (unless via backroom deals) and I think allowing a validator to keep all the rewards from a foundation delegation is setting a potentially harmful precedent (or at least doesn’t discourage greed).\nTotal stake of 3.5 million or less also feels high to me. I’d be surprised if validators staked at 1 million or more would even really notice the benefit of a max 140k delegation.\nAt first I was thinking that the cap should be the lowest staked amount of validators above the halt line, i.e. within the Nakamoto Coefficient. Then I checked and found that number is currently 3.8 million.\nNow I’m thinking that a cap of around 1.5 million makes sense. In this case the maximum foundation delegation would represent at the least 10% of a validator’s total stake.\nFinally a note on categorizing validators as “small”. I’ve been thinking of it more as “low(er)-staked” over the past few months, as some may interpret the term “small” as a negative indicator of capability, commitment and dedication.\nThanks again for your work here and I look forward to following its evolution!\n\n[Ben.Hawkins]: Hello Chris ,\nI’m glad to hear that the approach resonates with you, and I appreciate your thoughts on performance measurements, the matching program, and the broader aspects of the initiative.\nMatching Program and Stake Calculation: Yes, your understanding of the matching program is correct. For example, if an SFDP participant has 10,000 SOL in stake outside of the Foundation, they would receive an additional 10,000 SOL match from the Foundation, resulting in a total of 20,000 SOL. Plus, with the base delegation, the total would be ~60,000 SOL. Keep in mind that as the more stake is matched through out the participants the base delegation will decrease.\nCommission and Sustainability: Regarding the maximum commission, there seems to be a misunderstanding. We are keeping the maximum commission unchanged at 10% for now. Market forces are expected to naturally encourage lower commissions. The notion of a 100% commission does not align with our policies or the spirit of the SFDP.\nCap on Total Stake: The current maximum stake threshold is indeed 3.5 million SOL, but we are actively considering a cap of around 1 million SOL. This is still under discussion, and we intend to provide more details closer to the program’s launch. We are carefully evaluating what would be most effective, especially at the boundaries of this threshold.\nStakepool Stake as Outside Stake: For the time being, stake received from stake pools like Marinade will be considered as outside stake. However, this is an aspect of the program that could evolve over time. We are committed to an incremental and adaptive approach, ensuring that the program remains effective and relevant.\nThank you once again for your valuable input. It’s through such constructive dialogues that we can refine and enhance the SFDP. We look forward to your continued participation and insights as the program evolves.\nWith appreciation,\nBen Hawkins\n\n[Brian]: This is a huge upgrade. I’m fully supportive of these changes and expect them to have significant and immediate benefits to the health of the cluster. Thank you for all of your work on this project. Looking forward to the rollout of these changes ASAP!\nBrian Smith, Jito contributor\n\n[cfl0ws]: Thank you for the thorough and thoughtful feedback, here are a few additional comments…\n Ben.Hawkins:\nMatching Program and Stake Calculation: Yes, your understanding of the matching program is correct. For example, if an SFDP participant has 10,000 SOL in stake outside of the Foundation, they would receive an additional 10,000 SOL match from the Foundation, resulting in a total of 20,000 SOL. Plus, with the base delegation, the total would be ~60,000 SOL. Keep in mind that as the more stake is matched through out the participants the base delegation will decrease.\nPlease say more about how the base delegation decreases as the matched stake increases. I don’t think I caught that the first time around.\n Ben.Hawkins:\nCommission and Sustainability: Regarding the maximum commission, there seems to be a misunderstanding. We are keeping the maximum commission unchanged at 10% for now. Market forces are expected to naturally encourage lower commissions. The notion of a 100% commission does not align with our policies or the spirit of the SFDP.\nIt looks like I misread the 100% from here, which is intended for testnet.\n Ben.Hawkins:\nCap on Total Stake: The current maximum stake threshold is indeed 3.5 million SOL, but we are actively considering a cap of around 1 million SOL. This is still under discussion, and we intend to provide more details closer to the program’s launch. We are carefully evaluating what would be most effective, especially at the boundaries of this threshold.\nGlad to hear it!\n Ben.Hawkins:\nStakepool Stake as Outside Stake: For the time being, stake received from stake pools like Marinade will be considered as outside stake. However, this is an aspect of the program that could evolve over time. We are committed to an incremental and adaptive approach, ensuring that the program remains effective and relevant.\nSeems beneficial to me, as the stakepools typically delegate to validators consistent with the values of the SDFP.\nFinally, I like the idea of supporting multi-client infrastructure. For example, we’re currently running and Labs and Firedancer validator on testnet, effectively doubling our cost. It would be fantastic if the SFDP might additionally support validators who choose to run multiple validators using multiple clients on testnet, to further encourage client diversity.\n\n[Bryanzk]: Hi @Ben.Hawkins,\nThank you for all the previous detailed explanations. As someone eager to join the SFDP, I have a few questions that need your help! I appreciate it in advance!\nRe: voting cost covering and matching program. How exactly are they going to be implemented? When and through which channel will they be done?\nRe: The based delegation. How will it be implemented as well?\nRe: The based delegation and the stake being deposited to the pools. I don’t understand the relationship between these 2. Does it mean that the base delegation will be sent to the pools first, then we, the SFDP newcomers, will get the base delegation from the pools?\nA beginner question: Let’s say I only have one metal server. The proper way to join SFDP is to use this server to pass the testnet performance benchmark first, then switch the same server to the mainnet. And everything starts from there. Am I right?\nIf I’m running Jito’s validator node, am I qualified for this program?\nLooking forward to your answers. And wish all the best for the SFDP!!\n\n[7LayerMagik]: Might be nice if the stakeweight aspect isnt a hard cap but rather a curve with an upper bound. Like, if you hit 1 million sol stake you then just lose 100k sol + stake?\nAnother thought – personally I’d like to see the Sol going to stakepools only used for algorithmic delegation vs adding to the sub-pools controlled by voting. The Marinade directed stake pools are weirdly weighted these days and imo they’re just being value extracted by parasitic behavior (i.e. people with lots of vote weight and commissions that wouldn’t get them much stake otherwise).\n\n[jorzhik]: This is a very good idea and will allow you to develop, many are just afraid to start\n\n[Gabynto]: The Solana Foundation’s updated SFDP will better support validators by covering their voting costs for the first year and matching outside stake to boost their growth. With stricter performance requirements and a focus on decentralization, these changes aim to strengthen the network and help validators become self-sustaining.\n\n",
            "comment_count": 14,
            "original_poster": "Ben.Hawkins",
            "activity": "2024-07-31T10:28:44.119Z"
        }
    ]
}