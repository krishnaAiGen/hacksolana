id,title,url,description,comments,original_poster,views,reply_count,comment_count,posts_count,created_at,activity,category_name,category_id,last_posted_at
14,About the Releases category,https://forum.solana.com/t/about-the-releases-category/14,News on upcoming Solana releases and their changelogs,,jacobcreech,436,0,0,1,2023-02-23T01:46:31.428Z,,Releases,7,
175,Version 1.14.17 Release Summary,https://forum.solana.com/t/version-1-14-17-release-summary/175,"With the recent trepidation around v1.14 of the Solana Labs validator client, the following document aims to outline the major changes landing in v1.14.17, release timelines, and testing. This release version includes significant changes and improvements to the Solana Labs validator client, many of which have been eagerly awaited for some time.
The tentative upgrade schedule is as follows:
• 2023-05-08 - Ask for volunteers to upgrade 10% of mainnet-beta stake to v1.14.17.
• 2023-05-15 - Ask for volunteers to upgrade 25% of mainnet-beta stake to v1.14.17.
• 2023-05-22 - Recommend v1.14.17 for all mainnet-beta validators and RPC nodes.
The Solana Labs client version 1.14.17 has undergone a rigorous release engineering testing regime in preparation for the upcoming release. Testnet has been used to simulate the exact upgrade processes that will be done on mainnet-beta during rollout of the release.
First, testnet was downgraded and feature gates were reverted to match current mainnet-beta on v1.13.7. Following this, testnet was gradually upgraded to v1.14.17 over a period of 27 days. This was done gradually to identify any interoperability issues between the two versions. Then testnet was live downgraded from v1.14.17 to v1.13.7 to demonstrate that the cluster can downgrade versions while remaining online. From here, testnet was again upgraded to v1.14.17. This time it was done quickly, mostly as a way to get back to v1.14.17 for further testing, but it also provided engineers another period of testing mixed Solana Labs client versions live on testnet. As previously mentioned, on the first downgrade to v1.13.7 feature gates in v1.14.17 were reverted. The final part of the release engineering testing process was to re-enabled the feature gates to verify the full functionality of v1.14.17 on testnet. This also brought testnet back to the same state as before the February 25th incident.
Throughout this process stress testing on testnet was ongoing. This includes standard benchmarking and DDOS tests, as well as the new test cases outlined in detail in the outage report below. No outstanding issues were identified. In addition to the testnet testing, Solana Labs has been running test nodes against mainnet-beta with an assortment of 1.14 versions near the tip of the release. This has been ongoing since the February 25th incident and the test nodes have yet to encounter any problems.
If you would like to learn more about v1.14.17 you can start reading the pull requests which are linked in the notes on the release page.
If you’d like to better understand what caused the last outage check out the outage report.
tldr: v1.14 was not the culprit; v1.13.6 was also susceptible (both v1.13 and v1.14 have since been patched
A comprehensive list of changes in v1.14 can be found by reading the pull requests linked above, however an index of some of the more significant changes is listed below:
Compact Vote State*
Stake Program Changes*:
Minimum Stake Delegation Vote
Get Minimum Stake Delegation Instruction
Undelegated Stakes Allowed Below Minimum
Deactivate Delinquent Stake Instruction
Improvements to Caching
RPC Call to Get Estimated Priority Fees
Accounts Index on Disk
Turbine Improvements*
Erasure Batch Construction &amp; Transmission
Remove Redundant Turbine Path
Increased TX Account Lock Limits*
*These features will not go live immediately upon 1.14.17 release but at a later date. These features are feature gated and require cluster synchronization on activation through consensus. A comprehensive list of feature gates scheduled for 1.14 can be found here.",,ZenLlama,4089,5,0,1,2023-05-02T23:39:37.910Z,2023-05-02T23:39:37.973Z,Releases,7,2023-05-02T23:39:37.973Z
201,Feature: RPC Call to Get Estimated Priority Fees (1.14.17),https://forum.solana.com/t/feature-rpc-call-to-get-estimated-priority-fees-1-14-17/201,"Version 1.14.17 introduces the getRecentPrioritizationFees RPC API, referencing the “priority” fees requested and returned via the ComputeBudget::SetComputeUnitPrice instruction. The endpoint returns a list of priority fees over the last 150 blocks that was used to successfully land at least one transaction with matching input parameters. The endpoint takes the optional argument of an array of accounts. If no accounts are provided, it matches any transaction. If an array of accounts is provided, it matches against any transactions that specified all of the provided accounts as writable. Developers may read the associated API docs in full detail here.
This API is provided so clients can use the data to estimate with what percentage likelihood and what priority fee is currently needed to land a transaction, or a transaction with a particular writable account set. This is useful for wallets or dApps wishing to take full advantage of the localized fee markets that the introduction of priority fees made possible on Solana.",,ZenLlama,1016,0,0,1,2023-05-06T04:41:31.659Z,2023-05-06T04:41:31.729Z,Releases,7,2023-05-06T04:41:31.729Z
200,Feature: Turbine Improvements (1.14.17),https://forum.solana.com/t/feature-turbine-improvements-1-14-17/200,"Turbine is the name for Solana’s block propagation methodology. Version 1.14 brings with it two significant changes to Turbine. The first of these changes is in the construction and transmission of erasure batches during turbine broadcast and the second is a modification to the Turbine Tree topology.
Erasure Batch Construction &amp; Transmission
When a leader needs to transmit their block to the rest of the network, they do so by breaking their block data into maximum transmissible unit (MTU) sized chunks called “shreds.” In order to increase the reliability and censorship resistance of the network, these data shreds are used to generate paired recovery shreds through a Reed-Solomon erasure coding scheme. The set of data shreds and their paired recovery shreds together is known as an erasure batch. The scheme is designed such that the entirety of the block can be reconstructed even if some portion of the erasure batch is missing. With the erasure batch in hand, the leader sends data and recovery shreds one-by-one. For each individual shred, a stake weighted shuffle of the staked nodes is performed in order to construct a tree-like topology to transmit over. This is colloquially referred to as the Turbine Tree.
The old method of constructing erasure batches required a set of 32 data shreds, out of which 32 recovery shreds would be generated, creating a 32:32 erasure batch. However, for a variety of reasons, receiving nodes in the Turbine Tree may not always receive enough data shreds to generate a 32:32 erasure batch by the time they are scheduled to retransmit their data over Turbine. As a result, nodes with less than 32 data shreds would send the shreds un-encoded when called upon to retransmit, but also hold onto the shreds in memory to retransmit again as a proper 32:32 erasure batch when the missing data shreds could be acquired.
This results in delayed reliability guarantees, provided by Reed-Solomon encoding, to lower staked nodes in the Turbine Tree who are more likely to be placed in lower levels. This is because reliable propagation to lower levels is now bottlenecked by higher levels getting all the data shreds in time. If the higher level node does not get all 32 data shreds required to generate a proper erasure batch in time, lower level nodes now suffer because they only get sent the raw data shreds and not a Reed-Solomon encoded erasure batch. They eventually will get the encoded erasure-batch, but only after the higher level nodes can obtain the missing data. This bottleneck does not reduce the overall probability of a node getting all the block data over Turbine, but it extends the time over which it takes to get this data.
Version 1.14 changes the erasure batch construction in order to eliminate this delay in reliable retransmission of data shreds. This is achieved because nodes in the Turbine Tree no longer require 32 data shreds to generate an erasure batch. Instead, an equivalently reliable erasure batch is constructed from how many ever data shreds the node has at the time of retransmission. While the Reed-Solomon coding scheme used before targeted 32:32 erasure batches, the new version is able to construct variable size erasure batches with the equivalent reliability of a 32:32 batch. For example, in the worst case of only having 1 data shred, a 1:544 is constructed and transmitted over Turbine. This data shred now has the same reliability as the data shreds in a 32:32 erasure batch. This change eliminates the bottleneck described above, meaning that lower staked nodes lower in the Turbine Tree do not have delayed reliability guarantees as compared to higher staked nodes.
Remove Redundant Turbine Path
In order to understand the Turbine topology changes, it’s important to first define some relevant terms. Structurally, Turbine consists of “layers”, made up of “neighborhoods.” A layer’s number refers to the shortest number of hops the leader’s messages must make in order to reach a node in that layer. Therefore nodes in Layer 1 are one network hop from the leader, nodes in Layer 2 are two network hops from the leader, so on and so forth. Nodes in each layer are grouped into sub-groups called neighborhoods. A neighborhood consists of N distinct nodes, the first of which is referred to as the “anchor.” Turbine also defines a parameter known as the “fanout.” The fanout of the network specifies how many nodes each neighborhood node forwards data to in lower layers. For example, for a given fanout M, a node in Layer X forwards data to M other nodes in Layer X+1.
Prior to activation of this feature, the number of neighborhoods in each layer is a function of the fanout such that each neighborhood in Layer X communicates with M other neighborhoods in Layer X+1. Therefore the number of neighborhoods in each layer is M^(X-1) where X is the layer number and M is the fanout. You may notice this matches the number of nodes each node in Layer X sends data to in Layer X+1. This is by design, as nodes in Layer X don’t just send data to M other nodes in Layer X+1, but each of these nodes is in a distinct neighborhood as well. Therefore, each node in position “k” in a neighborhood, sends its data to M other nodes in position k of a neighborhood in the layer below. Additionally, the anchor node of each neighborhood forwards data to every node in its neighborhood. This additional data path from the anchor to all nodes in the neighborhood is referred to as the “redundant turbine path.” It’s referred to as redundant because nodes are already receiving this data through the normal fanout path each node takes to distribute data to lower layers. Figure 1 below shows a visual description of the Turbine topology prior to this feature activation.
turbine-legacy1718×600 47.2 KB
Figure 1: Turbine Tree with Redundant Turbine Path
Activation of this feature removes the redundant turbine path, effectively eliminating the concept of anchor nodes and neighborhoods. This can be easily visualized in Figure 2 below.
turbine-fanout-3-new1002×697 122 KB
Figure 2: Turbine Tree without Redundant Turbine Path",,ZenLlama,1088,0,0,1,2023-05-06T04:30:10.392Z,2023-05-06T04:30:10.475Z,Releases,7,2023-05-06T04:30:10.475Z
190,Feature: Accounts Index on Disk (1.14.17),https://forum.solana.com/t/feature-accounts-index-on-disk-1-14-17/190,"Version 1.14.17 of the Solana Labs validator client introduces a command line interface (CLI) option --enable-accounts-disk-index that enables storage of the AccountsIndex on disk rather than in RAM. This option can be enabled on validator start-up and reduces the RAM usage of a mainnet-beta validator by about 40-50GB as of May 4th 2023. It reduces RAM usage by storing the Accounts Index on disk, and using an in-memory least recently used (LRU) cache to store the most actively accessed accounts.
This feature is not enabled by default. It is recommended only for validators with their Ledger folder backed by 2 performant NVMe SSDs. Validators with only one SSD or older style SSDs may encounter performance degradation evident by disk IO reads/writes increasing significantly. This is particularly evident during startup/catchup such that the validators do not catch up quickly.",,ZenLlama,1805,0,0,1,2023-05-04T23:39:55.163Z,2023-05-04T23:39:55.247Z,Releases,7,2023-05-04T23:39:55.247Z
189,Feature: Increased TX Account Lock Limits (1.14.17),https://forum.solana.com/t/feature-increased-tx-account-lock-limits-1-14-17/189,"This feature is quite straightforward and the title indeed does it justice. Version 1.14.17 increases the transaction account lock limit from 64 to 128. When a transaction on Solana is composed, it must specify all the writable accounts it wishes to access. These writable accounts require locks to prevent race conditions where multiple parties are trying to read and write to the same accounts which can result in incorrect state being returned to the caller. As such, writable accounts are locked. While an account is locked however, no other transactions that need to write to the same account can be executed. Thus, the more accounts a transaction locks, the other less transactions writing to that account can be parallelized. With this in mind, the increased account lock limits should be used judiciously so as to not unnecessarily consume excess cluster resources.
Note that this feature is gated as explained in the v1.14.17 Release Summary. Therefore, it is included in the release but will not go live till the feature gate is activated. You can track activation of the feature through the associated GitHub issue.",,ZenLlama,1474,0,0,1,2023-05-04T21:58:36.201Z,2023-05-04T21:58:36.273Z,Releases,7,2023-05-04T21:58:36.273Z
174,Feature: Compact Vote State (1.14.17),https://forum.solana.com/t/feature-compact-vote-state-1-14-17/174,"Currently on mainnet-beta on-chain votes are sent incrementally. While network packets are always subject to some packet loss, resulting in missing votes, another issue arises when votes are received out of order. This can lead to a situation where the receiving validator has an inaccurate view of the on-chain vote state and will have to wait for its local lockout to explore before votes start to land.
While waiting for lockout to expire is standard behavior when a validator needs to switch forks. When a validator’s local vote state diverges from the on-chain vote state, there is potential for the validator to think its lockout is much higher than was observed by the rest of the cluster. This can therefore lead to a situation where the validator spins unnecessarily waiting for the lockout to expire, or simply continues voting with invalid votes and does not have its stake properly observed by the rest of the cluster. This can then cause a chain reaction where other validators on the network are slower to switch off of minority forks because they are delayed in observing the divergent validator’s stake. Therefore, the further a validator’s local vote state diverges from the on-chain vote state, the more potential for degraded fork choice performance within the cluster.
In order to address this potential issue, the Solana Labs validator client leverages two conjoined changes. Firstly, each validator no longer just sends the incremental vote, but instead sends its entire vote state, known as its vote tower. This reduces forking because the local vote-state of a validator is now more tightly in sync with the on-chain vote state. The code responsible for this part of the changes has been included in the Solana Labs validator client codebase since v1.10, however the feature gate to activate it has not been activated on mainnet-beta. It has however been live on testnet since epoch 365 and has received extensive testing as a result.
The second conjoined change comes in v1.14 and motivates the reason why the first part was not activated on mainnet-beta while being active on testnet for so long. While sending this extra information has the effect of improving fork choice performance, it causes significantly more block space to be used for votes. Testnet trials showed this change to increase the vote instruction size by around 4 times. This caused a noticeable increase in the number of broadcast shreds, but did not cause any consensus issues. In light of this, Solana Labs engineers opted to delay activation till further optimization could be achieved. This optimization comes as part of v1.14, and addresses the increase in memory and gives the change its namesake, “Compact Vote State.”
These vote states are now losslessly compressed by utilizing a few tricks. The first is that the vote trees index their slots by offsets from the root, allowing the use of smaller data types for indexing their position in the vote tower. Secondly, a special serialization method is employed that allows representation of the data during transmission to be further compacted. All in all, the resulting compact vote state now consumes comparable memory overhead to the original votes. Only increasing the memory footprint of votes by about 20%, while allowing that vote to contain the full vote tower of votes rather than a single sequential member.
While this change improves fork choice performance of the network, it also enables the implementation of Solana Improvement Document (SIMD) titled “Timely Vote Credits.” Without this change the proposed changes are unable to proceed due to dependency on the vote tower.",,ZenLlama,810,0,0,1,2023-05-02T23:35:36.933Z,2023-05-02T23:35:36.998Z,Releases,7,2023-05-02T23:35:36.998Z
181,Feature: Stake Program Changes (1.14.17),https://forum.solana.com/t/feature-stake-program-changes-1-14-17/181,"Version 1.14 introduces a few changes to the native stake program. These changes improve the performance of the network when doing various stake related activities such as delegating stake, and paying out rewards. The changes in 1.14 set the groundwork for more changes slated for future releases, one of these such changes being SIMD-15, “Partitioned Epoch Rewards Distribution.""
Vote: Minimum Stake Delegation
One of the stake program changes in v1.14 enables an on-chain vote to take place that, if approved by the validators, will increase the minimum required stake delegation to 1 SOL. Existing stakes below the minimum will be grandfathered in, but will be allowed to deactivate or merge with another stake that results in a stake whose delegation is above the minimum. Once the software release goes live, validators will be provided with a chance to vote on the enablement of this feature through the Feature Proposal Program. The vote window will last 2 weeks. When the vote starts, the program distributes a stake weighted amount of voting tokens to staked validator accounts. Validators then approve the vote by sending these tokens back to the account they were sent from. If at the end of the two week voting period the number of tokens in the account is greater than 67% of the total tokens distributed, the proposal passes and the program triggers a feature flag to enable the new minimum stake delegation in the next epoch.
Get Minimum Stake Delegation Instruction
In light of the potential modification of the minimum stake delegation based on the aforementioned vote, a new instruction to get the minimum stake delegation has been added to the program. This is needed as it is no longer a statically defined constant. This instruction is exposed in v1.14 via the command line interface (CLI), Web3.js, and the RPC method getMinimumStakeDelegation.
Undelegated Stakes Allowed Below Minimum
The stake program will now allow undelegated stake accounts to contain less than the minimum required stake delegation. This is needed now that the minimum stake delegation will change from 1 lamport if the minimum vote delegation vote passes.
Deactivate Delinquent Stake Instruction
A new instruction is added to the stake program that allows permissionless deactivation of delinquent stake on the network. Currently the only way to deactivate stake delegated to an abandoned vote account is to induce a hard fork or upon network restart. This new instruction can be invoked to force deactivate stake delegated to a vote account if the vote account has not voted in the last 5 epochs; about 15 days in real time.
Abandoned vote accounts cause stake to become unproductive as it will never be available for consensus. Unfortunately however, it continues to be counted towards the total effective stake. As a result, a proportional amount of productive stake is now needed to overcome this lingering unproductive stake; effectively increasing the total percentage of productive stake that is needed for consensus. Furthermore, and arguably more impactful, is the consequence that delinquent validators with enough stake still get leader slots. However because they are delinquent, their blocks will always be skipped, degrading performance of the entire network.
This new instruction allows network participants to clean up stake delegated to abandoned vote accounts. When the instruction is invoked, a second vote account is also required to be passed to prove that the chain has been active over the last 5 epochs. This is a security measure to prevent a very unlikely attack vector where a malicious actor could force the destaking of accounts if they are able to warp the chain ahead by 5 epochs.
Improvements to Caching
Version 1.14 also brings some general optimizations to reward distribution. The stake program now uses information stored in the bank as a stake cache to read and write to stake accounts. The stake cache allows reading and writing stake accounts to avoid the need to go all the way into the accounts database to access the stake accounts. This allows the program to avoid costly operations that access the hardware’s disk rather than their RAM. This optimization is showing a 30% reduction in reward distribution time.
The stake cache is updated at the same time a transaction updates the accounts database, so it remains synchronized with the accounts database at all times, even upon software update. Nevertheless, to avoid any potential issues with consensus due to a mismatch in the stakes calculation, the use of the stakes cache is feature gated.",,ZenLlama,899,0,0,1,2023-05-03T18:36:04.032Z,2023-05-03T18:36:04.104Z,Releases,7,2023-05-03T18:36:04.104Z
