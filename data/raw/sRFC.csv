id,title,url,description,comments,original_poster,views,reply_count,comment_count,posts_count,created_at,activity,category_name,category_id,last_posted_at
16,Solana Request for Comments Info READ FIRST,https://forum.solana.com/t/solana-request-for-comments-info-read-first/16,"What is a SRFC?
Solana Request for Comments is a proposal for an application standard on Solana. The proposal should document the rationale for the standard and provide enough documentation to understand the implementation
An application standard can be something like the following:
Wallet standard api requirements
Metadata retrieval flow
Identity standard
A SRFC does not apply to SIMDs or core protocol proposals, but it can propose one if the standard requires a SIMD.
How do I submit a SRFC?
To get requests for comments on your new application standard on Solana, submit a topic under this category. You can include links out to further documentation, implementations, or PRs.
How will I know if my SRFC gets accepted?
In the current state there is no “acceptance” of SRFCs. They’re requests for comments on potential application standards and it is up to the author to help drive consensus on the standard.","[jacobcreech]: 

",jacobcreech,1019,0,1,2,2023-02-24T05:12:00.382Z,2023-02-24T05:12:41.804Z,sRFC,6,2023-02-24T05:12:41.804Z
3155,sRFC-35 - Address/Domain Association Specification,https://forum.solana.com/t/srfc-35-address-domain-association-specification/3155,"sRFC-35 - Address/Domain Association Specification
Summary
Solana addresses can be publicly linked to domain names using DNS TXT records or a well-known file. This allows for applications and services (like wallets, exchanges, and infrastructure providers) to easily validate if a company is associated (or denounce associations) with a token, program, or other Solana address that may be using their trademarks.
Background
As more companies and brands interact with permissionless blockchains (like Solana), having simple mechanisms for them to publicly associate (or denounce association) with onchain assets will enable verifiable trust for these groups and the broader Solana ecosystem.
Utilizing DNS TXT records or a well-known file on a website, domain administrators can associate blockchain addresses for tokens, programs (smart contracts), or other Solana addresses. This allows other companies and products to validate the original company’s connection to said Solana address.
Example use cases:
A brand issues a Solana token with their website listed in the token’s metadata. They add a DNS record to store the token’s Mint address on their DNS records. Popular token list providers, like Jupiter and CoinGecko, can fetch, parse, and validate the token in question was created by the brand. Enabling their verification processes to be more efficient while also enabling the token issuer to get easier access to liquidity at launch.
A company deploys a Solana program (smart contract) onchain. The company adds the DNS record associating the program address to their website. The program’s security.txt information includes the company’s contact information and website for security researchers to responsibly report vulnerabilities.
Note: This practice of domain verification through DNS records is already a common practice in the broader tech industry. For example, connecting your website to the Google Cloud Console or verifying domain ownership for various AWS products.
Specification
To associate a Solana address with a domain name, the domain administrator should create a DNS TXT record on their registrar or a solana.txt file on their website’s .well-known directory to store the “association records” using any of the applicable well-known tags listed below.
Association through DNS TXT records
(most recommended)
Domain administrators can add one or many DNS TXT records, each storing a single “association record”.
Note: DNS TXT records have a few limitations that are not expected to be an issue for this specification, but worth noting in the event they are. If either of these are a concern, see solana.txt below for another option:
TXT records are limited to 255 characters, per the DNS spec. A typical “association record” per this spec should normally be a max of &lt;80 characters.
Some domain registrars and platforms, like Google, recommend no more than 49 TXT records due to this being the max supported by domains themselves.
Association through the solana.txt file
Due to company policy or technical limitations, it is possible that some administrators are unable to create additional DNS TXT records for their domain. In such cases, address association can be accomplished by creating a solana.txt file and storing it in the .well-known directory (see RFC-8615).
For example:
Domain: example.com
Location of the association file: example.com/.well-known/solana.txt
The solana.txt file should only store one “association record” per line to allow efficient parsing by record validators.
Note: It is highly recommended that administrators allow frontend applications to access their solana.txt by setting correct CORS headers. This will help ensure anyone that would like to can validate the address/domain association.
Association records
A single entry in the DNS TXT or line in the solana.txt file is called an “association record”.
Each entry should store the UTF-8 string of a space separated list of tag-value pairs (using the well-known tags supported below).
Each association record should contain a single “address tag” and a single Solana address as a base58 encoded string, followed by any additional qualifier tag-value pairs (deny, allow, network, etc).
If a domain or website wants to deny all association with all Solana addresses (tokens, programs, etc), they can create add a single “deny all” association record of solana-address=denyall. This special record takes precedence over all other association records.
Well-known tags
Each association record should contain a single “address tag” that best describes the purpose of the Solana address
solana-mint-address - associates a token’s Mint address to the domain.
solana-program-address - associates a program address to the domain.
solana-address - associates any other Solana address with a domain (such as a verified signer, program deployer, or other authority).
In addition to the blockchain specific tags above, the following generic tags (qualifiers) are also supported to add amplifying information about the associated relationship:
deny (optional) - Explicitly deny the association of the address with the domain.
allow (optional) - Explicitly allow the association of the address with the domain.
network (optional) - Define the Solana or SVM network the address is used on. This value can be either a Solana network moniker (mainnet, devnet, testnet) or the genesis hash of a SVM network, as a base58 string. If not set, defaults to Solana mainnet.
Qualifier notes:
The value of a qualifier should be the case-insensitive string or numeric equivalent for a boolean value (i.e. allow=true and allow=1 set “allow” to the boolean true)
Records should not contain both allow and deny
When no allow or deny is set, defaults to allow=true signifying the domain is in fact associated with the address.
So an association record would resemble the following examples:
# token on mainnet
solana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v
# token on devnet
solana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v allow=1 network=devnet
# generic address
solana-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v deny=1 network=devnet
solana-mint-address
This association record’s “address tag”-value pair should be used to store a token’s Mint address as a base58 string:
To allow association:
solana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v
To deny association:
solana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v deny=1
Example: To associate the USDC token with a domain, create an association record with a value of:
solana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v
Which is also the same as:
solana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v allow=true
solana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v allow=1
solana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v network=mainnet allow=1
To deny association of a token’s mint:
solana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v deny=1
Which is also the same as:
solana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v deny=true
solana-mint-address=EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v network=mainnet deny=true
solana-program-address
This association record’s “address tag”-value pair should be used to store a program’s address as a base58 string:
To allow association:
solana-program-address=SMPLecH534NA9acpos4G6x7uf3LWbCAwZQE9e8ZekMu
To deny association:
solana-program-address=SMPLecH534NA9acpos4G6x7uf3LWbCAwZQE9e8ZekMu deny=1
solana-address
This association record’s “address tag”-value pair should be used to store any address as a base58 string. This is considered a fallback address tag and should only be used when the other, more explicit tags do not fit the address’ use case.
To allow association:
solana-address=nicktrLHhYzLmoVbuZQzHUTicd2sfP571orwo9jfc8c
To deny association:
solana-address=nicktrLHhYzLmoVbuZQzHUTicd2sfP571orwo9jfc8c deny=1
Linking a domain within onchain information
If there is a domain referenced in onchain-linked data, including the contents of referenced offchain JSON files, an association check should be performed to verify a connection to the brand/company’s linked domain.
When wallets, token lists, block explorers, and other applications are presenting information about onchain data (i.e. url for a token’s image, primary media file, or url for its additional offchain metadata file), they should also present the address/domain association status when available.
These “association validators” can poll the DNS records (or solana.txt file) for the domain in question and validate if the domain and onchain information point to each other. If they do and are in the “allow” state, the ecosystem can consider these associated. Otherwise, not-associated and handled as such.
Listed below are some of the most commonly used standards where domains are referenced in onchain-linked data. As future metadata standards develop and are adopted, they should also consider and adopt similar pattern. And therefore be supported by this specification.
Metaplex’s Fungible and Non-fungible asset standards - contain support for the image, external_url, and animation_url fields
SPL Token metadata interface - contain support for the a uri field that links to offchain metadata file, normally as a JSON format. This JSON file commonly follows one of Metaplex’s standards.
security.txt - contain the project_url field and other domain related fields
Security considerations
Permissioned access. Modifying DNS records or uploading the solana.txt file require permissioned access to a domain and its resources. If an attacker was able to gain permissioned access, they could modify the association records. Due to this not being an attack vector being specific to this specification, it is consider out of scope. Domain administrators should ensue they have strong security practices to prevent this.
Homograph attacks. With the use of domain names to validate if an addresses is associated, attackers will likely attempt to perform these letter substitution attacks on domains/users.
Since the address/domain association checks will be performed by code and automated systems, so it is not a concern there.
However, when the domain names are presented to users they could become fooled by the letter substitutions. As such, applications should be diligent in helping to minimize/prevent fake domains by performing mitigation steps. See examples here.
DNS cache poisoning. Attackers could attempt to poison the DNS cache for servers and users attempting to get the association records from a domain. If an attacker is successful in this type of attack, association validators could be fooled into validation the attacker-inserted association. Resulting in applications and users falsely thinking the attacker’s address is truly associated with the domain.
DNS cache poisoning can be prevented by domain administrators enabling and using DNSSEC. It is strongly recommend that administrators enable DNSSEC and that automated validation processes check for DNSSEC enabled domains.","[joshmo_dev]: This is awesome! Voicing my support as this initiative will not only significantly help companies who are on-chain to be verified, but also prevent companies who are purely off-chain from becoming the target of nuisance crypto scams.

[bjoerndotsol]: Love the idea. If we go with the solana.txt file we could even think of including the actions.json for blinks in there

[wedtm]: I think this would need some additional record akin to DS records that contains a signature from the authority (wallet / update).
Without this, a nefarious website could link any program/address to their domain.
If client implementations utilize sRFC-35 for sensitive or verification purposes this could lead to loss of integrity in the security sense and make for a bad time.

[blockiosaurus]: You will lose 90% of companies as soon as they have to learn to deploy a Solana program.
I agree with @wedtm that DNS doesn’t really seem like the best place for this.
You really want some sort of on-chain attestation, most of which can be built with existing tools like Civic KYC, Core Autograph plugins, Bonfida domains, Solana ID, etc. I think it makes the most sense to create a spec with existing partners instead of building a new tool.

[blockiosaurus]: Edit to 1, maybe I misread and the program deploy only relates to associated program IDs.
Addendum, this is already possible with mints using the Verified Creators array on Token Metadata. A DAO or KYC wallet can sign to verify themselves as a creator for the token/NFT mint.

[nickfrosty]: The other DNS record types are interesting 
I’m not sure I agree with your point about nefarious websites linking to real tokens. Since the token would also have to link back to the website.
Both have to point at each other for the association to be valid. Same for programs.
For a generic address, this might apply though.

[nickfrosty]: How does a TM verified creator help here? It ensures the Singer was capable of singing. But it in no way ensure the signer is also actually in control of any of the domains listed in the metadata fields

[nickfrosty]: Program deploys applied to the metadata already stored in the programs info (like security txt)
If a company does deploy a custom program, they lost their domain in the security txt metadata, then update their domain to effectively acknowledge that they own the program.
If they don’t want to deploy a program, they don’t have to

[blockiosaurus]: Verified creator is a two way handshake. The token authority must add the creator to the array (e.g. Metaplex DAO wallet gets added to MPLX) and the added creator must also sign (Metaplex DAO wallet signs the metadata). Bidirectional attestation.

[beeman]: I really like this idea, it seems like the only real way for a Web2 company to verify an on-chain identity.
Using a Solana-based protocol for verification doesn’t actually solve the problem since there’s no guarantee the company is who they claim to be. It just shifts the issue somewhere else.
It’s also the only way a company can actually deny being associated with anything on-chain. Asking them to sign a Solana transaction to prove they aren’t on Solana doesn’t seem to make much sense.

[wedtm]: Attack scenario:
I can vanity a token address that matches the first and last N characters of a popular token to avoid the common pattern of middle-out truncation.
link it to a scam site, and implement sRFC-35 with only the suggested.
start a common spam campaign with a 1-2% success rate across 100k impressions
This doesn’t even begin to go down the rabbit hole of orgs that do wildcard entries to places like vercel, allowing anyone to create a new vercel site with scamsite.legitcompany.com.
We could probably role-play a few more scenarios, but the real benefit of the signature is that it allows cryptographic validation by machines. This would enable something like the green lock / address bar for extended validation TLS certificates.
THIS would incrase user experience massively as wallets and clients could use this bi-directional verification as a first line of defense in filters.
If the DNS signature is from the token deployer, that’s a cryptographic prove that the same entity controls access to both.

[nickfrosty]: This still only connects addresses to other addresses. It doesn’t solve the same issue that this sRFC addresses. It does nothing in the way of attesting offchain info (like a domain name) is validated or not. Its just two addresses connecting together

[nickfrosty]: Re: your attack scenario
The existing ecosystem trust assumptions still exist. The only benefit of sRFC-35 is that if multiple tokens are created and point to the same domain, you can tell which is the one true token. This spec does not technically improve anywhere else (yet). Even with your signature idea, the same attack scenario still exists.
They are issuing their scam token and association on their scam site. They could add the signature into their record. Signature does not prevent this, it just validates that the scam token minter updated the DNS entry.
Therefore, what is the actual benefit of putting a signature in the association record? I can only think of it negating scenarios where people try to associate with a token they did not issue (which is still useful). So I’m totally game to add something into the spec here for it. Do you have a suggestion on how it should be added?
My initial thought is adding the signature into the association record so it could be verified by others. The message to be signed should a standard format including the domain itself and the address and maybe a timestamp of some sort?
For tokens, token issuer signing makes sense but you would also have to go back in chain history for that (since mint authority could have changed or been removed). Mint authority could be used but what if it is removed? Same for upgrade authority.
For programs, upgrade authority could be used to sign but it can also be removed. So what then?
For generic addresses, they could sign their own message. So I guess that works

[wedtm]: Signature does not prevent this, it just validates that the scam token minter updated the DNS entry.
The signature allows easy denouncement. Any app/dapp can automatically invalidate any token that claims to be from a domain that:
Has a valid signature in it’s DNS, and;
doesn’t have a signature for the token in question.
I do think that tokens are the most problematic like you’ve pointed out since any link could be renounced later.
Potentially the spec could specify that only the most recent authority is considered valid unless it’s completely renounced and then the last is considered authoritative?
That doesn’t really solve for the opposite where a domain might eventually change ownership as well.

[seeker]: I have published an IETF Internet Draft for mapping that we hope will become an internet standard.
 
 
 IETF Datatracker
 
 
 
DNS to Web3 Wallet Mapping
 
This document proposes a implementation standard for mapping wallets to domain names using the new WALLET RRType, allowing for TXT record fallback while the WALLET RRType propagates through DNS providers. The goal is to provide a secure and scalable...
 
 
 
 
 
 
Would primarily use the WALLET RRType and DNSSec to secure the addresses.
Would like to see if we can unify this into an RFC to everyones benefit.

[nickfrosty]: Just read through the IETF draft you proposed. It looks like we have a lot of similar ideas on this idea.
After a first read, your proposal only allows associating base chain coins (e.g. BTC, SOL, etc) to a domain, but does not really allow more fine grain association of tokens created within the chain’s themself. This would be fine for the chains that do not really support additional tokens being created by users of the network (like bitcoin), but for smart contract chains like Solana and Ethereum, your proposal does not really support people/companies associating other tokens to their domain. Example: Circle’s USDC on Solana and/or Ethereum or Paypal’s PYUSD on Solana
In addition to wallet addresses that can hold the base chain token’s, addresses can be used for other purposes aside from holding tokens. Solana programs also have a base58 encoded address similar to a regular “wallet address”. Same for Ethereum smart contract addresses.
How do you propose unifying our proposals? Including to ensure we can cover other types of address that are not just the base layer coin?

[nickfrosty]: @seeker accidentally didn’t reply, so tagging you here

",nickfrosty,362,13,17,18,2025-02-06T17:07:10.088Z,2025-03-06T01:14:12.452Z,sRFC,6,2025-03-06T01:14:12.452Z
2026,sRFC 33 - Sign message in Actions/blinks,https://forum.solana.com/t/srfc-33-sign-message-in-actions-blinks/2026,"sRFC 33 - Sign message in Actions/blinks
TLDR
Add the ability for Actions and blinks to ask users to sign a plaintext message. This is a common and no-cost way to validate a user actually controls a given wallet address.
Background
Currently, all actions ultimately require the user to sign and send a transaction to be confirmed on chain. So there is always transaction fee for users and therefore always a realized cost. Even in action chaining, the user must sign a transaction before proceeding to the next action. Each time paying a transaction fee.
A very common flow within dApps is asking the user to sign a plaintext message to validate they do in fact control a specific wallet address. This is great because it is completely free for users. There is no transaction fee paid.
Adding the ability for actions/blinks to request users to sign message will unlock new use cases for blinks, including:
reducing costs for users
blinks could authenticate a user’s wallet address natively within the blink
allow blinks to craft customized blink metadata based on the wallet interacting with it
Proposal
Note: specific implementation details to be worked out in PRs.
add the ability for an action to request a user sign a transaction OR plaintext message via updating LinkedAction to support both
ActionPostResponse should handle different “action response types”, one being for transaction signature requests and another being message signature request
messaged being signed should have a consistent structure and ensures their can be a security mechanism to ensure messages originated from its own action api
Requesting the user sign a message
LinkedAction be updated to support multiple types. Something similar to this:
export type LinkedActionType = ""transaction"" | ""sign-message"";
export interface LinkedAction {
 type?: LinkedActionType;
 href: string;
 label: string;
 parameters?: Array&lt;TypedActionParameter&gt;;
}
When the LinkedActionType is a sign message type:
the blink client will make a POST request to the href with the user’s account address (just as transaction focused actions do now)
the api server will respond with the message for the user to sign (see message sign request)
after the user signs the message, the blink client should make a POST request to
Response payload for a sign message request
When an action api server is requesting a user sign a plaintext message, vice a transaction, the server should respond with an updated ActionPostResponse described below:
Note: Sign message requires action chaining. Therefore when an action api sends a sign message request to the client, the api server must also include a PostNextActionLink to inform action-clients where to send the signature of the user signed message.
export type ActionPostResponse = TransactionResponse | SignMessageResponse;
export interface TransactionResponse {
 type?: ""transaction""
 transaction: string;
 message?: string;
 links?: {
 next: NextActionLink;
 };
}
export interface SignMessageResponse {
 type: ""sign-message""
 data: SignMessageData; // see ""Structured sign message data""
 state?: string;
 message?: string;
 links: {
 next: PostNextActionLink;
 };
}
The state should be a utf-8 string of a MAC created by the action api server using a secret stored on that server. Action clients should pass this value back to the api server in the PostNextActionLink request. This enables api servers to cryptographically verify that the initial sign message request came from their server by generating a HMAC on their server. It also make it so they are not required to maintain server state of which messages their api requested users sign.
After the user signs the provided message, the blink client will make a POST request to the included next action (i.e. perform action chaining) with a payload as follows:
account (required) - the user’s wallet address that signed the message
data (required) - the structured data that the user was requested to sign. See Structured sign message data
signature (required) - the signature created by the account singing the data (as a base58 encoded string)
state (optional) - the same state value the action api initial provided, relayed back from the client.
An example of the updated NextActionPostRequest looks like this:
export interface NextActionPostRequest extends ActionPostRequest {
 /** signature produced from the previous action (either a transaction id or message signature) */
 signature: string;
 /** */
 data: SignMessageData; // see ""Structured sign message data""
 /** */
 state?: string
}
Note: since the data already supports a key-value object, no change to that type should be required.
After receiving the NextActionPostRequest, the action api should perform all required validation checks on the signature, data, and state to satisfy their business constraints.
The action api can now return the metadata for the next action, and the user can continue within the blink experience.
Structured sign message data
For better user experience and improved security, the plaintext message a user will be prompted to sign should be structure with a few required fields:
domain (required) - domain requesting the user to sign the message
address (required) - base58 string of the Solana address requested to sign this message
statement (required) - human readable string message to the user. it should not contain new line characters (i.e. \n)
nonce (required) - a random alpha-numeric string at least 8 characters. this value is generated by the action api, should only be used once, and is used to prevent replay attacks
issuedAt (required) - ISO 8601 datetime string. This represents the time at which the sign request was issued by the action api.
chainId (optional) - Chain id compatible with CAIPs, defaults to Solana mainnet-beta in clients. If not provided, the blink client should not include chain id in the message to be signed by the user.
type SignMessageData = {
 domain: string;
 address: string;
 statement: string;
 nonce: string;
 issuedAt: string;
 chainId?: string;
}
Note: this structured data is similar to the Sign In With Solana spec, but without the additional rarely used fields. Therefore structure of this data is compatible with SIWS.
chainId is consistent with sRFC 31: Compatibility of Blinks and Actions.
issuedAt should be validated by the action api during their signature verification process in order to perform any desired expiration checks (i.e. was this sign request issued in the last 10 minutes? if not, my api will reject it)
At a minimum, the required fields in the structured message should be presented to the user at or before they are prompted to sign said message.","[tsmbl]: @nickfrosty, this is great
General question - is there a reason for enforcing consistent structure and specifically use SIWS-like structure for every signed message in context of Blinks?
My understanding is that SIWS is great for initial authentication, but it is not strictly needed for every single message signing operation in solana. There can be application specific message signing operations, that are not SIWS compliant and this is fine. Imo, same applies to Blinks.
From the SIWS docs:
SIWS aims to standardize message formats in order to improve authentication UX and security, replacing the traditionally clunky connect + signMessage flow with a one-click signIn method.
SIWS shifts the responsibility of message construction from dapps to the wallet.
I like idea of SIWS, but I would propose to allow more freedom and let developers decide on final message structure by supporting arbitrary message signing, instead of enforcing SIWS-like SignMessageData structure. The approach below also has less coupling by relying on composition
export interface SignMessageResponse {
 type: ""sign-message""
 data: string // can still be SIWS-like message if it's needed by use-case
 state?: string;
 message?: string;
 links: {
 next: PostNextActionLink;
 };
}
This way we still can support both options:
a) use SIWS via composition, we even can provide utility functions for this
b) use arbitrary business specific messages
 nickfrosty:
The state should be a utf-8 string of a MAC created by the action api server using a secret stored on that server. Action clients should pass this value back to the api server in the PostNextActionLink request. This enables api servers to cryptographically verify that the initial sign message request came from their server by generating a HMAC on their server.
Like the idea of having optional message signature, can serve as an extra security level if use-case needs it. Do you have an idea or example of what attack can be prevented by verifying that the initial sign message request came from their server?

[nickfrosty]: I like idea of SIWS, but I would propose to allow more freedom and let developers decide on final message structure by supporting arbitrary message signing, instead of enforcing SIWS-like SignMessageData structure. The approach below also has less coupling by relying on composition
Developers still have the ability to ask for any message to be signed via the statement field in my proposed SignMessageData. This is the plain text message that the user will see when signing. So they get the same composability and business logic specific messages put into the statement field.
Enforcing a structure like this allows wallets to present all of this info the the user so the user can be sure that the data is coming from a place that they expect. Since blinks can (eventually) be on any domain, having this data being provided to the wallet can help boost user confidence and present a better UX in the wallet signing modal itself.
The fact that it is compatible with SIWS is sort of just a nice side effect and convenience.
Do you have an idea or example of what attack can be prevented by verifying that the initial sign message request came from their server?
Since anyone and any bot can send a signed message to the api server, including an HMAC allows the api server to validate the initial request came from their server to begin with (and do other logic checks like expiration checks, etc).

[nze]: hey everyone, thanks for moving sign message forward.
I agree with @tsmbl that SIWS seems to be too specific for a feature called “sign message”.
I would vote to keep sign message generic as it’s done in the wallets, and create a separate sRFC for SIWS support in actions/blinks if needed. Basically as a rule of thumb I’d propose to keep the same level of abstraction which is used for wallets and for wallet standard and not to overthink for developers, I believe a lot of devs might want to use a good old low level sign message for their needs

[tsmbl]: nickfrosty:
Since anyone and any bot can send a signed message to the api server, including an HMAC allows the api server to validate the initial request came from their server
Thanks for details! I understand the general idea, but couldn’t imagine a use-case, when this is useful.
Regarding bots, in my understanding, any bot can first make a request to API to generate the message to be signed, so what difference does having HMAC make?
Do you have an example use-case in mind, that demonstrates in which context this extra validation is needed? I guess this feature is inspired by some user request or specific idea of Blink, so curious to learn more about the specifics.

[nickfrosty]: the state can help it so the server does not need to store the state of the message-to-be-signed, since it can be verified via it being an HMAC string (if they chose to use it).
also to note, since the state is an arbitrary string that should not be modified by the client, they can also pass any other data if they want. HMAC was more of just an example of something to pass which can be used for extra verification/validation if the api provider wants to use it

[rexap]: One example use case for sign message is if I want to integrate non-blockchain actions that are still done by the same user/wallet. Take for example doing email verification for a user that is signing up for a platform via blink. Why should I pay or force the user to pay some even be it small tx fee just to move on to the next action in a multi-action blink?
Or as another example if I’m playing a game via blink not every action/move the user makes is a tx especially for games that are not fully on-chain.
In both cases, SIWS is too specific bc I don’t want to force a user to sign in every time they take an action that doesn’t require a tx.

[aliquotchris]: Hey all, chiming in here to get this initiative unblocked. Had some IRL chats including with Jon. Plan based on those conversations is as follows:
We’re going to get started on an implementation here alongside a couple design partners, specifically DRiP.
We’ll start with Nick F.'s proposal on a more opinionated implementation as is proposed here in the original post. While this isn’t strictly sign message in the most general sense, &amp; I am concerned about that from an educational &amp; specification perspective, I understand the desire to enforce a stricter message structure for safety &amp; security.
As we proceed with the implementation, we’ll report back if we learn that maintaining the original, generic implementation of sign message is what teams want.
Let’s get some great experiences shipped.

[aliquotchris]: This is good feedback rexap. Let’s connect on Telegram and discuss what you’re building. My username is @ chrisoss if you want to send me a message.

[aliquotchris]: @nickfrosty if the above sounds good, can you update the actions spec types to reflect this? Then we’ll have what we need to get started.

[nickfrosty]: there is another proposal here (SRFC #32) that covers “optional transactions” in actions/blinks.
effectively, it proposes the ability for actions to be only making a POST request and never asking the user to sign anything. I think this exact proposal is what you would be asking for here: “non-blockchain actions”?

[Qalbehabib]: Could you provide an example repo or code snippet to demonstrate the implementation of SignMessageResponse, structured message validation, and NextActionPostRequest? This would greatly help in understanding and implementing the feature.

[nickfrosty]: This is already live in the @solana/actions package: solana-actions/packages/solana-actions/src/signMessageData.ts at main · solana-developers/solana-actions · GitHub

",nickfrosty,438,8,12,13,2024-08-22T19:12:35.253Z,2025-02-07T14:46:05.725Z,sRFC,6,2025-02-07T14:46:05.725Z
2876,sRFC 34 - Standardized Relayer API,https://forum.solana.com/t/srfc-34-standardized-relayer-api/2876,"sRFC 34 - Standardized Relayer API
Summary
This RFC proposes a standard API that teams can adopt/implement to enable on-chain activity without end users holding SOL (gas abstraction/gasless transactions). Creating a network of relayers would enable end users, or companies, to choose their preferred relayer, or provide sponsored transactions with certain partnerships. Creating an up to date relayer also allows teams to build faster as they would not have to build their own. The ultimate aim is to create a marketplace of permissionless relayers with differentiated features that improve user experience.
Motivation
Many companies currently create their own version of a relayer (or Paymaster in EVM speak) to support their own business and use case. Historically, teams tend to fork Octane and re-work it to their needs. This, however, is less than ideal as it has not been updated in three years, was not audited, and is not production ready. Additionally, relayers have proven themselves to be a required primitive in many scenarios which would benefit from composability and a permissionless standard. Solana thrives on well established primitives rather than copy pasting code for each use case.
Use Cases
This relayer is designed to be flexible to support the major flows teams encounter. This means that it can provide customers a way to transfer SPL tokens, or execute an arbitrary transaction without needing to own any SOL. The relayer may choose to sponsor transaction fees for a transfer completely or request a fee to be received in the SPL token that is transferred. Here are the main use cases I am targeting:
Payments: Transferring tokens without needing SOL.
Smart wallet activity: As a PDA cannot be a fee payer, it requires a relayer or another party to pay transaction fees. Having a standardized set of relayers which can support these transactions increases their proliferation and usage.
Embedded wallets: Embedded wallets are often used as a way to abstract the complexities of wallets away from end users. As an example, in game actions that utilize an embedded wallet can use a relayer to execute and sponsor game activity.
Proposal
Important: This spec is purposefully not prescriptive on the implementation details of the relayer and instead is purely focused on utilizing a common API interface. Each relayer provider can differentiate on features and capabilities. As an example, it may be worthwhile for one relayer to add lighthouse guards to transactions but not useful for another. Additionally I expect a number of “add-on” standardized features to be developed and provided by different providers such as Jito bundling.
An Rust based implementation named “Kora” is in progress by Solomon here.
Below is an overview of the proposal. To see the full implementation proposal, please refer to this doc and provide direct comments where desired.
Relayer RPC HTTP Methods Overview
Method Name
Description
Required Parameters
Result
feeEstimate
Estimate the fee for an arbitrary transaction using a specified token.
transaction, fee_token
estimated_fee, conversion_rate
transactionTransfer
Create a transfer transaction for a specified token, sender, and recipient. The token supplied will be assumed to be the token to also be used for fees. Returns a partially signed transaction.
amount, token, source, destination
transaction, fee_in_spl, token, fee_in_lamports, valid_until_blockheight
transactionPrepare
Prepare a transaction by adding relayer-specific instructions. Returns a partially signed transaction.
transaction, fee_token
transaction, fee_in_spl, fee_token, fee_in_lamports, valid_until_blockheight
transactionSign
Sign a prepared transaction without submitting it to the blockchain.
transaction
transaction, signature
transactionSignAndSend
Sign and submit a transaction to the blockchain.
transaction
transaction, signature
transactionSend
Submit a fully signed transaction to the blockchain.
transaction
signature
getSupportedTokens
Retrieve a list of tokens supported by the relayer for fee payments.
(none)
tokens (list of token metadata)
Key Terminology
transaction: Base64-encoded serialized Solana transaction. This could be a signed or unsigned transaction.
signature: Unique “transaction hash” that can be used to look up transaction status on-chain.
source: Source wallet address. The relayer is responsible for deriving and the TA.
destination: Destination wallet address. The relayer is responsible for deriving and creating the TA if necessary.
fee_token: Token mint address for the fee payment.
fee_in_spl: Fee amount the end user will pay to the relayer to process the transaction in spl tokens in the smallest unit of the spl token (no decimals)
fee_in_lamports: Fee amount in Lamports the Paymaster estimates it will pay for the transaction.
valid_until_block_height: Expiration block height for time-sensitive operations.
tokens: Array of supported token metadata (e.g., symbol, mint, decimals).
features: Array of features enabled by the relayer (e.g., bundle support, sponsorship).
Note: I am in no way married to the name “relayer”. I just don’t really want to use paymaster.",,ilan_g,520,0,0,1,2025-01-07T21:30:08.728Z,2025-01-07T21:30:08.786Z,sRFC,6,2025-01-07T21:30:08.786Z
2539,Directly supporting blinks in wallets (aside from external sites/x),https://forum.solana.com/t/directly-supporting-blinks-in-wallets-aside-from-external-sites-x/2539,"Adding blinks support for QR scanning wallets
Summary
Browser wallets like Phantom support blinks by reading a URL and rendering data received back. For example, displaying options for the user to pick from, entering an amount, etc… before submitting the transaction.
What would be good, is if this same flow and UX was initiated by wallets scanning a blink url (outside the context of any existing site or social media like X).
Flow:
mobile wallet scans a blink URL
the URL returns data such as options for the user to select
the wallet renders the options (same as it does in X for blink) and allows the user to input some data and/or make selections
the user then submits the transaction
Simple Use Case:
User-selected payment method. The simple scenario that we’d like to build goes like this:
merchant displays a blink as a QR code
mobile app scans the the code and submits the initial GET request
the action provider returns a series of payment options (USDC, USDT, BONK, etc…)
the wallet renders the options for the user to select from
user/wallet selects an option and submits the returned transaction
This would greatly improve the flow and UX, as right now the merchant has to select the payment currency before generating the QR code to scan.
One ideal improvement here would be the ability to submit the wallet pubkey in the initial GET request so that the action provider could respond back with actual available options instead of blindly returning selections that may or may not be available to the user. Pretty obvious though, and I know it’s already been mentioned by a lot of folks.",,silostack,94,0,0,1,2024-11-30T17:20:21.607Z,2024-11-30T17:20:21.654Z,sRFC,6,2024-11-30T17:20:21.654Z
2072,Multiple txs in actions/blinks,https://forum.solana.com/t/multiple-txs-in-actions-blinks/2072,"Multiple txs in actions/blinks
Summary
Currently Action POST response only allows for the returning of a single transaction (tx) as a string.
/**
 * Response body payload returned from the Action POST Request
 */
export interface ActionPostResponse&lt;T extends ActionType = ActionType&gt; {
 /** base64 encoded serialized transaction */
 transaction: string;
 /** describes the nature of the transaction */
 message?: string;
 links?: {
 /**
 * The next action in a successive chain of actions to be obtained after
 * the previous was successful.
 */
 next: NextActionLink;
 };
}
There are many cases where more complex on-chain actions with programs need to be taken due to the limitation of Solana account and tx sizes. In such cases forcing the user to separately sign multiple txs when wallets already support the signing of multiple txs at once (ie. phantom with signAndSendAllTransactions) simply results in worse UX. Oftentimes this means making a decision between relaying/proxying txs for better UX or creating long more transparent action chains with worse UX.
Implementation:
Expanding the transaction field to an array of txs would solve this problem and allow for users to still have great UX while retaining the transparency of executing more complex on-chain actions with multiple txs directly. For example something like:
/**
 * Response body payload returned from the Action POST Request
 */
export interface ActionPostResponse&lt;T extends ActionType = ActionType&gt; {
 /** array of base64 encoded serialized transaction(s) */
 transactions: [];
 /** describes the nature of the transaction */
 message?: string;
 links?: {
 /**
 * The next action in a successive chain of actions to be obtained after
 * the previous was successful.
 */
 next: NextActionLink;
 };
}","[Damien]: I think for most cases it makes sense to batch multiple instructions into one transaction and return it instead of returning an array of multiple transactions.
It’s possible I’m wrong, however as far as I know the only benefit of signing and sending multiple transactions as once is to go around the max size limit and account read/write limit of a single transaction on Solana. On the other hand, multiple transactions mean the user pays more in fees and the transactions are not atomic, meaning that some could fail and others may succeed, which are all cases that would cause harm to the UX instead of improving it.
For these reasons I see that specifically for Actions and Blinks transaction batching makes more sense instead of returning multiple transactions to be signed and sent.

[rexap]: Yes the tx size limit is the primary issue. Most complex on-chain actions cannot be done all in one tx bc you cannot pack all of the required ixs into one tx.
Currently for most non blink use cases standard practice for failed txs is to simply retry them. I would imagine something similar could be implemented in blinks use cases.
Am curious what you mean by “Actions and Blinks transaction batching”? Do you mean action chaining?

[mat]: This would be very beneficial for us at Access Protocol as well as we could implement a blink for reward claiming.
We are batching as many operations into a single transaction as possible but even with this approach there are multiple transactions to be signed due to the transaction size limit.
I think that this is a good idea as the most prominent wallets have an implementation of signTransactions method

[rexap]: @nickfrosty @tsmbl @aliquotchris @0xaryan would love your feedback on this

",rexap,228,2,4,5,2024-09-09T23:00:05.617Z,2024-10-21T20:11:01.823Z,sRFC,6,2024-10-21T20:11:01.823Z
2267,sRFC 33: Media Types of Blink,https://forum.solana.com/t/srfc-33-media-types-of-blink/2267,"Summary
Currently, the media types, support for the icon field are svg , png, webp, which limits the design space for the developer to show content.
The goal is to introduce default media types supported by most of the latest browsers, which will widen the type of content that blinks can show starting with video/audio and more types of images.
Media Types
1. Image Types
JPEG (image/jpeg)
GIF (image/gif)
BMP (image/bmp)
2. Audio Types
MP3 (audio/mpeg)
WAV (audio/wav)
OGG (audio/ogg)
AAC (audio/aac)
WebM Audio (audio/webm)
3. Video Types
MP4 (video/mp4)
WebM (video/webm)
OGG Video (video/ogg)
MPEG (video/mpeg)
Implementation
Currently the ActionGetResponse looks like below
/**
 * Response body payload returned from the initial Action GET Request
 *
 * note: `type` is optional for backwards compatibility
 */
export interface ActionGetResponse extends Omit&lt;Action, ""type""&gt; {
 type?: ""action"";
}
/**
 * A single Solana Action
 */
export interface Action&lt;T extends ActionType = ""action""&gt; {
 /** type of Action to present to the user */
 type: T;
 /** image url that represents the source of the action request */
 icon: string;
 /** describes the source of the action request */
 title: string;
 /** brief summary of the action to be performed */
 description: string;
 /** button text rendered to the user */
 label: string;
 /** UI state for the button being rendered to the user */
 disabled?: boolean;
 links?: {
 /** list of related Actions a user could perform */
 actions: LinkedAction[];
 };
 /** non-fatal error message to be displayed to the user */
 error?: ActionError;
}
If we modify the Action to include 2 additional fields, media and mediaType
/** 
* Media Types for media in the action
*/
export type MediaType = ""video/mp4"" | ""video/webm"" | ""video/ogg"" | ""video/mpeg"" | ""image/jpeg"" | ""image/png"" | ""image/gif"" | ""image/svg+xml"" | ""image/webp"" | ""image/bmp"" | ""image/x-icon"" | ""audio/mpeg"" | ""audio/wav"" | ""audio/ogg"" | ""audio/aac"" | ""audio/webm""
/**
 * A single Solana Action
 */
export interface Action&lt;T extends ActionType = ""action""&gt; {
 /** type of Action to present to the user */
 type: T;
 /** image url that represents the source of the action request, and
 will be used as a fallback image incase
 media URL is un-reachable/un-renderable */
 icon: string;
 /** media URL that represents the content to be shown upon request**/
 media : string;
 /** represents the type of media URL provided */
 mediaType : MediaType;
 /** describes the source of the action request */
 title: string;
 /** brief summary of the action to be performed */
 description: string;
 /** button text rendered to the user */
 label: string;
 /** UI state for the button being rendered to the user */
 disabled?: boolean;
 links?: {
 /** list of related Actions a user could perform */
 actions: LinkedAction[];
 };
 /** non-fatal error message to be displayed to the user */
 error?: ActionError;
}
Then this would allow the blink developer to include more media types, icon field would still be present to maintain backward compatibility + used as a fallback if the media URL is un-reachable / un-renderable.","[rexap]: Very much agree the current media types can be limiting.
I do think perhaps not restricting to just one file though could be helpful and more forward compatible as spec evolves. Perhaps formatting the media field more like the optional links field would allow for this while also keeping the spec compact:
media?: {
 /** list of media items to display */
 items: MediaItem[];
 };
where media items could be:
interface MediaItem {
 url: string;
 type: MediaType;
}
Of course this brings up the question of how to render multiple media files. An initial thought is providing an optional display field could define how to show the media files with the default being simply displaying the first item.
media?: {
 items: MediaItem[];
 display?: ""default"" | ""carousel"" | ""gallery"" | ""list"";
 };

[0xaryan]: What use-case do you think for array of Media, that would make it a gallery like layout, which imo would not be required by majority of use case

[rexap]: This comes primarily from e-commerce use case where want to have multiple images of a product without forcing a user to create complex product images or having to custom generate one image from a set of images. I also think social, gaming, and RWA blinks would find this useful too.

",0xaryan,111,2,3,4,2024-10-17T13:29:58.603Z,2024-10-21T16:01:41.206Z,sRFC,6,2024-10-21T16:01:41.206Z
1971,sRFC 32: Optional Transactions in Action Chaining,https://forum.solana.com/t/srfc-32-optional-transactions-in-action-chaining/1971,"sRFC 32: Optional Transactions in Action Chaining
Summary
Transactions in Action Chaining are non-optional right now, therefore for each action in the chain the user need to sign completely differnet transactions which works fine for a limited use-cases but for majority of the user-cases and also to create a better UX, the user should only sign the transaction once whenever possible, that would be mostly the last action in the chain.
Same way as any form works, taking information from the user and only showing the submit button at the last. This could open up some interesting design spaces, where a game is getting played on the blink and when game ends, the winner can claim the price, or generating NFT’s with help any AI directly from the blink and deploying the collection, these are just some examples to support.
Implementation:
Currently when the client makes the POST request, then ActionPostResponse is sent with following structure.
/**
 * Response body payload returned from the Action POST Request
 */
export interface ActionPostResponse&lt;T extends ActionType = ActionType&gt; {
 /** base64 encoded serialized transaction */
 transaction: string;
 /** describes the nature of the transaction */
 message?: string;
 links?: {
 /**
 * The next action in a successive chain of actions to be obtained after
 * the previous was successful.
 */
 next: NextActionLink;
 };
}
If, we modify the ActionPostResponse to keep the transaction as an optional field
/**
 * Response body payload returned from the Action POST Request
 */
export interface ActionPostResponse&lt;T extends ActionType = ActionType&gt; {
 /** base64 encoded serialized transaction */
 transaction?: string;
 /** if transaction is present, describes the nature of the transaction */
 message?: string;
 links?: {
 /**
 * The next action in a successive chain of actions to be obtained after
 * the previous was successful.
 */
 next: NextActionLink;
 };
}
The client can now skip the pop-up to user to sign the transaction and depending upon the NextActionLink render the blink.
export type NextActionLink = PostNextActionLink | InlineNextActionLink;
/** @see {NextActionPostRequest} */
export interface PostNextActionLink {
 /** Indicates the type of the link. */
 type: ""post"";
 /** Relative or same origin URL to which the POST request should be made. */
 href: string;
}
/**
 * Represents an inline next action embedded within the current context.
 */
export interface InlineNextActionLink {
 /** Indicates the type of the link. */
 type: ""inline"";
 /** The next action to be performed */
 action: NextAction;
}
If the NextActionLink is of type InlineNextActionLink then the client can directly render the metadata and no callback would be required.
If the NextActionLink is of type PostNextActionLink then the action server can return metadata depending on the previous inputs by the user address
It should be upto the aciton server to server a InlineNextActionLink or PostNextActionLink type of NextAction to make a better UX for the specific use-case.
This way, the action server can also track informations across a series of actions happening on the client. The idea of JWT also crosses the mind, but imo having each action linked to an address and on client only the given address can sign the transaction removes a lot of concerns.","[tsmbl]: I like the idea, allowing user actions without requiring direct wallet interaction can significantly improve the UX for certain use cases, opening up new design and application possibilities.
As we explore this further, adding more action types makes sense, especially with potential future needs like signing messages or navigating to external resources. To support this, I suggest refining the implementation to be more strict and explicit, below is a rough idea of how the interfaces could look with respect to my comment
// Can be extended to ""tx"" | ""post"" | ""sign-message"" | ""external-link"" in the future
export type LinkedActionType = ""tx"" | ""post"" 
export interface LinkedAction {
 type?: LinkedActionType; // Can be used to tailor UX/UI depending on action to be performed
 href: string;
 label: string;
 parameters?: Array&lt;TypedActionParameter&gt;;
}
As an idea we can also correspondingly define multiple response types, it can also simplify the flow a bit
export type ActionPostResponse = TxResponse | PostResponse;
export interface TxResponse {
 type?: ""tx""
 transaction: string;
 message?: string;
 links?: {
 next: NextActionLink;
 };
}
export interface PostResponse extends Action {}
Anyway, I think we can discuss the final interfaces during the PR in actions-spec.

[0xaryan]: Agreed, on having more action types, just saw a sRFC requesting for external-link so the implementation will make more sense in the long term.
// Can be extended to ""tx"" | ""post"" | ""sign-message"" | ""external-link"" in the future
export type LinkedActionType = ""tx"" | ""post
I like the idea of defining multiple response types, it will enable more explicit types. I’ll make a PR in action-specs

[scriptscrypt]: Instead of optionally passing / framing the transaction, If we could pass the tx base64 back to the server.
txbacktoActionServer974×162 69.4 KB
Meaning, Instead of exposing the tx directly to the client, we can pass the tx to the chained action,
So that once the chain of actions are completed, user can only sign the transaction once before the last step,
This also ensures all the actions have a transaction, and can be found onchain.
The chained actions should check for Previous / Incoming Transactions (from the params / body), append that to the current transaction, then construct a transaction, before it reaches the completed action.
The code for merging the tx might look like this :
import { Transaction, Message } from '@solana/web3.js';
function mergeTransactions(tx1Base64: string, tx2Base64: string): string {
 // Decode base64 transactions
 const tx1Buffer = Buffer.from(tx1Base64, 'base64');
 const tx2Buffer = Buffer.from(tx2Base64, 'base64');
 // Deserialize transactions
 const tx1 = Transaction.from(tx1Buffer);
 const tx2 = Transaction.from(tx2Buffer);
 // Create a new transaction and combine instructions
 const mergedTx = new Transaction();
 mergedTx.add(...tx1.instructions, ...tx2.instructions);
 // Set the feePayer and recentBlockhash from one of the original transactions
 mergedTx.feePayer = tx1.feePayer;
 mergedTx.recentBlockhash = tx1.recentBlockhash;
 // Encode the merged transaction back to base64
 const mergedBuffer = mergedTx.serialize();
 const mergedBase64 = mergedBuffer.toString('base64');
 return mergedBase64;
}

[nickfrosty]: I really like this idea and concept @0xaryan: not all actions must result in a transaction. The current “a transaction is required” stemed from the general design for blinks/actions of “actions return transaction”, but there is no reason we need to continue to shoehorn into that design space anymore (I think)
I also prefer @tsmbl’s recommendation of clarifying the types to make them more extensible. Multiple “response types” will already be needed for the Sign Message functionality, so this is also inline there too.
I think proposed PostResponse simply extending Action is interesting since I think it would also enable actions to never prompt for a transaction or other signable event. The api server could continue to return an “action” metadata until eventually return type=completed, then the user would never be prompted to sign anything. I don’t see this as something we need to prohibit, but I do think it may require more thought on implications.
@tsmbl what is the benefit of adding a type into LinkedAction? What “tailor UX/UI” do you foresee?
Should the ActionPostResponse have an single type? My thought is that on the blink/client side, performing the checks of what to do with the POST response, you would have to now check for all type variants for the post ActionPostResponse’s type and the ActionType

[nickfrosty]: this would then result in the transaction able to be altered on the client side before it is sent back to the server. and that should seriously be avoided!
with @0xaryan general idea here, and the existence of action chaining now, you can already build a blink with multiple actions that can build a complex transaction like you are proposing doing with merging the instructions together. except without all the security risk of transaction being alterable on the client side.
it looks like the missing piece for your described flow is what Arun is proposing: not all actions require the user to sign a transaction, effectively just allow a form submit. so you ultimately built one single transaction for the user to sign, based on all the inputs and action chaining steps you collected

[0xaryan]: I have made a PR which can clarify the whole flow.
It took me time to wrap my head around the whole flow, but it finally made complete sense with Alexey’s idea.
IMO, having type in the LinkedAction can help the blink client look for only specific data fields when the action endpoint returns the data.
for example,
in the case of type post, the blink client would know to not look for transaction in the response
in the case of type sign-message, the blink client would only look for data: SignMessageData
In a general sense, to create more explicit types on both the blink client and action sides.
 nickfrosty:
I don’t see this as something we need to prohibit, but I do think it may require more thought on implications.
I think, enabling this would simply make the blinks, a metadata standard to the URLs which anyone can use to take any form of data from the user. The ability to sign transaction/messages is just the cherry on top.

[nickfrosty]: I saw the PR and I am skimming it now 
having type in the PostResponse makes sense to me, just not in LinkedAction for this. it seems irrelevant to be in LinkedAction.
for example, the initial GET request to display the first action to a user:
the LinkedActions are displayed as form elements the user can interact with
no matter if there is a type representing “transaction” or “post” requests, the client will still make a POST request to the href endpoint. this POST request will be made either if type=post or type=transaction
the RESPONSE of this POST request should have a type noting what data is being returned for the user to interact with:
if type == “transaction” =&gt; asking the user to sign the transaction (the current flow of all actions)
if type == “sign-message” =&gt; ask the user to sign a message
if type == “post” =&gt; nothing for the user to sign, we just wanted to save the user input fields in our database
now we process the links.next action
this same flow would be the same concept for chained actions too, not just the initial GET request
I don’t see any need to add a type into LinkedAction, just the PostResponse to determine how the action-client should handle the response

[0xaryan]: ah, got it. makes sense. i’ll update the PR for the same.

[tsmbl]: nickfrosty:
@tsmbl what is the benefit of adding a type into LinkedAction? What “tailor UX/UI” do you foresee?
I don’t see any need to add a type into LinkedAction, just the PostResponse to determine how the action-client should handle the response
The only potential benefit I see is customization of the button press behaviour &amp; button UI. One example can be External Linking, for this case we can change button to visually display external link icon somewhere on button and change client behaviour to open a new tab, instead of making a POST request.
I’ve noticed that you also proposing to have type in LinkedAction in Sign Message proposal. Is it a bit outdated based on what was discussed in scope of this sRFC, or you have different considerations for having it in LinkedAction in your proposal?
@nickfrosty, the flow you’ve described above looks right to me if we don’t consider External Linking.

[nickfrosty]: The only potential benefit I see is customization of the button press behaviour &amp; button UI. One example can be External Linking , for this case we can change button to visually display external link icon somewhere on button and change client behaviour to open a new tab, instead of making a POST request.
I only think it is needed in the POST response. I don’t think a type on LinkedActions really matters, at least right now. Right now, no matter what, the linked action will always make a POST request to the href. Adding a type field in the POST response is what would help determine how the response is handled (i.e. is this a transaction, sign message request, external link, etc). The client would handle this then.
I’ve noticed that you also proposing to have type in LinkedAction in Sign Message proposal. Is it a bit outdated based on what was discussed in scope of this sRFC, or you have different considerations for having it in LinkedAction in your proposal?
Good catch, yes that is outdated then. I do not think LinkedActions should have a type at all with any of the current sRFCs or any specific features I foresee in the future.
I’m happy to have my mind changed, I just do not currently see a need for it.
@nickfrosty, the flow you’ve described above looks right to me if we don’t consider External Linking .
what special consideration do you see for “external linking” as an action?
my thought was:
if type == “external-link” =&gt; nothing for the user to sign, no post request, just display a button/link to open the provided href as a full link in a new tab (likely requiring this be an absolute url and displaying the domain to the user below the button/link)
aside from good UX design from designers (like you mentioned in the other sRFC), do you foresee something different?
Side notes:
I generally thing “external links” should NOT be allowed in the initial GET metadata, ONLY in follow on chained actions. If external links were supported, then people might simply put a external link and no other action. This feels like an anti-pattern for what actions are.
I feel fairly strongly that external linking should be considered a terminal action, like the current completed state. After the user has completed all interactions with the blink, the provider can provide an external link to be display to the user. I think this because external links inherently take the user away from the blink they are interacting with.
so this is contributing to me thinking there is no need to add a type field into LinkedAction

[tsmbl]: nickfrosty:
I do not think LinkedActions should have a type at all with any of the current sRFCs or any specific features I foresee in the future.
what special consideration do you see for “external linking” as an action?
my thought was:
if type == “external-link” =&gt; nothing for the user to sign, no post request, just display a button/link to open the provided href as a full link in a new tab (likely requiring this be an absolute url and displaying the domain to the user below the button/link)
My consideration was based on the External Linking proposal and previous experience with other tools, that clearly indicate the action type on button in certain cases. I was assuming that External Linking is proposed to be implemented by adding type to LinkedAction, allowing to process external link without making extra POST call. I also didn’t notice that you’re hesitant about the External Linking proposal.
 nickfrosty:
I only think it is needed in the POST response. I don’t think a type on LinkedActions really matters, at least right now. Right now, no matter what, the linked action will always make a POST request to the href.
I fully agree that having type only in POST response is sufficient to deterministically cover the general flow. That said, it looks healthy thing to have type in LinkedAction in scope of External Linking for the reasons below
Implementing extra POST endpoint &amp; making extra network call to implement external link feels a bit redundant to me
Visually displaying external link on button can slightly improve the UX
Please reflect your thoughts in Blinks CTA: External Linking, we should probably discuss it there, lol.
 nickfrosty:
I generally thing “external links” should NOT be allowed in the initial GET metadata, ONLY in follow on chained actions. If external links were supported, then people might simply put a external link and no other action. This feels like an anti-pattern for what actions are.
I feel fairly strongly that external linking should be considered a terminal action, like the current completed state.
Mostly agree, however there still might be cases when “external links” useful in any GET metadata, including the initial one. For instance developers might need to have external link to the text document that doesn’t fit the blink description, e.g. governance proposal text or ToS. So, imo what you are describing sounds like a common sense and best practice to me, but it should not be a strong constraint in specification.

[nickfrosty]: my post here in that proposal was meant to funnel the conversation into this one, since they are very closely related (at least for implementation)
I also didn’t notice that you’re hesitant about the External Linking proposal.
Not hesitant, I’m generally for it. Just trying to work out the finer details for it. I think the spec supporting external linking in some way is very useful.
Implementing extra POST endpoint &amp; making extra network call to implement external link feels a bit redundant to me
why would there be an extra POST endpoint?
Mostly agree, however there still might be cases when “external links” useful in any GET metadata, including the initial one. For instance developers might need to have external link to the text document that doesn’t fit the blink description, e.g. governance proposal text or ToS. So, imo what you are describing sounds like a common sense and best practice to me, but it should not be a strong constraint in specification.
I think a url like this for a governance proposal doc could and should be linked in the description (with the blink client making it clickable), not as a primary action button with the rest of the actions. Unless it is a terminal/completed action of some sort.
Having an “action button” that is an external link that fits into the form ui feels wrong and out of place to me. It takes the user away from the blink vice interacting directly with it.
The only time it feels right is for completed states: instead of a generic “completed button” the blink would render an “external link” in place of the completed button.

[tsmbl]: nickfrosty:
why would there be an extra POST endpoint?
External linking sRFC proposes to add type attribute to LinkedAction, author proposes that it will be used by client to determine whether to open the link or making POST network calls.
At the same time, based on your comments
 nickfrosty:
Right now, no matter what, the linked action will always make a POST request to the href .
I do not think LinkedActions should have a type at all with any of the current sRFCs or any specific features I foresee in the future.
Adding a type field in the POST response is what would help determine how the response is handled (i.e. is this a transaction, sign message request, external link, etc)
Following this, if I understand correctly, your proposal is to keep LinkedAction untouched. Meaning, client will make POST request that will return a response of external-link type. This means that as an action developer I need to implement extra endpoint, rather than just indicating the type in LinkedAction.
 nickfrosty:
I think a url like this for a governance proposal doc could and should be linked in the description (with the blink client making it clickable), not as a primary action button with the rest of the actions. Unless it is a terminal/completed action of some sort.
Ideally you should be able to do both. Sometimes links are very long, buttons provide better UX in many cases - let’s consider telegram bots, where you’re free to decide how to encode link.
 nickfrosty:
Having an “action button” that is an external link that fits into the form ui feels wrong and out of place to me. It takes the user away from the blink vice interacting directly with it.
The only time it feels right is for completed states: instead of a generic “completed button” the blink would render an “external link” in place of the completed button.
Hm, not sure. Imo, blinks are not limited to form UI. Developers may do variety of use-cases and even in form UI you may need to include links to external resources. I would advocate to be more open and flexible closer to Frames / Telegram bots experience where developers are free to decide what buttons and where do they need, rather than adding constraints.
Again, I agree with your thoughts, but it feels more like a best practice &amp; common sense to me and should not be spec-level constraint.

[nze]: Hey all, thanks for moving this forward. Sorry if I’m a bit late to the discussion.
Frankly speaking I don’t see any disadvantages in adding type in the LinkedAction, looks very simple and clear solution which is commonly used in general in the programming. And knowing it in advance would allow for blinks to render different buttons differently, for example add a link icon for buttons that’s supposed to redirect user.
And in terms of limitations it’s def better to not to limit external link button to terminal state only. I believe developers can decide on their own where and when they want to redirect their users. I would really feel bad for not allowing it, because it seems like a baseline for me.
I would really recommend to look into telegram bots api i think it’s well designed and well tested on hundred thousand of use cases(which are not yet possible in blinks btw, but would really love to make blinks as power)
 
 
 core.telegram.org
 
 
 
Telegram Bot API
 
The Bot API is an HTTP-based interface created for developers keen on building bots for Telegram.
To learn how to create…
 
 
 
 
 
 
image750×576 63.2 KB

[0xaryan]: Hey @tsmbl / @nickfrosty, what can we do to move this sRFC forward form here?

[nickfrosty]: sounds like I am the only one that had a concern or other desires from your proposal @0xaryan, so I can void those right now and I think we can move forward with saying this SRFC is “approved”
to clarify what changes are being proposed after all the conversation above:
make the transaction optional in the type (blink clients need to handle this flow)
add a type to LinkedAction with the following to start:
transaction - if the linked action should include a transaction
post - if the LinkedAction is effectively going to just make a post request, with nothing for the user to sign
external-link - originally from this SRFC (we could add this in via the same implementation)
sign message - in the future with the sign message SRFC
update ActionPostResponse to add and handle the same type described above
no restrictions on where these external links can be presented to the user
did I miss anything?
cc @tsmbl @nze

[tsmbl]: Thanks for the summary, @nickfrosty. This looks good to me, I believe we can work out the remaining details during implementation, if necessary

[nickfrosty]: this spec update is now live: v2.3
 
 github.com/solana-developers/solana-actions
 
 
 
 
 
 
 
 
 [spec] v2.3 - Optional Transaction In Action Chaining and External Link Support
 
 
 solana-developers:main ← thearyanag:spec-optional-transaction
 
 
 
 opened 02:44PM - 22 Aug 24 UTC
 
 
 
 
 thearyanag
 
 
 
 
 +803
 -24
 
 
 
 
 
 # TLDR
Currently, Actions can be chained together in successive series, but e…ach action in the chain requires the user to sign a transaction thus creating a bad UX. 
1. A Post Request Type ( Optional Transaction ) has been added to avoid signing the transaction on each action.
2. A External-Link type has been added, to direct the user to an external website.
This would enable developers to 
 - providing better UX by avoiding individual pop-ups for each transaction, with the help of optional transactions.
 - support cases such as adding a `Learn More` or `Manage your Squads` kind of thing, with the help of an external link.
 - Later can be extended to support use cases like sign-message where each flow on the blink client end will be slightly different.
## Rationale
Blinks and actions currently require individual transactions for each action in the chain. If the user wants to move to the next action in the chain, they can't without signing a transaction. Having optional transactions would enable a better UX and open more design space for the developers. 
sRFCs :
1. https://forum.solana.com/t/srfc-32-optional-transactions-in-action-chaining/1971/18
4. https://forum.solana.com/t/blinks-cta-external-linking/2018/10
 
 
 
 
 
 
 
blink client implementation will follow shortly, then wallet support after

",0xaryan,493,11,18,19,2024-08-12T17:12:23.587Z,2024-09-25T16:08:33.145Z,sRFC,6,2024-09-25T16:08:33.145Z
2018,Blinks CTA: External Linking,https://forum.solana.com/t/blinks-cta-external-linking/2018,"CTA Links for Blinks
Letting users define links to external sites
We are looking forward to add CTA before or after somebody sends a transaction. For example, someone could add a “Learn more” button that sends the user to some website, or “Manage your asset” after a minting transaction has been sent that redirects them to a website we host.
Right now the way to external link the user, is to make them click on the image of the blink. Which is not a good UX for a CTA, and is limited to only link.
Implementation: 
Adding a type to the LinkedAction, that denotes if the button should call an action or send the user to the defined website
export interface LinkedAction {
 /** URL endpoint for an action or the URL to redirect the user */
 href: string;
 /** button text rendered to the user */
 label: string;
 /** parameters used to accept user input within an action */
 parameters?: ActionParameter[];
 /** whether is an external|action link, defaults to action */
 type?: string;
}","[nickfrosty]: I think adding some sort of CTA “action type” is a good idea. There is a decent amount of overlap in this post: sRFC 32: Optional Transactions in Action Chaining for the implementation of something like this.
What type of CTAs do you think would be useful? The only one I can really think of is a link that should open a new tab for the user.
For user safety, should there be any restrictions on what websites can be linked to? Or would be displaying the domain below the button be sufficient enough

[c4b4d4]: Will think about which other CTAs could be useful. In the post you linked, @0xaryan mentions: post, sign-message, external-link and tx of course.
About the security issue: I think it would be the same security issue than already trusting the action’s entity with the transaction building, so I believe there’s the same level of trust for these links? Unless they are user-generated.
Taking that in consideration maybe we can have some type of “cors policy” in the host’s /actions.json defining which hosts are trusted.

[0xaryan]: I believe, adding the domain below the button should be sufficient, there should be no restrictions on what sides it can be linked to.
If the website the user is getting redirected to, is a bad actor then Wallets ( blowfish ) can handle that IMO.

[nickfrosty]: yeah, I agree. I think displaying domain should be sufficient. I just wanted to see what others thought

[nickfrosty]: what would adding some cors policy type data to the actions json accomplish?

[tsmbl]: The proposal makes sense to me, I’ve seen several requests and questions for supporting this.
 0xaryan:
I believe, adding the domain below the button should be sufficient
This can work, as an idea we can visually indicate external link by a special icon and/or may be even show a warning before opening external link. I personally think we should ask for recommendations from UX/UI experts during client implementation and polish this.
 c4b4d4:
I think it would be the same security issue than already trusting the action’s entity with the transaction building, so I believe there’s the same level of trust for these links
This framing looks right to me

[c4b4d4]: In cases that the links are user-generated. It would be a way for all to agree that an entity would only and only allow X domains.
NFTs have an external url field where creators can put a website there, a Blink could add a button that links to this website. The server’s entity could filter malicious ones, but wanted to extend an idea to have a public list that anyone could read, regarding the security concern.
Does it make sense? Could also be too extra and not worth the maintenance of it on the long term, exhausting protocols with too much stuff is not cool.

[Damien]: I would personally love to see this. There’s a lot of use cases where action buttons which link to external sites would be useful.
Farcaster frames had this since the beginning and a lot of the frame actions were actually linking to external sites, most of them were ones which required some other external authorization/signup before executing a transaction, therefore it had to be done on the third-party website instead of directly through the Frame.
Other use cases were already mentioned and also come in very handy, such as having a link to a page where users are presented with additional information (think “Learn more” button).
The implementation makes sense, I think for now it’s enough to have a LinkedAction which can either be one that triggers an endpoint call (action) or is just a link that opens a new tab in the browser.

[nickfrosty]: to progress the proposal along, I want to vocalize that I am for this proposal.
for the specific implementation, I think it can and should be easily included into the same implementation effort as SRFC 32
effectively making “external links” a different type of action. some actions will be transactions, some will be external links, etc.
the one caveat I think the spec update SHOULD include is that blink clients should at least display the domain of the link to the user somehow. eg: under the button or something. the design specifics can be up to each blink client
cc @c4b4d4 @tsmbl @nze

[tsmbl]: I support this proposal as well and agree that it can be easily integrated into the same implementation effort as SRFC 32. I think we are ready to move forward with the implementation, and any additional details can be addressed as they arise during the process.

[nickfrosty]: this spec update is now live: v2.3
 
 github.com/solana-developers/solana-actions
 
 
 
 
 
 
 
 
 [spec] v2.3 - Optional Transaction In Action Chaining and External Link Support
 
 
 solana-developers:main ← thearyanag:spec-optional-transaction
 
 
 
 opened 02:44PM - 22 Aug 24 UTC
 
 
 
 
 thearyanag
 
 
 
 
 +803
 -24
 
 
 
 
 
 # TLDR
Currently, Actions can be chained together in successive series, but e…ach action in the chain requires the user to sign a transaction thus creating a bad UX. 
1. A Post Request Type ( Optional Transaction ) has been added to avoid signing the transaction on each action.
2. A External-Link type has been added, to direct the user to an external website.
This would enable developers to 
 - providing better UX by avoiding individual pop-ups for each transaction, with the help of optional transactions.
 - support cases such as adding a `Learn More` or `Manage your Squads` kind of thing, with the help of an external link.
 - Later can be extended to support use cases like sign-message where each flow on the blink client end will be slightly different.
## Rationale
Blinks and actions currently require individual transactions for each action in the chain. If the user wants to move to the next action in the chain, they can't without signing a transaction. Having optional transactions would enable a better UX and open more design space for the developers. 
sRFCs :
1. https://forum.solana.com/t/srfc-32-optional-transactions-in-action-chaining/1971/18
4. https://forum.solana.com/t/blinks-cta-external-linking/2018/10
 
 
 
 
 
 
 
blink client implementation will follow shortly, then wallet support after

",c4b4d4,360,6,11,12,2024-08-20T20:45:26.628Z,2024-09-25T16:08:21.689Z,sRFC,6,2024-09-25T16:08:21.689Z
2051,Blinks Language Localization,https://forum.solana.com/t/blinks-language-localization/2051,"Blinks Language Localization
Adding Language code to requests for localizing texts
Localizing texts to the user’s preferd language gives a better ux, plus it facilitates onboarding non-english speakers.
Since the text of stuff is rendered on the server side, there are two ways we could have it done:
The client tells the server what languages does the user’s computer has
The server sends different JSON paths for every language it supports, and the client picks the one that matches the user’s browser setting
Addressing possible implementations for each:
1. Client tells the server the user’s languages
Adding a query parameter to the URL with the user’s language code.
https://host.com/blink/action?locale=en,es,fr
Which languages are available on JavaScript thru:
navigator.languages
//[""en-US"", ""en"", ""fr""]
//or fallback, if navigator.languages is not available in the users browser
navigator.language
//""en-US""
2. Server sends multiple JSONs
For this option, it could be done in multiple ways:
a) Passing an object on keys that can be localized, where the key of every entry is the language code and the value the localized text.
{
 ...
 ""label"" : { ""es"": ""Coleccionar gratis"", ""en"": ""Collect for free"" },
 ...
}
b) Having the entire JSON be in different keys, easier to maintain but it would not make sense for ** ActionPostResponse** that also have the transaction data, since you would have it repeated in both localized JSONs.
{
 ""en"":
 {
 ...
 ""label"" : ""Collect for free"",
 ...
 },
 ""es"":
 {
 ...
 ""label"" : ""Coleccionar gratis"",
 ...
 },
}
c) Having some sort of dictionary tags and a dictionary that has all the meanings of the tags, could be too much but adding it to the discussion.
{
 ...
 ""label"" : ""collect_for_free"",
 ...
}
Dictionary would look something like this:
{
 ""en"" : {""collect_for_free"":""Collect for free""}
 ""es"" : {""collect_for_free"":""Coleccionar gratis""}
}
What are your thoughts? Solana is very global, we don’t want to have Blinks be in english-only all the time, do we? 
Edit:
I think it should also apply to images, since in most cases we place text on images too. So letting define an URL or different URLs for images would be part of localizing it.","[wagg]: Text Localization would be a great optional addition for developers to add to their GET and POST responses.
Browsers and mobile devices already do a decent job of providing the current session’s language preferences on request, and individual extensions by wallets and other integrators could allow a user to set a default language preference for blinks that would override the browser/mobile preference if it exists.
Handling the default as English unless provided gives full backwards compatibility with the current solution, and providing the text in all languages (that dev has setup for this blink) handles scenarios where the user may want to toggle between languages on the UI without a full data refetch from server.
Making this a queryable parameter forces the spec to start reserving keywords or parts of the path, complications that would be good to avoid in the implementation spec. Instead I would advocate for returning all language definitions as defined by the developer, as an additional object in the fetchResponse or postResponse for each language localization. This would give frontends more control over how they choose which version to render by default and allow for smooth language handling in scenarios where preferred language isn’t defined by the developer.
It would be ideal to lean towards best practices created by communities such as the one around i18n or other internationalization frameworks, as they have already worked through the issues we would encounter. Examples of solved problems include text-within-images, variable text length in other languages causing wrapping issues, and extended character libraries causing broken text rendering if fallbacks aren’t supported. Ideally this upgrade is championed by someone who has already implemented localization at scale.

[nickfrosty]: this is a great idea in general: enabling localization.
I do think a simple approach could be used with headers.
blink clients could set and send the AcceptLanguage header with the locale of the user
the action api can read/parse this header value and provide the appropriate localized strings
if they dont care about localization, they can still just return action metadata as they do now
this would also simplify implementation and upkeep on part of blink clients since blink clients simply display the strings as they are returned. it becomes up to the action api to return the desired localized strings or image urls
related: we could also support color modes of users too (light mode or dark mode) in a similar way using headers sent from the client

",c4b4d4,127,0,2,3,2024-09-03T00:41:49.003Z,2024-09-11T18:41:01.069Z,sRFC,6,2024-09-11T18:41:01.069Z
1661,sRFC 23 – Field Authority Interface (for Token Metadata),https://forum.solana.com/t/srfc-23-field-authority-interface-for-token-metadata/1661,"sRFC 23 – Field Authority Interface (for Token Metadata)
Summary
An interface that works alongside the Token Metadata Interface (sRFC 00017) which provides field-based authority to additional public keys.
Problem
The Token Metadata Interface outlines a basic standard which programs can implement for updating and retrieving fungible and non-fungible token metadata. The interface specifies a single update authority for all write operations. This works for basic tokens and NFTs.
But if you want to do something more advanced, such as allowing token holders to edit the token’s metadata, you have to either give them access to all fields or set the update authority to a PDA and carry out the logic in a custom program. The former doesn’t work because core fields could be edited like name and uri which would disrupt marketplaces and dapps. The latter “closes” the interface and takes us back to closed-program territory.
Solution
We propose the “Field Authority Interface,"" a way to specify additional public keys as authorities on specific metadata fields. The Field Authority Interface works alongside the Token Metadata Interface; it lives in the same program and writes to the same metadata account. The update authority from the Token Metadata Interface can add and remove Field Authorities. Field Authorities can be System Program keys or PDAs that “plug in” and implement custom logic – but this time they don’t close the interface.
field-authority-interface-diagram1950×1489 140 KB
Implementation / Live Example
At Garden Labs, we released a PFP collection called AI Aliens which implements the Field Authority Interface along with a Holder Metadata Plugin. We open sourced the code and provided a detailed write-up. The website and tweet thread focus on the holder metadata functionality, but it uses the Field Authority Interface under the hood. The implementation is fairly simple: adding a Field Authority creates a PDA with data, updating metadata via a Field Authority requires providing this PDA and a signature from the authority, and removing the Field Authority closes the PDA.
Limitations
This implementation only allows for one Field Authority per field. It also uses PDAs which perhaps limits discoverability. We want to keep the implementation simple for now while the Token Metadata Interface gets adopted / understood.
Further Thoughts
With the Field Authority Interface, most additional metadata functionality should be covered. Custom logic can be implemented via external programs that plug in. That said, as the ecosystem matures, it may make sense to turn some of these plugins like Holder Metadata into interfaces themselves.","[joncinque]: This is a really neat idea! Keeping the two interfaces in the same program simplifies a lot of the design.
My main idea for a multi-authority approach required the main authority to be set to a PDA on another program. This other program would handle the multiple authorities. The tradeoff is that you don’t need a bespoke program that implements two interfaces – instead, anyone can use the two deployed programs directly. On the flipside, the multiple levels of programs can be much more cumbersome to use unfortunately.
All in all, this is a great idea and a lovely use of program interfaces!

[jacksondoherty]: Thank you!
I originally considered the PDA approach but felt the cumbersome nature you’re talking about – for example, an NFT editing tool / website that uses the interface would present the wrong transactions. Maybe it could see that the authority is a PDA and know to use the PDAs program, but it would then have to know that program’s instructions and… we sort of end where we started, a closed Metaplex-like program.
A lighter approach to what I’ve proposed might be to just add a second optional authority to the Token Metadata Interface that could be for PDAs, etc.
At the same time, I think one of the benefits of the field-based approach is that you can have multiple of these secondary programs “plug in.” I think this architecture lends itself better to libraries of plugins that do different types of auth-ing – holder-based, multi-sig, additional token-gating, etc. etc. It looks like Metaplex might have independently converged on this idea with their plugins and authority types in Core.

[Gabynto]: The Field Authority Interface seems like a smart solution to expand metadata control without complicating the Token Metadata Interface. I would love to see how this can enable more flexibility and advanced use cases for NFTs and tokens.

[jacksondoherty]: I’ve implemented a V2 which removes the PDAs and uses a TLV-based approach. A list of field authorities are now placed inside the same account as the metadata via a second TLV struct. This allows for much easier composability, no longer having to pass extra accounts (code).
This also matches the UpdateField parameters types and accounts with Token Metadata Interface’s. @joncinque perhaps the Token Metadata Interface’s spec could loosen what is meant by update_authority in the UpdateField function? With TLVs it seems we can actually handle a much broader set of functionality / make this a bit more powerful without having to change any code.

",jacksondoherty,515,1,4,5,2024-06-12T00:37:31.068Z,2024-08-20T23:57:04.292Z,sRFC,6,2024-08-20T23:57:04.292Z
1734,sRFC 28: Blinks Chaining,https://forum.solana.com/t/srfc-28-blinks-chaining/1734,"Blinks as they exist now have a single depth to interactions. You fetch the entrypoint of the blink with a GET request, then use one of the button actions to make a POST request to the server to fetch a transaction with the user’s account info.
This single depth prevents good error handling, and can be easily expanded to allow for blink chaining by just reusing the ActionGetResult for the POST request (with an optional transaction field) and rerendering the blink.
This allows branching logic on the part of the blink, where actions can be shown specific to the user’s account, and even multiple transactions could be carried out. This also allows for better error messages and error handling, giving more info to the end user.
So what does this look like?
Currently ActionGetRequest looks like this:
export interface ActionGetResponse {
 /** url of some descriptive image for the action */
 icon: string;
 /** title of the action */
 title: string;
 /** brief description of the action */
 description: string;
 /** text to be rendered on the action button */
 label: string;
 /** optional state for disabling the action button(s) */
 disabled?: boolean;
 /** optional list of related Actions */
 links?: {
 actions: LinkedAction[];
 };
 /** optional (non-fatal) error message */
 error?: ActionError;
}
we can expand it to add:
export interface ActionUnifiedResponse {
 /** url of some descriptive image for the action */
 icon: string;
 /** title of the action */
 title: string;
 /** brief description of the action */
 description: string;
 /** text to be rendered on the action button */
 label: string;
 /** optional state for disabling the action button(s) */
 disabled?: boolean;
 /** optional list of related Actions */
 links?: {
 actions: LinkedAction[];
 };
 /** optional (non-fatal) error message */
 error?: ActionError;
 /** optional b64 encoded transaction */
 transaction?: string
}
This would get rid of the ActionPostResponse completely, and just use this unified response for all blinks.
So what would this look like?
User fetches the entrypoint to the blink with standard GET request and gets a list of actions
User clicks on an action, which POST requests with their account to the given action URL
This returns a new ActionUnifiedResponse with an optional transaction they can sign and rerender of potential links or errors the server had in case the account + path from previous request resulted in a transaction that didn’t work (like if that account was specifically timed out from interacting with that path).
Loop until done
Notes
Does chaining mean that servers have to hold state?
Not necessarily, all state can be URL encoded as path params
What are some other cool things this allows?
Right now one of the biggest challenges is blink generation unique to user has to be done off platform :- if you want users to have their own blinks they have to go to telegram or discord or a website or something to generate a blink cause twitter api access is trash. With this you could give out new blinks to uses on twitter itself","[pirosb3]: Hi @spacemandev this is a great idea, and definitely something I’m interested in.
For a secure authenticated session to take place, there needs to still be some form of transaction or signature. In the current implementation of Blinks only transactions are supported (looks like signatures are coming soon) - and it seems that once a transaction is sent, the blink reaches a final state. I would love to see this proposal also enable multiple subsequent states after a transaction takes place

[spacemandev]: Actually, this implementation would support secure sessions. After the first transaction (which can be a memo ix or transfer of lamport from and to the user), you can give a JWT to the user encoded in the query parameters.
This allows you do to a multi state system with a secure session

[nickfrosty]: I like this general idea of “blink chaining”. Support for chaining the Actions API responses in a sequential way like this could open some interesting designs and applications for actions/blinks.
Aside: I think technically it should be title “actions chaining” since the blink is just the renderer and not always required to be used. The actions are being chained and can be client agnostic.
Questions:
How many actions could be chained together? No limit or some limit?
Having no limit might be nice for some use cases, but I suspect it will lead to a lot of user drop of while submitting actions and signing transactions. Chaining alone might lead to some users getting confused when the UI gets updated on the subsequent chained action if the messaging in the previous aciton is unclear (I guess these are all general UI design best practices and not necessary actions specific though)
How would the action-aware client (like a blink) know to stop requesting for subsequent chained actions? Is it simply if the proposed UnifiedResponse has no links.actions declared?
In the current actions spec, if the GET response has no links.actions is not provided, then a single button is expected to be rendered in the client and the POST request be made to the same url as the GET request (aka backwards compatible with Solana Pay transaction requests). However, if links.actions does exist, then the POST request is made to the corresponding href value for the action a user submits via button click.
What should happen if the action api returns a transaction and the rest of the metadata to support chaining an additional action? Should the user be promoted to sign the transaction before rendering the new chained actions? Should it be before?
Why remove the ActionPostResponse in favor of your proposed ActionUnifiedResponse that seems to only add the optional transaction field?
Even in your proposal, it seems like after the very first GET response to collect the initial metadata and available actions, you are proposing to continue to make POST requests. Why not just add the optional transaction field to the ActionPostResponse interface? If this transaction field exists, it is the action api implicitly saying “I am performing action chaining”
With the idea of action chaining via your proposed “unified response” interface, should the error field become fatal? Halting the chain of events? Or should the only error drive fatal halting be when a proper http error coded response is returned from the action api?

[nickfrosty]: Also, with a goal to maintains backwards compatability between action-aware clients that support action chaining and those that do not, do you have concerns or thoughts on how an action api should react if the client does not perform the chaining?
With another change to the spec for action-aware clients somehow declaring “what features they support” (which I dislike the idea of), there is no way for an action api to know if their request to chain multiple actions will actually be facilitated by the client UI

[spacemandev]: How many actions could be chained together?
With the session based model on actions, it’d be up to the app developer to make sure they aren’t making their sessions so long that users drop off and have good handle on recovery
How would the action-aware client (like a blink) know to stop requesting for subsequent chained actions? Is it simply if the proposed UnifiedResponse has no links.actions declared?
Yes, you could still have ActionGet flow be the kick off flow to remain backwards compatibility and have no links.actions on that. You “end” the session when the user is done pressing buttons, not when there’s no links.actions left. I’m assuming the final branch of a flow would just return disabled is true to end the session
What should happen if the action api returns a transaction and the rest of the metadata to support chaining an additional action? Should the user be promoted to sign the transaction before rendering the new chained actions? Should it be after?
Definitely before. As a follow on, it’s also important that there’s a way to fetch the txn signature from the client and then have it in the context of the following post request so the backend can confirm it
Why remove the ActionPostResponse in favor of your proposed ActionUnifiedResponse that seems to only add the optional transaction field?
You’re confusing ActionPostResponse with ActionGetResponse. ActionPostResponse in the current spec only has transaction and message fields. Specifically the reason you wouldn’t want to add transaction field to ActionGetResponse is because passive rendering of an action (such as through a blink on twitter) should not popup a client transaction. It’d be pretty bad UX if you were scrolling on twitter and your wallet kept popping up. Only after the first GET should there be an option of POST requests with transactions
With the idea of action chaining via your proposed “unified response” interface, should the error field become fatal? Halting the chain of events? Or should the only error drive fatal halting be when a proper http error coded response is returned from the action api?
I don’t think the error field should become fatal. It can be used for a “try again” by users for example when the transaction cannot be confirmed or in case of other app logic.
In terms of backwards compatibility, this is a tough one. You could add a client version header on the initial GET request maybe?

[nickfrosty]: Yes, you could still have ActionGet flow be the kick off flow to remain backwards compatibility and have no links.actions on that. You “end” the session when the user is done pressing buttons, not when there’s no links.actions left. I’m assuming the final branch of a flow would just return disabled is true to end the session
Makes sense. I think using disabled to end the chaining session would be the easiest implementation for sure. For some reason, by brain does like it and it seems like not the best dev experience, but I cannot articulate why…
Definitely before. As a follow on, it’s also important that there’s a way to fetch the txn signature from the client and then have it in the context of the following post request so the backend can confirm it
A few people have suggested wanting some sort of “callback” functionality to verify the signature on their server side. With action chaining, it makes sense to desire this too. But on the flip side, since the transaction id is being provided by the client, it can easily be spoofed.
You’re confusing ActionPostResponse with ActionGetResponse. ActionPostResponse in the current spec only has transaction and message fields. Specifically the reason you wouldn’t want to add transaction field to ActionGetResponse is because passive rendering of an action (such as through a blink on twitter) should not popup a client transaction. It’d be pretty bad UX if you were scrolling on twitter and your wallet kept popping up. Only after the first GET should there be an option of POST requests with transactions
Ohh you are right. I misthough / mistyped. I meant in the PostResponse to add the same fields as the GetResponse. Updating the interface to be this is what I meant to suggest:
/**
 * Response body payload returned from the Action POST Request
 */
export interface ActionPostResponse extends ActionGetResponse {
 /** base64 encoded serialized transaction */
 transaction: string;
 /** describes the nature of the transaction */
 message?: string;
}
Specifically the reason you wouldn’t want to add transaction field to ActionGetResponse is because passive rendering of an action (such as through a blink on twitter) should not popup a client transaction. It’d be pretty bad UX if you were scrolling on twitter and your wallet kept popping up.
Totally agree lol. This is also why the initial GET request has no body payload since if it did, it would in theory send the user’s wallet address to every blink on the page. Bad experience and removes the user’s ability to interact or not interact with a specific blink.
I don’t think the error field should become fatal. It can be used for a “try again” by users for example when the transaction cannot be confirmed or in case of other app logic.
Makes sense, but the current non-fatal error does not follow this “try again” flow really.

[0xaryan]: Currently the ActionPostResponse looks like
/**
 * Response body payload returned from the Action POST Request
 */
export interface ActionPostResponse {
 /** base64 encoded serialized transaction */
 transaction: string;
 /** describes the nature of the transaction */
 message?: string;
When the user signs the transaction, the action server has no option to verify/know if the user has signed the transaction, therefore the server needs to scan all the transactions for the given programId and check if the user has made the transaction or not. Scanning all the transactions is not a feasible approach, which can be replaced by a callback URL , the callback URL would accept a signature ( signed by the user ) and the account ( base58-encoded representation of the public key of the user ) in the body to link the transaction.
/**
 * Response body payload returned from the Action POST Request
 */
export interface ActionPostResponse {
 /** base64 encoded serialized transaction */
 transaction: string;
 /** describes the nature of the transaction */
 message?: string;
 /** callback URL to be called after the transaction is confirmed */
 callback?: string;
}
After the user has signed the transaction, then the blink-client would use the callback URL field to send a HTTP OK JSON with the following payload
/**
* Response body payload returned from the Callback Post Request
*/
export interface CallbackPostResponse {
 /** user signature */
 signature: string;
 /** base58-encoded representation of the public key of the user */
 account: string;
}

[nickfrosty]: with regards to @spacemandev’s proposal for action chaining, how does your callback idea fit into it?
dev’s proposal suggests the post response would return the optimistic UI items that should be rendered after the previous transaction is successful, allowing 1 less network request and the user to immediately interact with the next action in the chain (as if it was a freshly rendered blink)
for your callback proposal, is the purpose to simply tell the action api server that the transaction was successful or to get the next action in the chain only after it was successful? or something else?

[0xaryan]: For the callback proposal, the idea was simply to tell the action API server that the transaction was successful so that the API server can do any event linked to the user success tx.
Now that I think of it,
In addition to the @spacemandev proposal of having the optimistic UI items in the post-response, the post-response should have 3 options:-
success optimistic ui - to be rendered after the previous transaction was successful
failed optimistic ui ( optional ) - to be rendered after the previous transaction failed. [ this would help in cases where the rendered blink is the last in the chain ]
callback URL - to enable a confirmation of the transaction to the action server, would remove the overhead on the action server to scan all the transactions related to that programId and also encourage the action developers to use third-party programs which otherwise would have a lot of on-chain transaction ( for example - a jupiter dca )
This can now open some interesting design space for the developers and creating better UX ( instead of having 5-6 input field in 1 rendered blink, it can be distributed among 2/3 steps )

[Gabynto]: Wow, lots of good stuff here. I am learning a lot about blinks chaining

[nickfrosty]: After chatting more with @spacemandev some on this, we are leaning towards this to both enable support for action chaining and provide the callback functionality. This will both prevent the new feature from being a breaking change and will play nice will other open spec change proposals and expected future ones (like message signing).
Note: The interfaces listed below may be simplified version of their final implementations. To improve the type safety and DX, the specific names and types may be adjusted but will accomplish the same functionality.
Proposal
Update the ActionPostResponse to allow passing the a url to discover the next action in the chain (via callback) or include the metadata for the next action in the action (without making a callback):
/**
 * Response body payload returned from the Action POST Request
 */
export interface ActionPostResponse {
 /** base64 encoded serialized transaction */
 transaction: string;
 /** describes the nature of the transaction */
 message?: string;
 /** support action chaining */
 links?: {
 /**
 * - when `next` is type=`string` aka url =&gt; make a POST to this address with a payload of `NextActionPostRequest` to retrieve the next action
 * - note: this url is required to be same origin as the POST request returning this response
 * - when `next` is type=`ActionGetResponse` =&gt; after transaction is confirmed, render this data
 */
 next: string | ActionGetResponse;
 };
}
export interface NextActionPostRequest extends ActionPostRequest {
 /** signature produced from the previous action (either a transaction id or message signature) */
 signature: string;
}
Explanation
When any ActionPostResponse include the links.next attribute, an action chain is created/continued.
The links.next can either be a string url or the blink metadata (ActionGetResponse) that will trigger the chaining after the provided transaction is confirmed.
If the links.next is a string url, it is required to be from the same origin that creates the ActionPostResponse. After the transaction is confirmed, the blink-client should make a POST request to the provided links.next url with the user’s wallet address (account) and confirmed transaction id (signature) in the POST body as JSON.
Think: on transaction confirmed =&gt; perform callback (with the transaction id) to get the next action in the chain
If the links.next is an object of ActionGetResponse, after the transaction is confirmed, this blink metadata should be rendered to the user (effectively making it appear as if a new blink) and no callback to the Actions API is made.
Think: on transaction confirmed =&gt; render this blink metadata, I don’t need to confirm the transaction on my backend
If an Action provider needs to track any state between their chained actions, they must handle/validate that themselves. For example, using a query param to track the user’s current “step in the action chain”.
When an Action provider wants to stop the action chain, they can return the final action as one with a disabled=true, which will stop the user from continuing via the blink UI.
An action chain can effectively be any length as long as the user continues to sign and confirm transactions and the action api continue to return signable transactions.

[tsmbl]: @nickfrosty, this is great update! I like the idea of links.next, it looks consistent and explicit.
 nickfrosty:
When an Action provider wants to stop the action chain, they can return the final action as one with a disabled=true , which will stop the user from continuing via the blink UI.
This will definitely work and allow to stop the chain. There is one case that came to my mind related to existing client behaviour. Currently blinks have a Completed state indicating the success of execution.
image1044×390 31.9 KB
As a developer I would like to preserve this behaviour and be able to clearly indicate the Completed state in the end of the chain. I believe setting disabled=true is not sufficient to cover this case.
As an idea, we could indicate Completed state explicitly, I see at least 2 options
a) Support 2 types in Chained POST Response to explicitly indicate terminal state, for example
export type NextActionResponse = Action | CompletedAction;
/** A response indicating next action to be displayed to a user */
export interface Action extends ActionGetResponse {
 type: 'action';
}
/** A response indicating that terminal action state reached */
export interface CompletedAction {
 type: 'completed-action';
}
b) Somehow indicate Completed state in the GET Response, could be something like
completed=true
variant='completed'
 nickfrosty:
If the links.next is an object of ActionGetResponse , after the transaction is confirmed, this blink metadata should be rendered to the user (effectively making it appear as if a new blink) and no callback to the Actions API is made.
I think we can simplify a bit by keeping a single option for chaining - using the POST request. Rationale:
Chaining via POST request already covers all functional cases and provides broader functionality comparing to chaining via ActionGetResponse.
Having fewer options is better for end developer experience.
In my opinion the only potential benefit of chaining using ActionGetResponse is saving one network call, but the performance benefits are not clear to me in this case. So, not sure if ActionGetResponse case needs to be specially covered.

[nickfrosty]: Currently blinks have a Completed state indicating the success of execution.
This is all in the blink-client though right? This same flow would be preserved if the final action in the chain simply does not return links.next, effectively just like now.
If we consider the current actions have a “action chain length of 1”, the “completed” state that the client displays shows after the transaction is confirmed and there is not links.next (since the spec does not exist yet). If there are more actions in the chain the user can continue to execute them, until when there is no longer any “next actions” set, then the blink-client can render this completed state.
Support 2 types in Chained POST Response to explicitly indicate terminal state,
I do think having a way for action api’s to explicitly declare the final action in the chain is a good idea. Adding a type field to the GetResponse (which we should likely rename now lol) I think is the best way to declare this. Add in type with a default of action for backwards compatibility.
Your proposed CompletedAction has a drawback of not allowing the action api to update the metadata at the very end of the chain which will be very useful (and I suspect will be very commonly used).
I think we should make sure to support this case, and be able to smartly handle both cases of the final actions wanting to update the metadata AND final actions not wanting to update the metadata.
In my opinion the only potential benefit of chaining using ActionGetResponse is saving one network call, but the performance benefits are not clear to me in this case. So, not sure if ActionGetResponse case needs to be specially covered.
Saving one network call is still useful imo. But the other benefit that I see is for showing the final “completed” state’s metdata at the end of the chain. After the final action is confirmed, the client already has the metadata to display and update the UI with what ever “completed metadata” the action api wants the user to see.
For example, if a blink is used to mint an nft, the api can return the nft artwork and other metadata when they send the transaction to the user. After the transaction is confirmed, show the nft the user just minted

[tsmbl]: nickfrosty:
This is all in the blink-client though right?
Yes, it’s about current blink-client behaviour, that is possible because client assumes that there’s a single action.
 nickfrosty:
If there are more actions in the chain the user can continue to execute them, until when there is no longer any “next actions” set, then the blink-client can render this completed state.
I think there is a scenario when this criteria is not sufficient: when the final action in chain requires submission of signature. Let’s say as an action developer I would like like to submit tx or message signature to actions API as a final step, no further user activity is required afterwards, so I expect Completed state to be rendered. Based on current the proposal, links.next is a part of POST response, but linking always returns ActionGetResponse - this response will be rendered instead of Completed state.
 nickfrosty:
I think we should make sure to support this case, and be able to smartly handle both cases of the final actions wanting to update the metadata AND final actions not wanting to update the metadata.
Yes, agreed here. So, option (a), as formulated above, doesn’t fit in this framing. But I still believe we need an explicit Completed state indication somewhere to achieve this.
 nickfrosty:
Adding a type field to the GetResponse (which we should likely rename now lol) I think is the best way to declare this. Add in type with a default of action for backwards compatibility.
This sounds close to option (b) from my message above. Can you elaborate on adding type to ActionGetResponse? Do you expect to have separate data structures for each type, or type should serve as a Completed state indicator?
Yes, def should rename, lol.
 nickfrosty:
Saving one network call is still useful imo. But the other benefit that I see is for showing the final “completed” state’s metdata at the end of the chain. After the final action is confirmed, the client already has the metadata to display and update the UI with what ever “completed metadata” the action api wants the user to see.
From my perspective, optimizing to save a network call at this stage might be a bit premature. Using POST as a single mechanic can achieve the same UX, for example same approach has been successfully implemented and proven to work in Farcaster frames. Generally, I think it’s simpler and more consistent to have a single option if we don’t sacrifice flexibility or feature completeness.

[nickfrosty]: This sounds close to option (b) from my message above. Can you elaborate on adding type to ActionGetResponse ? Do you expect to have separate data structures for each type , or type should serve as a Completed state indicator?
Let’s say we update the spec types/interfaces to this:
/**
 * A single Solana Action
 */
export interface Action {
 /**
 * @default `action`
 */
 type?: ""action"" | ""completed"";
 /** image url that represents the source of the action request */
 icon: string;
 /** describes the source of the action request */
 title: string;
 /** brief summary of the action to be performed */
 description: string;
 /** button text rendered to the user */
 label: string;
 /** UI state for the button being rendered to the user */
 disabled?: boolean;
 /** */
 links?: {
 /** list of related Actions a user could perform */
 actions: LinkedAction[];
 };
 /** non-fatal error message to be displayed to the user */
 error?: ActionError;
}
/**
 * Response body payload returned from the initial Action GET Request
 */
export type ActionGetResponse = Action;
/**
 * Response body payload returned from the Action POST Request
 */
export interface ActionPostResponse {
 /** base64 encoded serialized transaction */
 transaction: string;
 /** describes the nature of the transaction */
 message?: string;
 /** support action chaining */
 links?: {
 /**
 * - when `next` is type=`string` aka url =&gt; make a POST to this address with a payload of `NextActionPostRequest` to retrieve the next action
 * - note: this url is required to be same origin as the POST request returning this response
 * - when `next` is type=`Action` =&gt; after transaction is confirmed, render this data
 */
 next: string | Action;
 };
}
export interface NextActionPostRequest extends ActionPostRequest {
 /** signature produced from the previous action (either a transaction id or message signature) */
 signature: string;
}
The initial GET response returns the metadata to render the blink. When the user clicks a button to begin interacting with an action, it makes the POST request (just as it does now).
If a user does NOT want to chain an action, they do NOT return links.next. After the transaction is confirmed, the blink-client can render the “completed” state. Just as they do now.
Any time ActionPostResponse does NOT return links.next, the blink-client knows this is the final transaction in the chain and can render the completed state after the transaction is confirmed. Just as they do now.
If a user wants to chain an action:
the POST response returns a transaction and includes the links.next which is the callback url (same origin only) to give the api server the tx id and fetch the next action
since the links.next was declared, you know the action chain is not yet in the final “completed” state
after the transaction is confirmed, the blink-client makes a POST request to this links.next url which returns the next action (aka Action aka ActionGetResponse) which declares a type value to determine what the UI should do, including handling the “completed” state
type=action (the default value) =&gt; regular action chain. not the terminal/completed state
type=completed =&gt; terminal state of the action. this allows developers to return updated metadata for the UI to render after the transaction is confirmed. (aka the completed state, but with custom updated metadata)
if this action includes either type=action or does not declare type since it defaults to action, then this response is the next action in the chain and the user can interact with it as if a “fresh action”. it goes through the lifecycle of requests to ultimatly get the ActionPostResponse with another transaction and maybe another chained action via links.next
this loop will continue until either:
no links.next is returned via the ActionPostResponse (implicitly declaring “this is the final action”). the blink-client can then render a standard “completed” ui just like it does now. no additional UI metadata gets updated (like the image, title, description, etc)
or the action api returns type=completed, the blink-client now explicitly knows this is the final action in the chain and can render the “completed” state and also update the metadata displayed to the user (since this payload just gave it to you)
I also think a the type value of completed is maybe not the best word to use. Something closer to the desire to “update the UI with this metadata”.
The type interface for Action should likely be updated to not allow including links when type=completed since this is effectively the terminal state of metadata to render to the user, and no further action exists in the chain.
In my opinion the only potential benefit of chaining using ActionGetResponse is saving one network call, but the performance benefits are not clear to me in this case. So, not sure if ActionGetResponse case needs to be specially covered.
I think it will be very useful for people and improved the developer experience of building blinks. And can be handled by blink-clients with basically a single if statement check.
If you are really hung up on it and so opposed to having it, we can just cut it so we can finalize this sRFC and get action chaining shipped to the public.

[tsmbl]: @nickfrosty, this looks conceptually complete and should work from the request/response flow and information model perspective.
 nickfrosty:
I also think a the type value of completed is maybe not the best word to use. Something closer to the desire to “update the UI with this metadata”.
I think we can figure out the exact semantics for type and polish other aspects such as type safety on later stages during the PR in spec repo, wdyt?
 nickfrosty:
I think it will be very useful for people and improved the developer experience of building blinks. And can be handled by blink-clients with basically a single if statement check.
If you are really hung up on it and so opposed to having it, we can just cut it so we can finalize this sRFC and get action chaining shipped to the public.
Not strongly against conceptually, we can keep it if you still think it’s useful. Let’s just ensure it’s well typed and extensible during implementation.

[nickfrosty]: Agreed on getting the types all working well in the PR.
I will work on opening a PR tomorrow then!

[0xaryan]: I don’t know how I missed this discussion.
The idea of links.next along which includes the signature and POST URL is a banger.

",spacemandev,1173,10,18,19,2024-06-27T13:11:53.528Z,2024-08-04T04:52:52.046Z,sRFC,6,2024-08-04T04:52:52.046Z
1892,sRFC 31: Compatibility of Blinks and Actions,https://forum.solana.com/t/srfc-31-compatibility-of-blinks-and-actions/1892,"sRFC 31: Compatibility of Blinks and Actions
Context &amp; Problem
Actions Spec Version Compatibility
Different Blink clients, such as wallets, update at different rates. This means they might not support the latest features, which can lead to situations where Blink is rendered incorrectly, with elements not being visible or displayed correctly, or being completely non-functional.
Blockchain Compatibility
Currently, the action doesn’t specify which chain to target, other than the recent blockhash being invalid on another Solana network. Also, the community has started building actions for non-Solana chains following the spec. As actions are built for various chains, there is a risk of consuming and unfurling incompatible actions, which can break client functionality.
Proposed Solution
Proposing to include compatibility metadata across Action API responses, specifically
The version of the spec being used.
The chains it supports.
This approach allows Blink clients to decide whether to unfurl the action based on the data provided by the server. The benefits of the approach are
Minimizes breaking changes, including CORS configuration for existing action providers.
Ensures low friction for end action developers by reducing the code required to maintain consistent and expected behaviour across different versions of clients.
To ensure smooth operation, Blink clients should follow the best-practices below:
If the action version is higher than the action spec version used in the client, clients should render a fallback UI indicating that version is incompatible and suggesting to update if possible. This practice encourages wallets to adopt newer spec versions and users to upgrade their clients.
If the client doesn’t support the blockchain, the action should not be unfurled.
Implementation Proposal
Technically the proposal is to introduce compatibility metadata in the Actions API response headers. Headers are suitable for passing such metadata for several reasons:
Consistency: Uniform application across all types of requests, including GET requests where bodies are not applicable.
Clarity: Keeps versioning metadata separate from request data, maintaining a clean separation of concerns.
Efficiency: Headers facilitate making HEAD requests to check compatibility before fetching the entire action, improving performance.
In order to ensure consistent encoding and identification of blockchain networks, the proposal is to utilize chain-agnostic standards that are gaining traction in the industry, specifically CAIPs.
So, two response headers are proposed to be returned by action developers:
X-Action-Version to show what spec version the action API server is using. For example, X-Action-Version: 2.1.3.
X-Blockchain-Ids to list blockchains the action supports, using CAIP-2 compatible strings. For example, X-Blockchain-Ids: solana:5eykt4UsFv8P8NJdTREpY1vzqKqZKvdp for Solana mainnet or X-Blockchain-Ids: solana:EtWTRABZaYq6iMfeYKouRu166VU2xqa1 for Solana devnet. See Solana Namespace - Addresses | Chain Agnostic Namespaces for more details.
For backward compatibility, Blink clients should treat:
The absence of the X-Action-Version header as the last pre-compatibility release.
The absence of the X-Blockchain-Ids header as a Solana mainnet action.
Note: Wallets cannot get custom response headers from browser requests by default. The Action Provider needs to add Access-Control-Expose-Headers: X-Blockchain-Ids, X-Action-Version to the response to make metadata available to scripts running in the browser.
Out of Scope &amp; Future Work
Introducing compatibility metadata in Blink client requests:
Request Header: X-Accept-Action-Version to show the max spec version the Blink client supports.
Request Header: X-Accept-Blockchain-Ids to list blockchains the client supports, using CAIP-2 compatible strings.
This will allow for accurate handling when necessary by relying on client request headers and having extra logic to handle different client versions. While it is possible that action developers might not accurately handle the version provided by the client initially, it could be useful to have this data.
Proposing to include this change in separate sRFC to maintain backward compatibility, as current APIs would break unless existing servers update CORS settings to allow 2 extra CORS headers Access-Control-Allow-Headers: X-Accept-Action-Version, X-Accept-Blockchain-Ids.","[nickfrosty]: Having clear guidelines and best practices around Actions and blinks is super important. Especially as blink clients, like wallets, ship update on different timelines (just like you called out).
With a versioning system like you describe (both the blink client and action server stating which they support), we can ensure the best possible user experience. I am very much for this update! Especially by using standardized headers sent by BOTH blink clients and action API servers.
If the action version is higher than the action spec version used in the client, clients should render a fallback UI indicating that version is incompatible and suggesting to update if possible. This practice encourages wallets to adopt newer spec versions and users to upgrade their clients.
I think for some future features in the spec, their maybe be a reasonable fallback option to give to users instead of simply rendering a fallback UI stating its incompatible. In some cases, sure this will not be possible. But for many types of updates, I suspect having some reasonable fallback functionality should be expected on the client side.
How would a multi-blockchain action declare the chains it supports? Should the X-Blockchain-Ids be a comma separated list? Or something else?
To help with adoption of these headers, I can add helper functions, constants, and types into the @solana/actions sdk and the @solana/actions-spec package

[tsmbl]: Thanks for feedback!
 nickfrosty:
With a versioning system like you describe (both the blink client and action server stating which they support)
I agree that ideally, both the client and server should state what versions and chains they support and this is how I see this generally.
However, I have mixed feelings about the implementation sequencing. Adding compatibility metadata in request headers from the Blink Client will break existing action providers due to incompatible CORS configurations. On the other hand, adding compatibility metadata to Action API response headers is safe and backward compatible.
Therefore, I would propose starting with adding compatibility metadata to Action API response headers, but postponing the implementation of Blink client headers.
What are your thoughts on this concern?
 nickfrosty:
I think for some future features in the spec, their maybe be a reasonable fallback option to give to users instead of simply rendering a fallback UI stating its incompatible. In some cases, sure this will not be possible. But for many types of updates, I suspect having some reasonable fallback functionality should be expected on the client side.
Agreed here
 nickfrosty:
How would a multi-blockchain action declare the chains it supports? Should the X-Blockchain-Ids be a comma separated list? Or something else?
Realistically, I think it will be a single blockchain id in most cases for the nearest future. However, it’s comma separated list for extensibility, several examples of what could be possible in the future
Define actions that support both devnet and mainnet:
X-Blockchain-Ids: solana:5eykt4UsFv8P8NJdTREpY1vzqKqZKvdp, solana:EtWTRABZaYq6iMfeYKouRu166VU2xqa1
Support multiple blockchains in single action (e.g. donation blink that supports both Solana and some other chain)
X-Blockchain-Ids: solana:5eykt4UsFv8P8NJdTREpY1vzqKqZKvdp, eip155:1
That said, if we want full multi-chain experience, I think we will need extra sRFC that will specify several extra aspects of operating in such environment.
 nickfrosty:
To help with adoption of these headers, I can add helper functions, constants, and types into the @solana/actions sdk and the @solana/actions-spec package
This would be awesome! Def will simplify adoption

[Gabynto]: Introducing compatibility metadata in the Action API responses sounds like a smart move to handle version mismatches and blockchain support issues more effectively. It’ll help Blink clients avoid breaking changes and improve overall compatibility with diverse blockchain actions.

[nickfrosty]: tsmbl:
Therefore, I would propose starting with adding compatibility metadata to Action API response headers, but postponing the implementation of Blink client headers.
I think your plans makes the most sense. Straightforward to accomplish.
 tsmbl:
That said, if we want full multi-chain experience, I think we will need extra sRFC that will specify several extra aspects of operating in such environment.
Also makes sense, future sRFC it is.

",tsmbl,438,2,4,5,2024-07-29T15:51:50.978Z,2024-07-31T16:34:13.130Z,sRFC,6,2024-07-31T16:34:13.130Z
1860,sRFC 30: Account Abstraction Interfaces,https://forum.solana.com/t/srfc-30-account-abstraction-interfaces/1860,"Summary
This proposal aims to introduce a standardized interface for Account Abstraction (AA) in the Solana ecosystem. By following this interface, any protocol interested in AA development on Solana can unify the operation object, allowing users to focus on their intended instructions rather than dealing with different AA provider interfaces.
Motivation
Currently, only a few protocols in the Solana ecosystem are developing Account Abstraction (AA) solutions, each with varying interfaces. This early stage presents an ideal opportunity to create a unified interface for AA operations. A standardized interface will simplify integration, reduce fragmentation, and enhance the overall developer and user experience. By focusing on constructing a unified operation object, EOAs (Externally Owned Accounts) can ensure interoperability and streamline interactions across different AA vendors, ultimately reducing complexity and fostering a more cohesive ecosystem.
Specification
UserInstruction Struct
pub struct UserInstruction {
 pub inner_instructions: Vec&lt;InnerInstruction&gt;,
 // additional fields for replay attack
 pub nonce: u64,
 // fields for timestamp validation
 pub valid_from: u64,
 pub valid_until: u64,
 // executed with modulars ids
 pub modulars: Vec&lt;u32&gt;,
}
pub struct InnerInstruction {
 // conventional Solana Instruction fields, except data
 pub program_id: Pubkey, //_dstAddress
 pub keys: Vec&lt;AccMeta&gt;, // solana req
 pub data: Vec&lt;u8&gt;, //_payload
}
UserInstruction Struct Fields Explanation
The UserInstruction struct is designed to encapsulate the necessary information for executing a series of instructions on the Solana blockchain while addressing concerns such as replay attacks and timestamp validation. Here is a detailed explanation of each field:
inner_instructions: Vec&lt;InnerInstruction&gt;
Type: Vector of InnerInstruction structs
Purpose: Holds a list of inner instructions that define the specific operations to be executed on the blockchain. Each InnerInstruction represents a single action, such as transferring tokens or invoking a contract.
Details: This field is primarily used for multi-call operations, allowing multiple inner instructions to be batched together. By grouping multiple inner instructions into a single UserInstruction, the transaction size is reduced, and the most relevant fields are batched together.
nonce: u64
Type: Unsigned 64-bit integer
Purpose: Provides a mechanism to prevent replay attacks by ensuring that each UserInstruction has a unique value.
Details: The nonce should be incremented for each new UserInstruction sent by the same user or entity. If an instruction with the same nonce is detected, it can be rejected as a replay attack.
valid_from: u64
Type: Unsigned 64-bit integer (timestamp)
Purpose: Specifies the earliest time at which the UserInstruction can be executed.
Details: This field ensures that the instruction is not processed before a certain time, providing control over the timing of execution. This feature allows for future-dated transactions, meaning a transaction can be set to execute at a future time. Such transactions can be executed by anyone once the valid_from time is reached, enabling scheduled operations and enhancing automation capabilities on the Solana network.
valid_until: u64
Type: Unsigned 64-bit integer (timestamp)
Purpose: Specifies the latest time at which the UserInstruction can be executed.
Details: This field prevents the execution of stale instructions by setting an expiration time. If the current time exceeds valid_until, the instruction should be rejected. Combined with valid_from, this field ensures that transactions can be precisely scheduled within a specific time window.
modulars: Vec&lt;u32&gt;
Type: Vector of unsigned 32-bit integers
Purpose: Identifies the modular components or modules that are relevant for the execution of this UserInstruction.
Details: This field allows for the extension and customization of the execution logic by specifying which modules should be considered during execution. Each module might represent different aspects of the operation, such as pre-execution hooks, post-execution hooks, validation, authorization, or custom business logic. The modular approach enables flexible and reusable execution patterns, ensuring that additional logic can be incorporated before and after the main execution flow.
Customization and Security
The inclusion of nonce, valid_from, and valid_until fields not only provides replay attack prevention and timestamp validation but also allows for customized signatures within the hash value, instead of relying on block hash signatures in Solana. This design enables greater flexibility and security in transaction validation and execution.
InnerInstruction Struct Fields Explanation
The InnerInstruction struct represents individual instructions that make up a UserInstruction. Here is a detailed explanation of its fields:
program_id: Pubkey
Type: Public key
Purpose: Specifies the Solana program to be called by this instruction.
Details: The program_id determines which on-chain program will process the instruction. In the context of Solana, this field is used to perform a Cross-Program Invocation (CPI) call, which allows one program to invoke another program on the blockchain.
keys: Vec&lt;AccMeta&gt;
Type: Vector of AccMeta
Purpose: Lists the accounts that are read or written by the instruction.
Details: Each AccMeta includes information about the account’s role in the instruction (read-only, writable, signer, etc.). This ensures that all necessary accounts are correctly referenced and used during execution. Proper account metadata is crucial for CPIs to ensure that the invoked program has access to the required accounts with the correct permissions.
data: Vec&lt;u8&gt;
Type: Vector of unsigned 8-bit integers (bytes)
Purpose: Contains the instruction-specific data, typically the parameters or payload to be processed by the program.
Details: This field encodes the actual operation or command to be executed by the target program. The data field allows for the passing of specific parameters required for the execution of the CPI call, enabling the target program to understand and process the instruction accordingly.
UserInstruction Summary
inner_instructions: List of operations to be performed.
nonce: Unique identifier to prevent replay attacks.
valid_from: Start time for when the instruction can be executed.
valid_until: Expiry time after which the instruction cannot be executed.
modulars: Identifiers for additional modules relevant to the execution.
In the context of Solana and the provided code snippet, serializeInIx refers to the serialization of the InnerInstruction object into a format that can be hashed. This serialization process converts the InnerInstructionstruct into a byte array that can be processed further, such as being hashed using Keccak256.
Explanation of Serialization with UserInstruction
In the context of Solana and the provided code snippet, serializeInIx refers to the serialization of the UserInstruction object into a format that can be hashed. This serialization process converts the UserInstructionstruct into a byte array that can be processed further, such as being hashed using Keccak256.
Serialization using Borsh
Borsh (Binary Object Representation Serializer for Hashing) is a binary serialization format designed to serialize data structures in a compact, deterministic manner. In Rust, Borsh is commonly used for serializing data structures before storing them on-chain or hashing them.
To serialize a UserInstruction using Borsh, you would typically implement the BorshSerialize trait for the struct, if not already provided, and then use the .try_to_vec() method to serialize it.
Example:
use borsh::{BorshSerialize, BorshDeserialize};
#[derive(BorshSerialize, BorshDeserialize)]
pub struct UserInstruction {
 pub inner_instructions: Vec&lt;InnerInstruction&gt;,
 pub nonce: u64,
 pub valid_from: u64,
 pub valid_until: u64,
 pub modulars: Vec&lt;u32&gt;,
}
#[derive(BorshSerialize, BorshDeserialize)]
pub struct InnerInstruction {
 pub program_id: Pubkey,
 pub keys: Vec&lt;AccMeta&gt;,
 pub data: Vec&lt;u8&gt;,
}
// Usagelet serialized_user_ix = user_ix.try_to_vec().unwrap(); // Serialize the UserInstruction
Explanation of the Hash Construction
The provided hash construction uses Keccak256, a cryptographic hash function, to create a unique hash for the UserInstruction object, including its associated program ID and serialized inner instructions.
Here’s a breakdown of the code:
ethers.keccak256(
 Buffer.concat([
 Buffer.from(aaFactory.programId.toBytes()),
 ethers.getBytes(ethers.keccak256(ethers.hexlify(serializeInIx(userIx)))),
 ])
);
Summary Steps Explained:
Serialize UserInstruction:
serializeInIx(userIx): This function serializes the UserInstruction object (userIx) into a byte array. In a Solana program using Rust, this would be done using Borsh serialization as shown above.
Convert to Hex and Hash:
ethers.hexlify(serializeInIx(userIx)): Converts the serialized byte array into a hex string.
ethers.keccak256(...): Computes the Keccak256 hash of the hex string representing the serialized UserInstruction.
Concatenate with Program ID:
Buffer.from(aaFactory.programId.toBytes()): Converts the program ID to bytes.
Buffer.concat([...]): Concatenates the bytes of the program ID and the Keccak256 hash of the serialized UserInstruction.
Final Hash:
ethers.keccak256(Buffer.concat([...])): Computes the Keccak256 hash of the concatenated buffer, producing the final hash that includes both the program ID and the serialized UserInstruction.
Execution Function
In the context of Account Abstraction (AA) on Solana, the execution function is a standardized interface that allows any UserInstruction signed by the user to be executed across different AA provider programs. This unification is crucial for ensuring interoperability and reducing complexity for developers and users.
Purpose
The purpose of the execution function is to provide a common method that AA provider programs can implement to process and execute UserInstruction objects. By following this standard interface, users can sign a UserInstructiononce and use it with any compliant AA provider program, fostering a more seamless and consistent experience across the Solana ecosystem.
Function Definition
pub fn execute_operation(
 accounts: &amp;[AccountMeta],
 instruction_data: &amp;[u8],
) -&gt; Result&lt;(), ProgramError&gt; {
 // Deserialize the UserInstruction 
 let user_instruction: UserInstruction = UserInstruction::try_from_slice(instruction_data)?; 
 
 // Logic to process and execute the UserInstruction 
 for inner_instruction in user_instruction.inner_instructions { 
 // Process each inner instruction 
 // Invoking another program with CPI 
 }
 
 Ok(())
}
Explanation
Function Signature:
accounts: &amp;[AccountMeta]: A slice of account metadata that provides the necessary account information for the execution context.
instruction_data: &amp;[u8]: A slice of bytes that represents the serialized UserInstruction object.
Result&lt;(), ProgramError&gt;: Returns a result type to handle execution success or errors.
Deserialization:
let user_instruction: UserInstruction = UserInstruction::try_from_slice(instruction_data)?;: Deserializes the byte array into a UserInstruction object using Borsh.
Processing the UserInstruction:
Iterates over each InnerInstruction within the UserInstruction.
Processes each inner instruction (e.g., invoking another program using CPI).
Conclusion
The design choices for the UserInstruction and InnerInstruction structs, as well as the unified execution interface, prioritize flexibility, extensibility, simplicity, and security. These design choices include:
UserInstruction can encapsulate multiple InnerInstruction objects, enabling complex operations to be executed atomically. The modulars field allows for customizable pre-execution and post-execution hooks, validation, authorization, and other business logic.
The unified execution interface (execute_operation) standardizes processing across different AA provider programs, fostering interoperability and simplifying development. This standardization ensures a consistent experience for developers and users, reducing fragmentation and complexity across different AA providers.
Security is enhanced through the nonce field, preventing replay attacks by ensuring unique UserInstruction instances, and valid_from and valid_until fields, providing timestamp validation to prevent executing stale instructions.
Efficiency is achieved by batching multiple InnerInstruction objects into a single UserInstruction, reducing transaction size and overhead, improving network efficiency. Keccak256 hashing, including the program ID, ensures cryptographic consistency and unique, secure instructions.
By adopting this standardized interface for Account Abstraction (AA) in the Solana ecosystem, the operation object can be unified across different AA provider programs. This unification enhances AA capabilities, streamlines user interactions, and simplifies the development process for protocols and users alike, fostering a more integrated, efficient, and user-friendly blockchain environment. The Solana ecosystem benefits from improved interoperability, consistency, flexibility, security, and efficiency, ultimately contributing to a more robust blockchain infrastructure.",,Elwin,239,0,0,1,2024-07-25T06:27:54.094Z,2024-07-25T06:27:54.199Z,sRFC,6,2024-07-25T06:27:54.199Z
1804,sRFC 29 - Input types of blinks and actions,https://forum.solana.com/t/srfc-29-input-types-of-blinks-and-actions/1804,"sRFC 29 - Input types of blinks and actions
TLDR
Add new input types within blink clients to improve the user experience and unlock new use cases for Action builders.
Rationale
The current Solana Actions and blinks specification only supports a single, generic input type of a non-structured text box. While this basic input is useful, having more explicit and declarative input types would allow Action builders and blink clients to unlock new use cases and improve the user experience of using blinks across the internet.
Current Spec
Per the current specification, an Action api will declare what user input fields it desires using the structured response (ActionGetResponse) from the initial GET request, specifically via any child item of links.actions containing the parameters field.
The current specification is this:
/**
 * Response body payload returned from the Action GET Request
 */
export interface ActionGetResponse {
 /** image url that represents the source of the action request */
 icon: string;
 /** describes the source of the action request */
 title: string;
 /** brief summary of the action to be performed */
 description: string;
 /** button text rendered to the user */
 label: string;
 /** UI state for the button being rendered to the user */
 disabled?: boolean;
 /** */
 links?: {
 /** list of related Actions a user could perform */
 actions: LinkedAction[];
 };
 /** non-fatal error message to be displayed to the user */
 error?: ActionError;
}
/**
 * Related action on a single endpoint
 */
export interface LinkedAction {
 /** URL endpoint for an action */
 href: string;
 /** button text rendered to the user */
 label: string;
 /** parameters used to accept user input within an action */
 parameters?: ActionParameter[];
}
Any ActionParameter declared will result in a plain text input be rendered to the user (with the optional placeholder text) in order to accept their input, which ultimately gets sent the Action api endpoint via the POST request.
Additionally, the user input data is only sent to the Action api via query parameters and template literals (e.g. {name}) declared within the specific linked action’s href field. This presents limitations and complexities when accepting longer user input values or more complex input types.
/**
 * Parameter to accept user input within an action
 */
export interface ActionParameter {
 /** parameter name in url */
 name: string;
 /** placeholder text for the user input field */
 label?: string;
 /** declare if this field is required (defaults to `false`) */
 required?: boolean;
}
Proposal
To support sending longer and more complex user input to the Action API (via the POST request), user input should be optionally sent via the POST request body, not just templatized query parameters (like the user’s account address is already being sent).
All user input should be sent to the Action API as follows for each LinkedAction:
any input parameters specified via template literals in query parameters of the href value should have their respective user input values sent via the its named query parameter (this is how the spec currently works)
all remaining input parameters (aka those that do NOT have a template literal declared in the href ) should be sent in the body of the POST request (along side the user’s account, but should never be able to modify the account value)
Note: if a template literal for any of the parameters is found, it should not be sent via the body. This will reduce the total data sent over the network and is always a best practice to not send duplicate data.
In the end, this allows developers to accept user input via query parameters (not breaking existing applications) or via the POST request body. And effectively set an implicit default of using the POST body to receive the user input, just like a typical HTML form.
Update the ActionParameter to support declaring different types of user input. In many cases, this type will resemble the standard HTML input element.
This new new type attribute should have a type declaration as follows:
/**
 * Input type to present to the user 
 * @default `text`
 */
export type ActionParameterType =
 | ""text""
 | ""email""
 | ""url""
 | ""number""
 | ""date""
 | ""datetime-local""
 | ""checkbox""
 | ""radio""
 | ""textarea""
 | ""select"";
Each of the proposed type values should normally result in a user input field that resembles a standard HTML input element of the corresponding type (i.e. &lt;input type=""email"" /&gt;) to provide better client side validation and user experience:
text - equivalent of HTML “text” input element
email - equivalent of HTML “email” input element
url - equivalent of HTML “url” input element
number - equivalent of HTML “number” input element
date - equivalent of HTML “date” input element
datetime-local - equivalent of HTML “datetime-local” input element
checkbox - equivalent to a grouping of standard HTML “checkbox” input elements. The Action api should return options as detailed below. The user should be able to select multiple of the provided checkbox options.
radio - equivalent to a grouping of standard HTML “radio” input elements. The Action api should return options as detailed below. The user should be able to select only one of the provided radio options.
Other HTML input types not specified above (hidden, button, submit, file, etc) are not supported at this time.
In addition to the elements resembling HTML input types above, the following user input elements are also supported:
textarea - equivalent of HTML textarea element. Allowing the user provide multi-line input.
select - equivalent of HTML select element, allowing the user to experience a “dropdown” style field. The Action api should return options as detailed below.
When type is set as select, checkbox, or radio then the Action api should include an array of options that each provide a label and value at a minimum. Each option may also have a selected value to inform the blink-client which of the options should be selected by default for the user (see checkbox and radio for differences).
 interface ActionParameterSelectable extends ActionParameter {
 options: Array&lt;{
 /** displayed UI label of this selectable option */
 label: string;
 /** value of this selectable option */
 value: string;
 /** whether or not this option should be selected by default */
 selected?: boolean
 }&gt;;
}
If no type is set or an unknown/unsupported value is set, blink clients should default to text and render a simple text input (just as they do now).
The Action API is still responsible to validate and sanitize all data from the user input parameters, enforcing any “required” user input as necessary.
For platforms other that HTML/web based ones (like native mobile), the equivalent native user input component should be used to achieve the equivalent experience and client side validation as the HTML/web input types described above.
Note: As with the current spec, if a LinkedAction does not declare the parameters attribute, then no user input is requested by the Action api and blink clients should continue to render a single button that performs the POST request to the href endpoint. This proposal does not change that.
Closing Notes
With the support of more input types, developers will be able to build more complex blinks and accept more user input. Having “too many” user input fields could result in a reduced user experience and also make users less likely to actually enter all the requested inputs.
To avoid UI bloat and degrading user experiences, blink-clients are likely to impose “soft limits” on the number of input fields they display. While this Actions/blink specification avoids taking an opinionated approach on the UI layer of blinks, the following is a reasonable guideline (and used by Dialect’s blinks SDK today):
10 buttons + 3 inputs (if separate Actions)
10 inputs (if it’s a form, and other actions are not rendered if there is a form in the response)
As such, Action API developers should limit the number of input parameters they request as blink-clients may limit how many input fields get shown to users. Developers should also keep in mind that if a user does not see all input fields (because the blink-client soft limits them as described above), then the user will have no way to enter a value. Therefore if all these fields are required by the Actions API, the user will NOT be able to actually execute your Action and get a transaction.","[0xaryan]: For the user input, instead of having different types for text , email , url, number , a general tag regexp can be used, which would provide more flexibility.
The ActionParamaterType can then be modified to
/**
 * Input type to present to the user 
 * @default `regexp`
 */
export type ActionParameterType =
 | ""regexp""
 | ""date""
 | ""datetime-local""
 | ""checkbox""
 | ""radio""
 | ""textarea""
 | ""select"";
The following regex pattern can be used for the text, email, url , number
text - .*
email - ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}
url - ^(https?|ftp):\/\/[^\s/$.?#].[^\s]*
number - ^-?\d+(\.\d+)?
any other regex can be used for more specific input types.
If ActionParamterType is set to regexp then the input field will be like
&lt;input type=""text"" id=""emailInput"" name=""emailInput"" pattern=""^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"" required&gt;
With the support of regexp, the input field provides a more flexible approach.

[nickfrosty]: Having a regex option makes a lot of sense, but could be really annoying for people out of the box that just want the benefit of simple input types like email or url.
We could support both though! Adding an additional type=regexp and accepting an additional regexp value in the ActionParameter type. This way people get the best of both worlds
Edit:
@0xaryan we could also simplify even more by adding the regexp attribute to the ActionParameter so it could be applied to any user input type, no matter the type.
The benefit of using the natively support input types (like email, url, etc) is that browsers and native clients will give the better UX and help enforce that input type in a native way (like if email is set and you try to enter a non email, the browser will not let you submit the form). Vice if you only use a regex, then the blink client would be the only layer of client side validation there, allowing users to still submit

[nickfrosty]: This is my official update to the proposal above, specifically point (2)
PS: For some reason I could not actually update the original post
Update the ActionParameter to support declaring different types of user input and allow providing regular expression patterns for more complicated input.
The updated ActionParameter should be updated as follows:
/**
 * Parameter to accept user input within an action
 */
export interface ActionParameter {
 /** input field type */
 type?: ActionParameterType;
 /** regular expression pattern to validate user input client side */
 pattern?: string;
 /** human readable description of the `pattern` */
 patternDescription?: string;
 /** parameter name in url */
 name: string;
 /** placeholder text for the user input field */
 label?: string;
 /** declare if this field is required (defaults to `false`) */
 required?: boolean;
}
The pattern should be a string equivalent of a valid regular expression. This regular expression pattern should by used by blink-clients to validate user input before before making the POST request. If the pattern is not a valid regular expression, it should be ignored by clients.
The patternDescription is a human readable message that describes the regular expression pattern. If pattern is provided, the patternDescription is required to be provided.
If the user input value is not considered valid per the pattern, the user should receive a client side error message indicating the input field is not valid and displayed the patternDescription string.
(the remaining contents of section (2) remains the same)

[0xaryan]: Having the input types coupled with regexp makes more sense, and I strongly agree on making a better UX for the user.
The pattern and patternDescription will create a good UX and also offer more design space to the developer.

[nickfrosty]: PR has been opened on the Actions spec repo:
 
 github.com/solana-developers/solana-actions
 
 
 
 
 
 
 
 
 [SPEC] v2.1 blink input types
 
 
 solana-developers:main ← solana-developers:spec-blink-input-types
 
 
 
 opened 05:41PM - 19 Jul 24 UTC
 
 
 
 
 nickfrosty
 
 
 
 
 +604
 -4
 
 
 
 
 
 ## TLDR
Add new input types within blink clients to improve the user experien…ce and unlock new use cases for Action builders.
## Rationale
The current Solana Actions and blinks specification only supports a single, generic input type of a non-structured text box. While this basic input is useful, having more explicit and declarative input types would allow Action builders and blink clients to unlock new use cases and improve the user experience of using blinks across the internet.
## SRFC
See SRFC #29 - https://forum.solana.com/t/srfc-29-input-types-of-blinks-and-actions/1804
The `next` tag has been published with this PR at version `2.1.0-beta`
```
npm install @solana/actions-spec@2.1.0-beta
```

[fsher]: Given current support of passing arguments is limited to url encoded query parameters (through string templates) and there is still a possibility to pass them as such, I think it’s worth specifying how are they passed for multiple option types (checkbox). I propose for types that are considered multi-option (checkbox, or maybe select with multiple in the future) to be inserted as a single comma-separated string.
Example:
href: ""/?test={test}"" , where test is a parameter type checkbox with options (just values for simplicity): 1, 2, 3, 4 . If user selects 1, 4 , then the request will be made with ?test=1,4 (url encoded)
POST body would return actions with multiple values as arrays of strings.

[fsher]: Also several other suggestions:
POST request parameters
Since the Actions spec may change in the future with proposals to add additional fields to POST request body, I think it’s worth moving parameter values to a separate object inside POST request body. This would avoid potential conflicts with incoming parameter action names.
POST request body type can look something like this:
{
 account: string;
 /** 
 Map of parameter values from user's interaction
 key - parameter name
 value - input value (by default `string`, if multi-option, `Array&lt;string&gt;`
 */
 data?: Record&lt;string, string | Array&lt;string&gt;&gt;;
}
Min/Max options
Most of the parameter types can support min and max natively in some way or another. Adding them would strongly help developers achieve better UX with forms or single actions.
Expected behavior:
text, url, email, textarea
min and max represent the minimum and maximum string length required. (should follow minlength and maxlength HTML attributes)
date, datetime-local
min and max represent the borders for date selection (should follow min and max HTML attributes)
number
min and max represent the minimum and maximum for the numerical value (see date, datetime-local links)
checkbox
min and max represent the amount of checked boxes that user can select
select, radio
not applicable
If select will support multiple options, then it would follow the same rules as checkbox.
Pattern &amp; Pattern Description
pattern can be provided to all parameter types (except select, radio, checkbox, date, datetime-local).
If pattern present, type is not empty and not equal to text, then type would become a more stylistic property (for blink clients to render the input styled for the type) and use pattern to validate the entered value.
patternDescription is definitely important for cases when pattern is specified, but also can be valuable for other parameter types for better explanation to the user. Which is why I’d like to suggest renaming to just description or more specific settingsDescription or validationDescription.
Would love some feedback around these points.

[nickfrosty]: This is a good callout @fsher, thanks for bringing it up. In my head I was assuming that a multi select option like checkbox would function the same as a traditional HTML form, but this could become inconsistent easily. Especially when considering non-web based platforms.
So I agree with making it more explicit in the spec, and your proposal looks good to me.
My one thought on it is that if the comma separated value is sent in a url query param: should we specify anything about the value being uri encoded, but not the commas?
Depending on the value that the Action API declares in the option, it could have spaces or special characters which would be required to uri encode/decode. And I think having the comma uri encoded might potentially having issues when parsing it server side?
In my head, ideally people would not use a query param for checkboxes and instead have it sent in the body, but you never know what a dev might do lol

[nickfrosty]: POST request parameters
Love this. it helps keep things more clean. You have my vote!
Do you think it is worth it in the spec types to allow developers to specify the keys for the data fields? This could improve DX if they decide to use it.
Something like this:
/**
 * Response body payload sent via the Action POST Request
 */
export interface ActionPostRequest&lt;T = string&gt; {
 /** base58-encoded public key of an account that may sign the transaction */
 account: string;
 /** 
 Map of parameter values from user's interaction
 key - parameter name
 value - input value (by default `string`, if multi-option, `Array&lt;string&gt;`
 */
 data?: Record&lt;keyof T, string | Array&lt;string&gt;&gt;;
}
Min/Max options
Adding min/max options make sense as it can provide a better UX on the client side and is an easy add too. Using them with checkboxes is an interesting idea too, I don’t think I have ever seen that in the wild anywhere, but it could be useful for people.
Pattern &amp; Pattern Description
Your clarification on pattern not actually applying to select, radio, checkbox, date, datetime-local is fair. I had the assumption of exactly this when writing it up, but we can make it more explicit in the spec details. Especially to help craft better types in the @solana/actions-spec package.
For the description, are you suggesting that this could simply be displayed to the user for any parameter field that’s declared regardless of if a regex pattern is being used?
If so, I’m game. I think it helps developers provide a better UX
If not, can you clarify?

[fsher]: My one thought on it is that if the comma separated value is sent in a url query param: should we specify anything about the value being uri encoded, but not the commas?
I believe since it’s in query params, the whole string should be encoded, including the commas. Servers should parse that into a regular string, or potentially just decode with decodeURIComponent (or similar in other languages). But I definitely agree, this is something that can be hard to debug. At the same time, since the spec now prefers sending values inside POST body, it could be ok (just a limitation that can be documented).
What do you think?

[fsher]: Do you think it is worth it in the spec types to allow developers to specify the keys for the data fields? This could improve DX if they decide to use it.
Yes, love it! Better typing, better DX!
Adding min/max options make sense as it can provide a better UX on the client side and is an easy add too. Using them with checkboxes is an interesting idea too, I don’t think I have ever seen that in the wild anywhere, but it could be useful for people.
Yep, it’s just an option that can be implemented. E.g. devs can be doing questioners or group together checkboxes to make them all required (like have 2 checkboxes and min to be 2). Again, just wild guesses, I’m sure people will think of something!
For the description, are you suggesting that this could simply be displayed to the user for any parameter field that’s declared regardless of if a regex pattern is being used?
The first one, yes. Sorry, should’ve made myself clearer :D. A description for the field to explain what’s expected in the input and potentially an explanation why the validation fails.
again, wild (and not the best) example:
type: ""url"", max: 20, description: ""Please provide a shortened link (e.g. bit.ly)""
description can initially show as a label, but if user somehow entered more that 20 symbols (assuming it’s possible), that description could become an error indicator inside the client.

[nickfrosty]: Understood! I like it all 
the whole string should be encoded, including the commas.
I think this might cause issue with complicated values, but I think it is fine just being a limitation because the POST body.
I’m okay with the simple blanket statement to the effect of “uri encode the comma separated list, but we recommend using the query param for it”

[fsher]: Just to be clear, POST body should receive an array of strings. If parameter used in query params, then it should become a comma-separated uri-encoded string.

[Gabynto]: Sure! It sounds like adding more input types to blinks will really enhance what developers can do with them. Keeping the user experience in mind by limiting the number of input fields makes a lot of sense too.

[nickfrosty]: Yup, we are on the same page

",nickfrosty,410,10,15,16,2024-07-15T19:12:28.961Z,2024-07-24T13:06:08.274Z,sRFC,6,2024-07-24T13:06:08.274Z
1741,Displaying all NFT media types in Blinks,https://forum.solana.com/t/displaying-all-nft-media-types-in-blinks/1741,"Displaying all NFT media types in Blinks
Summary
Adding “animation_url” and “category” to the blinks response would allow the unfurler/extension to include more exciting media in Blinks. (“category” coming from the metaplex nft standard)
This would greatly benefit digital artists sharing their art more and more of which is not just images. Especially generative artists who currently have no way to share their code based art live on social media.
Vr/models might be more challenging to implement afaik (would require using the model-viewer library)
But video and html should be pretty straight forward. The “category” would determine how the media is rendered, if not present just fall back to icon/image.
very basic/crude example would be:
{category === ""html"" &amp;&amp; (
 &lt;iframe
 src={animation_url}
 sandbox=""allow-scripts allow-same-origin""
 referrerPolicy=""no-referrer""
 style=""pointer-events: none; width: 100%; height: 100%""
 /&gt;
)}
{category === ""video"" &amp;&amp; (
 &lt;video
 src={animation_url}
 autoPlay
 muted
 loop
 controls=""false""
 style=""width: 100%; height: 100%""
 /&gt;
)}
Concerns to discuss
Security risks with iframe/html
Website performance
I’m not a security expert so please correct me if I’m wrong. But to my knowledge sandboxing the iframe should prevent most attack vectors. We also eliminate risks by preventing user interactions with “pointer-events: none” or an invisible overlay on top of the iframe.
For performance, I wonder how difficult it would be to add an intersection observer to each Blink, and only render the media when its in view (or even only render the highest Blink when multiple are in view)?
There would always be an icon/image to fall back on in the case of errors loading the media.
Curious to know everyones thoughts!","[EV3]: @c4b4d4 and @nickfrosty would love to get your input here!

[c4b4d4]: I think HTML has some implications that it wouldn’t be easy to decide if it’s good or not to include.
It is safe as in what is being hosted in the iframe cannot read/write its parent, but is not preventing people making HTMLs that look like Twitter asking for their password 
On the code you shared I wouldn’t use allow-same-origin, to make it secure, I think having it set makes it be treated as in the “same-origin” in certain validations
 EV3:
sandbox=""allow-scripts allow-same-origin""
3D would be pretty cool, I think model-viewer could work.
Hiding the element when not visible does help on performance (also adds complexity). I think Twitter in a normal screen size has ~15 pre-loaded tweets on its HTML
For the videos, on Twitter when you have multiple of them, if you had one playing and want to play another one, the first one stops and then the second one starts playing. Would catch it to keep a good UX.

[EV3]: Regarding phishing attacks, I think that could easily be prevented with either “pointer-events: none” or a invisible div overlay to prevent user interactions.
For “allow-same-origin” I’m open to removing that if it creates a security risk. Although it would greatly reduce what could be displayed (speaking from personal experience, many of my html nfts fetch other nfts/images or fonts)
3D would be rad, I guess I was thinking that it would be asking a bit much to add the dependency, but if thats not a concern, then yes absolutely we should add that to the spec as well

[nickfrosty]: I personally think the idea of more rich experiences could be interesting, at least displaying more than a simple image (even though this could be an animated gif already)
Video could be possible but depending on the file size and the streaming medium, this could get hard to handle and cache. Especially when using proxy services to help protect user’s privacy. Proxy streaming a video can be expensive. And some services do not actually allow streaming (I think you cannot stream videos off of arweave for some reason)
The animation_url idea could be useful, but even still today not all popular Solana wallets support this. So I suspect they would be unlikely to support it this way. I suspect it is due to some of the proxy/streaming issues I mentioned above.
For html, while this could be really cool I personally think it is too much of a security risk for people and honestly not likely to be supported. Even if in an iframe. Too many attack vectors, even if you get everything else right.
I am not familiar with VR models at all, would action-aware clients (like wallets and other extension) have to enable this individually?

[EV3]: hmm, yah hadn’t thought about the proxy aspect for video, figured it would all be client side.
Can you be more specific on what the security risks for a sandboxed iframe would be?
If that is an impassable issue, then perhaps we could come up with a standard with tight guardrails for uploading p5 projects directly, so that there’s never any raw html

[nickfrosty]: I am personally not sure of the security issues with iframes, I just know there can be lots. So it throws up read flags in my head

",EV3,698,2,6,7,2024-06-29T03:44:31.663Z,2024-07-08T16:39:36.298Z,sRFC,6,2024-07-08T16:39:36.298Z
283,sRFC 00017: Token Metadata Interface,https://forum.solana.com/t/srfc-00017-token-metadata-interface/283,"Token-Metadata Interface
Summary
Token-metadata is a very complex space, but at its base, all creators of
fungible and non-fungible tokens need a way to upload information about their token
on-chain. This proposal contains a spec for a simple token-metadata state
and instruction interface for SPL token mints. The interface can be implemented
by any program.
With a common interface, any wallet, dapp, or on-chain program can read the metadata,
and any tool that creates or modifies metadata will just work with any program
that implements the interface.
Motivation
Token creators on Solana need all sorts of functionality for their token-metadata,
and the Metaplex Token-Metadata program has been the one place for all metadata
needs, leading to a feature-rich program that still might not serve all needs.
At its base, token-metadata is a set of data fields associated to a particular token
mint, so we propose an interface that serves the simplest base case with some
compatibility with existing solutions.
With this proposal implemented, fungible and non-fungible token creators will
have two options:
implement the interface in their own program, so they can eventually extend it
with new functionality or even other interfaces
use a reference program that implements the simplest case
Spec
Token-Metadata Struct
A program that implements the interface must write the following data fields
into a type-length-value entry into the account:
type Pubkey = [u8; 32];
type OptionalNonZeroPubkey = Pubkey; // if all zeroes, interpreted as `None`
type TlvDiscriminator = [u8; 8];
struct TokenMetadata {
 discriminator: TlvDiscriminator,
 length: u32,
 update_authority: OptionalNonZeroPubkey,
 mint: Pubkey,
 name_len: u32,
 name: [u8; 32],
 symbol_len: u32,
 symbol: [u8; 10],
 uri_len: u32,
 uri: [u8; 200],
}
This struct has some ABI-compatibility with the Metaplex
Metadata struct.
The discriminator here is larger, at 8 bytes instead of 1, and is a different
value, but the following 318 bytes can be interpreted in the same way by wallets,
programs, and indexers.
The discriminator must be hashv(&amp;[""token-metadata-interface::state""])[0..8].
By storing the metadata in a TLV structure, a developer who implements this
interface in their program can freely add any other data fields in a different
TLV entry.
You can find more information about TLV / type-length-value structures at the
spl-type-length-value repo.
Instructions
Here are the instructions that must be implemented by a program conforming to
the interface.
Initialize Token Metadata
Discriminator: hashv(&amp;[""token-metadata-interface::initialize""])[0..8]
Accounts:
0. [writable] Metadata account
[] SPL mint account
[signer] Mint authority
[] Update authority
Data:
0. name: String
symbol: String
uri: String
The instruction processor must do the following:
check that the mint account is an SPL mint
check that the correct mint authority signed
check that the name / symbol / URI fit in the limits of the struct
check that the metadata account does not already have metadata written to it
write all of the information into the metadata account
NOTE: This instruction only covers initialization and assumes that the provided
account is properly created, meaning that it has enough space for the data, and
enough lamports to be rent-exempt.
Update Token Metadata
Discriminator: hashv(&amp;[""token-metadata-interface::update""])[0..8]
Accounts:
0. [writable] Metadata account
[signer] Update authority
Data:
0. name: String
symbol: String
uri: String
The instruction processor must do the following:
check that the metadata account is owned by the program and contains a valid
token-metadata TLV entry
check that the update authority signed
check that the name / symbol / URI fit in the limits of the struct
write the new information
NOTE: Strings are utf-8 encoded bytes, preceded by a little-endian u32
giving the number of bytes.
While these instructions aren’t strictly required to adhere to the interface,
they will integrate more nicely with other tooling.
For example, the JS SDK for token-interface can allow targeting any program id,
so if a program implements these instructions properly, then they can easily get
more usage.
Alternatives
As described at the start of this proposal, the space of token-metadata is vast
and comprises all sorts of functionality, including fees, programmability,
transferability, minting, etc.
This proposal is deliberately not specific to fungible or non-fungible tokens,
so it includes the base required for both, and nothing more.
For example, functionality for royalties could be implemented through a separate
interface.
(Optional) Program-Derived Address Convention
This base proposal only defines the functionality required to implement the interface,
and does not define any program-derived addresses for where the metadata should
be stored.
This approach is deliberate: by not requiring a particular address derivation,
it is possible for a token program to also implement the metadata interface!
On the other hand, since many metadata programs will likely not be token
programs, we include an optional convention for deriving a program address for
metadata as the following function call:
use solana_program::pubkey::Pubkey;
pub fn metadata_account_address(metadata_program_id: &amp;Pubkey, mint: &amp;Pubkey) -&gt; Pubkey {
 Pubkey::find_program_address(&amp;[b""metadata"", mint.as_ref()], metadata_program_id).0
}
Point for discussion: this is different from the Metaplex token-metadata derivation,
which repeats the metadata program id. This approach feels reasonable, given that
this interface already isn’t fully compatible with the Metaplex token-metadata accounts.
Client libraries need to parse Metaplex token-metadata accounts differently from
accounts that implement the token-metadata interface anyway, so special-casing
the address derivation for Metaplex seems reasonable.
On the flip-side, we can also completely omit this point and leave it for
another proposal regarding “token-metadata discoverability”, which may include an
interface for a token-metadata registry.
Further Work
This interface defines the minimum struct and instructions that a program must
implement in order to be considered a “token-metadata program”. It does not address
discoverability of token-metadata accounts.
For discoverability within the mint, spl-token-2022 will add a mint “extension”
to store the metadata account address.
As a proof-of-concept, spl-token-2022 will also implement this interface, and store the
metadata fields directly in the mint account.","[Jonas.Hahn]: I love this. Could the interface also include an optional list of traits? That would be great for dynamic nfts and games for example. I think it could open lots of possibilities.

[joec]: joncinque:
This interface defines the minimum struct and instructions that a program must
implement in order to be considered a “token-metadata program”. It does not address
discoverability of token-metadata accounts.
For discoverability within the mint, spl-token-2022 will add a mint “extension”
to store the metadata account address.
As a proof-of-concept, spl-token-2022 will also implement this interface, and store the
metadata fields directly in the mint account.
This is great, Jon!
So Token2022 will demonstrate a program adhering to the metadata interface by:
Implementing the required instructions in extension(s)
Using its Mint account to also store Metadata state
For anyone who might be confused, Token2022 is implementing this interface through extensions, but that particular way of implementing interfaces is specific to Token2022, and you can choose to implement the instructions/state however you see fit, so long as everything checks out with things like instructions, required accounts, discriminators, etc.
As Jon mentioned above, something we still need to hash out going forward is the discoverability of PDA accounts with Metadata state, when devs choose not to pack both states into the Mint.
I’m assuming we wouldn’t want to set any kind of standard for seeds (ie. “your PDA must be derived like this”)? This would make discoverability easier, but inhibit flexibility for devs.
An alternative might be Anchor-like discriminators - since we’re already enforcing a discriminator for the instructions?

[joec]: I think the idea with interfaces - especially when it comes to Token Metadata - is to have separate, modular interfaces for every “new” type of metadata or “extension” on metadata.
For example, you might implement:
Jon’s interface above for the base Metadata (name, symbol, uri)
Jonas’s interface for traits (traits, etc.)
*optionally any other interface you want
And then that’s how you can build expanded metadata, and you can choose whether or not to follow Metaplex’s idea of PDAs or pack it all into one state

[joncinque]: Can you describe these optional traits more? Do you mean something like “my token has some base metadata, but it also has other metadata that can change”?
If that’s the case, my guess is that they should be done through another interface, as Joe mentioned, since this is just for token metadata. But if you have a more complete view about how this can fit in, please let me know!

[joncinque]: I’m assuming we wouldn’t want to set any kind of standard for seeds (ie. “your PDA must be derived like this”)? This would make discoverability easier, but inhibit flexibility for devs.
We could! That would be part of the interface, especially where it makes sense. For token metadata, basing it off the mint address seems like a slam dunk. For more complicated cases like trading programs though, it would probably inhibit flexibility if you had to use just a mint address, and not a user wallet, for example.
An alternative might be Anchor-like discriminators - since we’re already enforcing a discriminator for the instructions?
Do you mean discriminators for seeds? We could certainly do that! On the flipside, since Pubkey::find_program_address is already hashing everything together, a byte-string would probably be simplest and also most flexible since you’re not beholden to any particular size.

[austbot]: Hey @joncinque
WRT my comment about changing the data layout I think in order to gain more flexibility the data layout needs to support any size of string uri and custom schematized content blocks .
This can be done by having a fixed size header and then manually deserializing blocks of typed data after the header. View functions on the rust or ts libs that allow a user to grab the specific content blocks that relate to uri or on chain full metadata or what have you.
I dont see the point of this unless its better than token metadata by alot. Unless you allow the developers to do more than they can now then this may not succeed . We also know that “composability” sucks in practice when you need to compose several accounts together to get a single entity. So allowing custom data blocks to be added will allow more use cases that other interfaces can describe like a traits interface, grouping interface, multi owner interface etcetera

[ngundotra]: tl;dr Proposal is missing indexing standard
 joncinque:
(Optional) Program-Derived Address Convention
If this is optional, how are indexers / wallet supposed to show metadata for programs that adhere to this sRFC?
One of the main challenges for Metadata programs is that they will never be shown in wallets.
The main benefit of something like a Metadata spec should be giving each metadata program equal opportunity to be shown in a wallet, which I imagine would require some form of indexing standard as well.
 joncinque:
use solana_program::pubkey::Pubkey;
pub fn metadata_account_address(metadata_program_id: &amp;Pubkey, mint: &amp;Pubkey) -&gt; Pubkey {
 Pubkey::find_program_address(&amp;[b""metadata"", mint.as_ref()], metadata_program_id).0
}
I think this makes sense, but this could also be extremely cost ineffective for large metadata collections.
Another missing item is the ability to support indexing by the following:
collection/grouping ID,
owner
delegate
creators
These are crucial to marketplace functionality that drives the economic usefulness of the already existing token-metadata program.
To be frank, I think this is useful metadata interface for tokens but without the indexing standard, I don’t think it’s useful for NFTs.

[joncinque]: austbot:
WRT my comment about changing the data layout I think in order to gain more flexibility the data layout needs to support any size of string uri and custom schematized content blocks .
That makes sense. I’ll update the proposal to reflect something more general. I’m thinking a Vec&lt;(String, String)&gt; which allows for key-value pairs. I worry that other types would overcomplicate the proposal, and ultimately go against the core of the proposal, which is storing non-functional data.
If an account has additional functional data, like multiple owners, then it’s no longer metadata, and should be accomplished through a different interface. Thanks to TLV data structures, it’s possible to store all of these in the same account, but to get different views on the same account.
But thankfully, if people want to violate that, they can always store JSON strings

[joncinque]: ngundotra:
how are indexers / wallet supposed to show metadata for programs that adhere to this sRFC?
We discussed this offline, and the solution we came up with was to add a view function which emits the data as an event, and to mark the “state” portion of the interface as optional, so that programs can have maximum flexibility in the implementation.
Programs that omit the “state” portion of the interface may face more integration challenges with wallets or programs that read the account data without a simulation or CPI.

[jacobdotsol]: I plan to review this more in-depth but from first glance uri_len can be a u8

[joncinque]: Note this has been implemented in https://github.com/solana-labs/solana-program-library/tree/master/token-metadata/interface along with an example program at https://github.com/solana-labs/solana-program-library/tree/master/token-metadata/example and an implementation in token-2022 at https://github.com/solana-labs/solana-program-library/tree/master/token/program-2022/src/extension/token_metadata, so feel free to put in an issue to SPL if you have any additional comments!

[Hamster]: Appreciate this a lot! Glad to see its implemented and will check the example out.
austbot
Hey @joncinque
WRT my comment about changing the data layout I think in order to gain more flexibility the data layout needs to support any size of string uri and custom schematized content blocks.
Yes this is extremely important! For example: with an increased uri size of 3-4000 characters, we can essentially have whole NFT metadata (JSON with traits and images) on-chain, within the same account. That size fits SMB’s, Tensorians, etc. and of course Blockrons.

[jacksondoherty]: Hey everyone! I just proposed a “Field Authority Interface (sRFC 23)” that works alongside this, check it out!

[jacksondoherty]: After working with clients to adopt this, I’d recommend pivoting to just parsing account data as the standard read method. emit() requires running a transaction with a signer – clients that want to just read will therefore need to audit the program first, making it harder for new programs to get adopted.
cc @ngundotra

[joncinque]: The idea of emit() is to use it with transaction simulation or as part of a running transaction, and not as a standalone instruction, unless you wanted to maintain proof of the state of an account at a certain point in time. Although reading the bytes from the account is easier, not all programs need to support it, which is the other reason to have the instruction.

[jacksondoherty]: Ahhh that makes sense, got it.

",joncinque,3209,10,16,17,2023-06-01T11:56:57.016Z,2024-06-26T13:42:00.448Z,sRFC,6,2024-06-26T13:42:00.448Z
1721,sRFC 27: Blockchain Links (Blinks),https://forum.solana.com/t/srfc-27-blockchain-links-blinks/1721,"Blockchain links (”blinks”, for short) are fully-qualified URLs intended to expose Solana Actions to numerous clients. Blinks come in a few forms:
actions.json published at the origin of a domain
This file acts as a configuration file to allow remapping of human-readable, domain-specific URLs to alternative URLs for resolving Actions.
URL-encoded Solana Actions endpoint included in the ?action= query parameter prefixed with solana-action:
A URL with the solana-action: URI scheme
Rationale
Blockchain applications suffer from many user experience issues. In order for an application to prompt the user to sign and send a transaction via a browser wallet extension, the application needs to be wallet-aware, limiting the surface area of possible blockchain-related use-cases across the internet to only a handful of websites.
Blinks provide an opportunity to “break out” of that requirement by establishing a consistent standard for surfacing Solana Actions for direct invocation without having to leave the website. This can happen because browser extension wallets have full visibility of everything on the page, including blinks. In conjunction with the Solana Actions associated with the blink, the wallet is able to derive enough information to construct the transaction, simulate the transaction’s behavior, and present it to be signed and sent by the user, all without the originating website knowing anything about blockchain, or anything about Solana. Ultimately, the Solana blockchain is the always the operational source of truth, and the need for rigorous authentication schemes (i.e., click on link, login, recommit to the transaction) are pushed to the cryptographically-simple signature verification of the Solana protocol itself, making typically complex transactions like e-commerce as easy as signing and sending a transaction.
Put in layperson’s terms, blinks allows any website on the Internet that can display a URL to potentially be the start of a Solana transaction.
Additionally, given the richness of metadata (including OpenGraph) returned by blinks, other clients like Discord, Telegram, or even iMessage may be able to provide rich, interactive wallet-aware experiences without the native platform knowing anything about Solana.
Safety and Security
Blinks are associated to one or many Solana Actions, and each Solana Action lives at a domain that must be trusted by the user in some form or capacity; the Solana transaction itself is not embedded without context into the URL. The transaction returned by the Action API must still proceed through simulation, and still requires the user to sign and send in order to be “successful”, hence Actions surfaced via blinks have similar trust assumptions as going directly to the dApp, connecting to wallet, and subsequently signing / sending the transaction.",,jnwng,354,0,0,1,2024-06-25T13:13:22.158Z,2024-06-25T13:13:22.208Z,sRFC,6,2024-06-25T13:13:22.208Z
1720,sRFC 26: Multi-Actions (Solana Actions v2),https://forum.solana.com/t/srfc-26-multi-actions-solana-actions-v2/1720,"Authors: nick@dialect.to, alexey@dialect.to
The Multi-Action Specification (MAS) is a extension of sRFC 25: Solana Actions and retains interoperability (but not strict backwards-compatibility). See detailed specification here.
Two notable differences:
The metadata returned from GET https://example-action-endpoint.xyz may include more than just label and icon, and may include more informational data (e.g., title, description) as well as a set of up to four linked Action endpoints.
Linked Actions introduces the concept of an input with dynamic input. An Action with dynamic input cannot be the first Action as backwards-compatibility with the v1 Actions Spec does not allow input. Actions with dynamic input receive additional parameters in the body of the POST request alongside the account public key.
All linked Actions themselves respect the v1 Solana Actions specification, and can individually respond to the GET / POST flow, with the exception of Actions with dynamic input.
The transaction returned from POST https://example-action-endpoint.xyz will always be the first of the set of linked Actions from the metadata retrieval step.
Rationale
Allowing the return of additional metadata + multiple Actions allows for more expressive, Action-driven interfaces to be built.",,jnwng,150,0,0,1,2024-06-25T13:12:48.597Z,2024-06-25T13:12:48.651Z,sRFC,6,2024-06-25T13:12:48.651Z
1719,sRFC 25: Solana Actions v1,https://forum.solana.com/t/srfc-25-solana-actions-v1/1719,"v1 of the Solana Actions API specification is identical to the Solana Pay transaction request specification, with one key differentiator. Instead of the solana: URI scheme, solana-action: is used to provide an opportunity for clients to treat Actions differently that existing payments schemes related to Solana Pay.
In part, this is due to a common failure case related to QR codes on iOS devices. The use of solana: was ambiguously used by various applications that lacked support for Solana Pay, resulting in native camera scans prompting the opening of apps with no action item. As a result, on iOS devices, the oldest installed application that has registered this URI scheme will handle the Action by default, and we expect a higher success rate since the use of solana-action: is much more deliberate.
This does not affect Android as the OS lets the user select the app that would handle that URI scheme.
The use of Universal Links was dismissed as this targets a specific URL and application, with no opportunity for other apps to equivalently serve that request.
solana: can be used as a fallback scheme; Actions-aware clients will support both schemes
Similarly, wallets like Phantom / Solflare / Backpack / TipLink made assumptions that URIs of this form would strictly be payments-related, and the resultant UX flow might be confusing for generalized use-cases unrelated to payments.
The use of APIs in conjunction with Solana programs provides the ability to express rich combinations of offchain / onchain business logic, reducing time to market and the use of the right technology for any given purpose.
Rationale
Solana Actions builds on the power of Solana Pay transaction request, but with an explicit reframing of the premise to encompass use-cases well-beyond payments. “Solana Actions” captures the wide-swath of diverse products built on the Solana blockchain, including (but not limited to) governance, games, and digital assets.",,jnwng,158,0,0,1,2024-06-25T13:05:58.697Z,2024-06-25T13:05:58.755Z,sRFC,6,2024-06-25T13:05:58.755Z
1681,sRFC 0024: Extending off-chain message signing standard,https://forum.solana.com/t/srfc-0024-extending-off-chain-message-signing-standard/1681,"sRFC 0024: Extending off-chain message signing standard
Summary
Off-chain message signing (OCMS) is a standard used by software wallets, hardware wallets and protocols to sign arbitrary text messages. These messages have a variety of use cases that all revolve around the same workflow - proving wallet ownership without requiring the user to sign and submit a transaction that pays gas.
Currently, OCMS lacks line break support in its most simple and widely used encoding. This sRFC proposes changes to the standard to add line breaks to allow better formatted messages from apps to improve overall UX.
Reference
Current OCMS standard version 0 is being used as a baseline for this sRFC.
The new standard version should emerge from forking the current reference and implementing the changes bellow.
Changes
Change the header version from 0 to 1.
From:
Only the version 0 header format is specified in this document
To:
Only the version 1 header format is specified in this document
Change the definition of “Restricted ASCII” to include the ‘\n’ (0x0a) character in version 1.
From:
** Those characters for which isprint(3) returns true. That is, 0x20..=0x7e .
To:
** Those characters for which isprint(3) returns true or the newline - \n character. That is, 0x20..=0x7e or 0x0a.
Examples:
In the examples bellow assume that all message encodings are set to 0 - Restricted ASCII, only header version changes
The following message works across both versions:
A sample message with no newline character.
The following message fails when the header version is 0 but is to be accepted with the newly specified version 1:
A sample message with line breaks.
This message allows apps to add line breaks and to better format to their messages.
Compatibility
Adding support to the new version is trivial, just checking the updated header version and the extra newline character on messages.
In order to be backwards compatible with the previous version for software that needs to parse and handle these messages, implementers just need to be slightly lax on header version checks. Instead of forcing the header_version == 0 one can check if header_version &lt;=1.",,qtmoses,396,0,0,1,2024-06-14T18:29:34.642Z,2024-06-14T18:29:34.786Z,sRFC,6,2024-06-14T18:29:34.786Z
370,sRFC 00020: RWA/Security Token Standard,https://forum.solana.com/t/srfc-00020-rwa-security-token-standard/370,"Summary
This sRFC proposes the development of an open-source Real World Asset (RWA) Security Token standard, specifically focused on real estate, but applicable to many other applications, to foster wider adoption of these assets on Solana. The establishment of a standardized framework for tokenizing real-world assets (“RWA”) holds the potential to significantly enhance transparency, accessibility, and efficiency in real estate transactions. This development would bring about substantial benefits for consumers and institutions involved in building and utilizing products within the DeFi space, ultimately contributing to the onboarding of the next 100 million users to the Solana ecosystem.
Introduction
Real-world assets (RWAs) are physical or tangible assets that have value and exist in the real world. Examples of RWAs include real estate, artwork, commodities, vehicles, etc. These assets are typically illiquid due to their physical nature, and their ownership is governed by legal documents. However, blockchain technology presents a unique opportunity to digitize or “tokenize” these assets, making them easily tradable and accessible to a global audience.
Tokenization of RWAs, in the context of real estate, involves converting the rights to an asset into a digital token on a blockchain. Real estate tokenization could bring about a fundamental transformation in the way real estate is owned, traded, and financed. However, the lack of a unified and widely accepted standard for real estate tokenization in the Solana ecosystem is a major impediment to this transformation. This proposal seeks to address this issue by establishing an open-source, ecosystem-endorsed standard for tokenizing real estate on Solana.
The Problem
As it stands today, the Solana ecosystem lacks a unified, widely-accepted standard for real estate tokenization. While Ethereum hosts numerous token standards that apply to real world assets, Solana predominantly relies on Metaplex’s Token Metadata Standard which was created for broad use NFTs. While Metaplex’s standard is great for general NFTs, it lacks crucial features required to allow for the mass adoption of real world assets like escrow facilities, token freeze/nullification, and a standard approach to property data input.
Proposed Solution
Create an open-source, ecosystem-endorsed standard for tokenizing real estate on Solana. This Real World Asset (RWA) standard aims to not only simplify the tokenization process for new entrants but also get broader adoption from existing real estate developers, brokerages, institutions. The development of this standard would thus serve as a public good, spurring growth in Solana’s RWA landscape.
Functional Requirements of the RWA Standard
Commoditized RWA Standard
Title Ownership: Ability to verify the legal ownership of the property by providing verified documentation or parsing via a third party data oracle.
Title Status: Ability to confirm whether the title is clean or dirty depending on liens (legal claims or holds on a property, either by financial institutions or other parties, to secure the repayment of a debt).
General Property Info Update (Updated via trusted RE Data Oracles)
Address Update: Ability to update the property’s address.
Square Footage Update: Ability to update the property’s square footage.
House alteration / major renovations
Securitized RWA Standard
Issuer Transaction Clawback: Ability for the issuer to clawback or reverse transactions if necessary.
Trading Halt Functionality: Ability for the issuer to halt trading in certain circumstances.
Issuance Whitelist: Ability to maintain and update a whitelist for issuance, controlling who can issue the tokens.
Secondary Trading Whitelist: Ability to maintain and update a whitelist for secondary trading, controlling who can trade the tokens after issuance.
Escrow Mechanism: Ability to implement an escrow mechanism, providing a secure way to hold funds or assets until specified conditions are met.
Regulatory Filings Verification: Ability for the public to verify that regulatory filings have been completed, ensuring regulatory compliance.
Dividend Distribution: Ability for the issuer to distribute dividends (such as rental income) to token holders.
Dividend Distribution Schedule Updates: Ability for the issuer to specify and update the schedule for dividend distribution.
Potential Drawbacks
Despite the potential advantages, a few potential drawbacks could arise from this proposal.
Regulatory Complexity: Due to the legal and financial nature of RWAs, particularly real estate, different jurisdictions may necessitate varying levels of regulatory compliance, potentially complicating the adoption process.
Oracle Dependency: The proposed standard would rely on trusted data oracles for real-time information updates, any disruption to these services could potentially impact the overall system’s operation.
Adoption Challenges: Though the standard would simplify the process of tokenization, achieving wide-scale adoption might be challenging, particularly from traditional real estate players unfamiliar with blockchain technology.
Privacy Concerns: Handling sensitive property information on-chain could raise privacy concerns. We need to ensure the appropriate safeguards to prevent unauthorized access and maintain privacy.
Conclusion
The tokenization of real-world assets such as real estate holds the key to unlocking untapped value by democratizing access, enhancing liquidity, and improving efficiency. This proposal aims to provide a robust framework that will help fuel the growth of the RWA landscape within Solana, bringing us a step closer to realizing the vision of truly decentralized finance. The open questions mentioned are essential for the community to resolve together to ensure that the proposed standard is both practical and compliant with regulatory standards.
Open Questions
All of the above is open to comments. Some specific questions to Resolve with 3rd Party Partners are listed below:
Should the standard support multiple US Security Regulations from the get go?
How should title claims be handled?
How are claims tracked?
How are claims proven?
What data should be uploaded for title claims? (title reports)","[joebuild]: Very cool. No strong takes overall, but having worked at a real estate data startup, you will quickly find that the fields in the ‘Commoditized RWA Standard’ as you have them are insufficient (there will be hundreds of fields worth tracking). Would recommend a flexible approach to fields there.

[Whisky]: This is a great idea and a FANTASTIC start to solidify Solana as the ideal chain for RWA solutions. I agree the Metaplex standard does not sufficiently account for the unique nature of RWA’s and their specific requirements in a token.
The reliance on oracles is always a concern, particularly if they rely on government recognition of any of the documents. Do you propose that documents be uploaded to arweave and stored on-chain with a link in the metadata to the title? Would the plan be to eventually incorporate governmental oversight into the transactions, and allow for government offices to sign on-chain transactions and transfers?
Wallet security. The revocable nature of the token is beneficial in that if someone is hacked the original token can be voided and re-issued, however, this also gives power to individual bad actors to make changes to the title that would normally require governmental recognition. How would one determine whether someone was hacked and lost their tokens or if they actually completed an off-chain transaction and transferred the token to the new owner, and attempted to claim it back? If the solution is a multi-sig, who are the signers?
KYC: If allowing for certain forms of financial actions or distributions would the token issuer be responsible for completing KYC? Would the tokens require permissioned transfers? How does one prevent potential violations of US Treasury sanctions or laws without a permissioned system?
The UCC also becomes particularly relevant here with regard to understanding one’s rights in the event of a default of an RWA in which the token represents the legal title. Can defaults trigger instant title transfers on chain in the event of a default? What would be physically necessary for the government to recognize those claims?
These are some of my initial thoughts off the cuff but I have been working on thinking through this issue for some time and would love to keep the conversation going here.

[yashhsm]: Screenshot 2023-03-12 at 8.24.50 PM (1)1774×1004 60.1 KB
Thanks for initiating this discussion - much needed. Great points, but I would argue that a generic RWA standard like Token 2022 would make more sense, as it would allow for greater configuration flexibility. For a specific use-case like Real Estate, projects can build low/no-code solutions on top of these standards to enable even traditional firms to tokenize their assets.
P.S. I was also working on designing an RWA token standard - essentially Permissioned Tokens (including the flowchart) - and would love to hear community comments.

[Basile]: I do believe that at the core of this standard permissioned tokens are a requirement, but I slightly disagree that an RWA Token Standard is essentially Permissioned Tokens. My thoughts are more along the lines of a “configurable” permissioned token that would allow the issuer to specify whether or not this token will be treated as a commodity or security and implement their own respective logic. The distinction between the two is important especially in the case of institutional adoption (funds would trade as securities). Depending on the “RWA Type” the issuer would be responsible for implementing certain functions such as a “Title Verifier”/“Legal Verifier” required by the Standard.
Ex. You want to tokenize your car: Create an RWA token of type Commodity. Implement the “Title Verifier” function (and whatever else might be required by the Standard) and you’re good to go.
Would love to hear your thoughts on this!

[Odai]: Great input and deep need for this in the ecosystem.
Some thoughts:
Claims should be tracked &amp; proven by providing PDF or link to county’s recorder office. A PDF should suffice of the title report and/or deed.
Regarding clawbacks, I think adding a freeze element would also be beneficial in the case where manual verification is needed.
How would you handle different regulatory requirements for a single offering? (Accredited Investor, Non-acccredited, Non-US) Specifically for different transfer restrictions for each? How are you verifying the end-buyer is confirming / acknowledging their acceptance of a restricted security token?
Appreciate the pioneering efforts to bring RWA’s to Solana!
Props &amp; Love to the Homebase Team
Odai - LiquidProp

[JoakimEQ]: Hey Dom, thanks for the well thought-out post. This is a great opener for what I think will be the killer app of Solana DeFi. RWA’s need fast chains. They are currently severly hampered by slog that is the EVM.
That being said, there are a few areas in your proposal that I’d like to comment on, most of which have also been pointed out by other members of this forum:
Real-Estate Focus - The language in the post appears to be largely tailored to Real Estate and might not fully encapsulate the wide range of possibilities that RWAs present. As we’ve observed, tokenized funds have gained considerable traction and could be the current primary market fit for RWAs on the blockchain.
USA Focus - The proposal seems to focus heavily on the US jurisdiction, an environment we know is susceptible to frequent and substantial regulatory changes. Rather, I suggest we consider areas where blockchain standards already have governmental support, like the EU.
Usage of Oracles - I recognize the need for updatable tokens/NFTs, but this brings up important security considerations that might pose obstacles to the broader adoption of the standard.
Regarding your closing queries:
I agree that we should take into account various regulations and perhaps work towards a flexible, jurisdiction-agnostic “category-based” model.
Title claims seem simple to track, at least based on my tech knowledge and talking with the people behind ERC-6065 (the EVM RWA standard). As each property would be a unique token, the tracking is inherent to the chain.
Ownership of a unique token could indeed be used as proof of ownership for the underlying asset.
While uploading data for title claims poses challenges, I believe that current developments in decentralized storage solutions, such as Filecoin and Arweave, show promise in addressing this issue.
I would be thrilled to contribute further to these discussions. We have extensive experience in developing a MiCA-compliant stablecoin, a general EU-based tokenization framework, and have contributed to some Solana-based projects in the past. Additionally, I am also deep in the RWA scene on Ethereum, and have been following and discussing the ERC-6065 spec for some while now.
Please feel free to reach out to me at joakim(at)equilibrium.co or via Telegram at @jommi to continue this conversation. Would love to collaborate!

[Dom]: Thanks for the feedback Joebuild!
I started this post with a focus on real estate, but after multiple discussions with other teams, it’s clear we need a more general RWA standard that can then be customized for different verticals (trading cards, real estate, commodities, etc)
Completely aligned that for the specific real estate vertical, there needs to be flexible approach to field input

[Dom]: Thanks for the feedback Odai!
Agreed that claims should be tracked and proven either via PDF (ideally a document with some level of “official” stamp) or link to county recorder’s office
Freeze element would be crucial, especially to comply with US securities regulation
With the assumption that the RWA standard would just be the framework for how to tokenize physical assets, depending on the jurisdiction, a regulatory wrapper would be needed specific to the country that you’re issuing the token in. In the case of the US wrapper, the issuer would still need to make sure they’re complying with US securities regulation when issuing the token (whether you sell to accredited investors only, non-accredited, etc). The RWA Standard would then ensure that you to have the right tools at your disposal to comply with different US securities regulation (For example: In the case of a Reg D filing, the tokens would automatically be frozen for 1 year after initial sale, etc). In addition, after the sale, the RWA Standard should ensure that the security tokens comply with Rule 144 (US specific).
In terms of verifying that the end-buyer is confirming / acknowledging their acceptance of a restricted security token, it’s likely that trading of security tokens (at least in the US) is primarily done on a permissioned DEX or CEX. The reason is that some entity needs to perform KYC on each user to comply with US regulation, and likely needs an alternative trading system license (ATS) to allow for P2P trading.
If you have any other thoughts on this, or disagree with any of my statements, would love the feedback! Very much thinking out loud here based on how I believe it’d work based on multiple discussions with lawyers, other teams, etc.

[Basile]: Hey Whisky, thanks for being a part of the discussion!
I agree that reliance on oracles would be a problem. In the initial state of the standard I envision document upload (title/deed + SEC docs if securities) to be done in a similar manner to NFT data. This would mean that issuers are responsible for uploading the correct documentation. In an ideal world we have some sort of Oracle provided by government or with government oversight that allows us to undeniably verify the validity of the uploaded documents. I would love more discussion around this piece!
Wallet security – this is an interesting one. It’s important to note that ideally the token truly represents legal title/ownership. Unfortunately there may be no way to universally guarantee that every token issuer has done the proper legal filing to support that without the involvement of governments/other third parties. To me, this means that it is very likely a decision that the issuer of the token will make as opposed to the standard per say. Security tokens require, legally, the capabilities for tokens to be voided and re-issued. How and when you do that is completely up to the issuer (ideally the issuer has a process in place for how they identify the legitimacy of a hack).
KYC: At it’s core the standard would be a Permission Token. If issuing the token as a security, issuers would have the ability to whitelist holders upon completion of KYC through their own third-parties. @yashhsm shared a flowchart below that showcases this! This would dictate who can perform what financial actions such as transfers, receipt of dividends, etc…
With regards to UCC, in an ideal state, yes defaults should trigger instant title transfers. Especially if loans are made on-chain. However, absent government adoption, this is likely not possible in countries that wouldn’t enforce this title transfer legally.
My takeaway from several discussions is that the standard itself likely cannot make guarantees on the validity of the documents uploaded without the involvement of government bodies (ex. SEC allowing us to verify the validity of security filings on-chain for a certain RWA token). These processes also vary from locale to locale, therefore making it difficult to build a standard that satisfies everyone, everywhere. What we can do is create a standard that defines a structured set of data requirements (documents to upload, parameters to set, etc…) and functional requirements that are required amongst most/all locales and leave it to issuers to build systems on top of that. Hopefully as time goes on that standard evolves and gets support from government bodies to achieve that aforementioned ideal world scenario. In short, I think it’s important that the standard remains robust and quite open to implementation by the issuer, so that it can be used in various locales/jurisdictions.

[Sherwood]: Thanks, Dom!
Agree with @yashhsm above - we should also be looking to create a more universally programmable security token asset/standard that many different types of ST/RWAs can be tokenized with (equity, debt, rev shares, etc.). The tracking of deed ownerships and even geodata associated with a property is interesting AND I don’t know that there is a market asking for this…
Token 22 was a good start and an even more adept ‘transfer restricted token standard’ or ‘security token’ standard can address RE RWAs and a broader spectrum of assets.
On NFTs, I have experience in security non-fungible token (sNFT) and while they’re novel for delivering financial upside to investors based on more origination (the creator) and copy/IP rights, they are far less adept than fungible assets when wanting to build (1) deep and high volume secondary markets and (2) composable finance. You can still have things like dividends, royalties and profit-taking distributions track against fungible token supplies all the same.
So, if we’re wanting a non-fungible RE specifically, this can be fine. I am unsure of the market size or problem being solved with such an asset standard. If we’re wanting to deliver a security token standard that is broadly configurable can create composable financial markets a la ERC-1404, I recommend going towards… that. This would open up far more developer contribution and value generation on Solana.

[roinevirta]: Hello Dom,
Thank you for initiating the discussion. I strongly echo @JoakimEQ’s sentiment; a flexible, high-level data model with optional fields would likely serve issuers &amp; other counterparties better globally.
Membrane Finance builds technologies to facilitate hybrid financial markets. We are currently bringing a MiCA-compliant stablecoin to Solana and we would be happy to contribute to the development of the new standard through our learnings and experience.
Please feel free to reach out to me at juuso.roinevirta(at)membrane(dot)fi or via Telegram at @roinevirta – we would be happy to collaborate and ensure global compatibility.

[ekryski]: @Sherwood great commentary!
 Sherwood:
they are far less adept than fungible assets when wanting to build (1) deep and high volume secondary markets and (2) composable finance
Genuine question, what makes you say this?
IMHO this is not necessarily true, but more a function of how the DeFi markets have developed thus far. Specifically, that all systems have historically been built to simply pool assets because it’s “how things are done”. I’ll admit it is certainly easier and often also cheaper for compute, but I think primarily this is a result of historical implementation baggage that originated with Ethereum and have been perpetuated over time.
Somewhat recently, from many of the more established players on Ethereum we’re seeing an evolution where platforms are creating more finely grained “pools” or other work arounds to solve for compliance and risk mitigation needs. Curve, Uniswap, Aave (and more) have all been taking these approaches which imho could be better solved by either ERC-1155 or ERC-1404 (or some improved smart contract designs around ERC-20 and ERC-721). I’ve long felt this way and did some early implementations 4-5 years ago but only recently started exploring this again more earnestly.
Curious if I’m missing something. Happy to take to DMs to prevent polluting here.

[Sherwood]: Love this thinking and feedback. When we think about 1155, that can be a great way to conceptualize indexed fungible assets associated with one originated (using securities language) asset.
I think that you can absolutely go down these routes and, bc you want to develop markets* MORE than wanting to create just more assets* and fungibility is key to this. Making the standard more fungible-capable open at first would just be a simpler place to start and would accomplish the markets creation.

[Cutler]: Hi Dom,
This is great stuff. I am a corporate/governance lawyer from Australia, working a digital legal contract management solution with self-building company asset registers. I think this could be the link for crypto RWA’s to find retail mass market adoption (i.e., an app that sits above the blockchain layer with an easy and familiar enterprise user interface). I have been looking for an RWA project that fits with this and what you are describing is perfect. Can we connect somehow to discuss working on minting RWA tokens for this platform as transactions flow through it?.
I am new to this forum so not sure how to go about this.
Any thoughts (anyone?)

",Dom,4130,6,14,15,2023-07-12T15:01:09.312Z,2024-04-30T05:46:11.986Z,sRFC,6,2024-04-30T05:46:11.986Z
1245,sRFC 22 - Extending support for mobile wallets in the Wallet Adapter,https://forum.solana.com/t/srfc-22-extending-support-for-mobile-wallets-in-the-wallet-adapter/1245,"Mobile Wallet Support - Wallet Adapter
Summary
Currently, the adapter’s support for wallet connection support is sub-optimal. Users that start a session within a mobile browser will lose session context when asked to connect their wallet. Existing wallets utilise deep-linking within the adapter to route the user to their wallets.
This however does not work all the time - there are instances whereby the user is re-directed to the app-store. On success, the user will lose session context most of the time and have to re-start their session within the in-wallet browser functionality thus leading to a poor experience for mobile users.
To solve for this, we propose to add a standardised interface within the wallet adapter that will handle mobile wallet connections utilising a third party to facilitate adapter to mobile wallet adapter communications in a similar fashion to how WalletConnect facilitates this on EVM based dApps.
Implementation:
This is a simple reference point of how this flow would look like, curious to get thoughts from wallets / dApps on what they think about this.
At a high-level, the interface would extend the existing connect functionality such that if the user is selecting a wallet and they are on mobile, the third party service provider would facilitate the mobile wallet to adapter communication.
All communication would pass through the third party on mobile and act as a relayer between the dApp and the mobile wallet.
Here is a sample diagram of how I imagine it’d look like:
 
 
 Whimsical
 
 
 
Wallet Standard - Extended
 
Whimsical combines whiteboards and docs in an all-in-one collaboration hub.",,ZeroTimeDrift,457,0,0,1,2024-03-21T14:26:53.036Z,2024-03-21T14:26:53.152Z,sRFC,6,2024-03-21T14:26:53.152Z
1027,Update Base Fees,https://forum.solana.com/t/update-base-fees/1027,"Title: Updated Base Fees
Summary
I had written an article about base fees and how the base fees should look depending on the cluster size.
 
 
 Notion
 
 
 
Notion – The all-in-one workspace for your notes, tasks, wikis, and databases.
 
A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team
 
 
 
 
 
 
I want to hear from the community about their opinion. There are multiple approaches to change the base fees.
 → Constant base fees
 → Linear base fees
 → Brackets based base fees
 → Exponential base fees
I will explain the pros and cons of each in a few words.
Why change base fees ?
Currently, base fees are constant 5000 lamports / signature. So cost of executing transaction on the cluster is measured in SOL than in USDC. If SOL becomes more expensive, then it will be more expensive to execute transactions for the user, while if SOL becomes less expensive, then it will be harder for the validator to keep running the cluster.
The base fee is constant and is not based on the block space used by the transaction. So user pays the same base fee if they execute transactions consuming 10K CUs, 100K CUs or a million CUs. Base fees should consider the resources used to execute the transaction on the cluster. This makes everyone send transactions requesting the maximum CU possible of 1.4 million CU. If transactions are not correctly estimated for consumed CU, the scheduler finds it difficult to schedule the transactions and fill the block completely, wasting some of the block space.
By changing the base fees based on CU requests, we can calculate the resource usage in CU appropriately and charge the user based on resource utilization.
Who will control the base fees
The base fees could be controlled by DAO or Multisig, giving them control over one or multiple variables. Various communities, validators, and developers can be part of the DAO. The current proposition does not consider this part, as it could be figured out later.
Requirement of new base fees
Should be based on resources used by the transaction.
Optionally discourage spamming.
Optionally encourage optimization of code by the programs.
Should be in interest of “most” of the community. (i.e should not be too expensive for user nor punitive to run a validator)
In general we have to find a middle ground to encourage dapp devs to write optimized programs, for users to send transactions with better estimates for resource use at the same time not discourage existing users to use Solana. It should also encourage validators to pack the block the fullest i.e consuming 48 M CU.
Impact
This proposal Validators, Dapps and everyone who uses Solana cluster to send and execute transactions.
New Base Fees
Constant base fee
Formula:
Base Fee = constant value
Pros:
Easy to calculate fee for transaction.
Already implemented just need to update the constant value.
Cons:
Does not discourage spamming of network.
Does not discourage large transactions consuming lot of CUs.
Does not encourage optimization of the programs.
Does not represent resource utilization.
Linear base fee
In this model base fee will be linearly dependent on the CU requested with a minimum floor value of the base fees. The rate at which base fee will increase will be called rate of change of base fees or just Rate.
Formula:
Base Fee = round(max( Minimum Base fee value, Rate * CU requested ))
Pros:
Relatively easy to calculate.
Encourages optimization of programs.
Encourages sending transactions with proper CU estimates.
Rate can be controlled by DAO or MultiSig.
Rate can be changed according to SOL/USD oracle price.
Depending on MinValue, can incentivize validator to prioritize smaller transactions.
Cons:
Hard to find correct Rate which will please everyone.
Lower value of rate will not discourage sending of large transactions nor encourage program optimization.
Higher value of rate may discourage users away from solana.
Bracketed base fee
Formula:
Base Fee = (0 to bracket_1) * rate 1 + (bracket_1 to bracket_2) * rate 2 …..
Pros:
Relatively easy to calculate.
Encourages optimization of programs.
Encourages sending transactions with proper CU estimates.
Can disincentives sending of large transactions.
Cons:
Any multisig or DAO will have multiple parameters to set.
Can have some weird behaviors.
May need some different burning rates for different rates.
Exponential base fee
Consider a multiplier multiplier that has to applied to a certain CU CU multiplier (i.e we want transaction requesting 1 million CU pay 5 times the minimum value then multiplier is 5 and CU multiplier is 1 million).
Formula:
Base Fee = round( Minimum Value * multiplier ^ (CU requested / CU multiplier) )
Pros:
Encourages optimization of programs.
Encourages sending transactions with proper CU estimates.
Rate can be controlled by DAO or MultiSig with 3 params.
Rate can be changed according to SOL/USD oracle price.
Will disincentivize spamming and heavy transactions.
Incentivizes sending of smaller transactions.
Cons:
Relatively hard to calculate.
Will make users to split transactions doing multiple instructions into multiple transactions.
Need to update burn rate so that validators do prefer executing heavy transactions.
Relatively hard to get consenseus over the variables.
As a community I would like to have your opinion .",,gmgalactus,460,0,0,1,2024-02-07T09:41:21.307Z,2024-02-07T09:41:21.383Z,sRFC,6,2024-02-07T09:41:21.383Z
949,sRFC 21 - Nested Account Resolution,https://forum.solana.com/t/srfc-21-nested-account-resolution/949,"Code: GitHub - ngundotra/srfc-21-nested-account-resolution
SRFC 21 - Nested Account Resolution
Summary
This sRFC introduces a standard protocol to enable on-chain and off-chain derivation of accounts needed to complete a transaction, allowing the definition of account-free program interfaces.
Motivation
Composing programs on Solana is hard because it is difficult to imagine being able to swap out any program for any other program. So program developers only call out to one program at a time. This process leads to hard-coded composition paths.
This is one of the most frustrating aspects of Solana programming. This specification aims to solve this by providing a base for community members to decide on program interfaces.
Background
Defining program interfaces requires specifying accounts and instruction data. Interfaces with a strict list of accounts will restrict program implementation, because programs may be required to put all their information into a few accounts that fit the interface description. Interfaces defined with a strict list of accounts restricts innovation in program development, and fails to take advantage of the SVM’s composability.
We want to enable interfaces to specify required accounts without restricting additional account usage. We believe this will allow exciting developments of innovation with program interfaces.
Two problems with this
How can programs specify additional accounts they need for a specific instruction?
If a program is CPI-ing into a unknown program that requests additional accounts, how can it expose those requested accounts, and then reliably pass such accounts to the unknown program?
Specification: Nested Account Resolution
We propose to solve these two problems by allowing programs to request additional accounts for an instruction with an additional instruction that requests accounts using return data.
Off-chain clients can simulate the additional instruction, parse the return data, and then construct a valid transaction for the program’s instruction using the requested additional accounts.
As of right now, this specification requires 3 things:
Program must have an Anchor IDL deployed on-chain
Instructions that request additional accounts must have 8-byte instruction discriminator derived from their name and a corresponding entry in the program’s anchor IDL.
The additional instruction that requests additional accounts must also have an 8-byte instruction discriminator derived from the target instructions name, and a corresponding entry in the program’s anchor IDL.
These additional instructions are called aar instructions, e.g. additionalAccountsRequest, or additionalAccountResolution.
Let’s go through how these are meant to be used before diving in any deeper. Our examples will focus on programs built with the anchor framework, since it meant to improve program composability and transparency on Solana.
Additional accounts request instructions, when implemented look like the following
use anchor_lang::prelude::*;
use additional_accounts_request::AdditionalAccounts;
use solana_program::program::set_return_data;
#[program]
pub mod my_program {
 use super::*;
 pub fn transfer(ctx: Context&lt;Transfer&gt;, args: TransferArgs) -&gt; Result&lt;()&gt; {
 ...
 }
 pub fn aar_transfer(ctx: Context&lt;TransferReadonly&gt;, args: TransferArgs) -&gt; Result&lt;()&gt; {
 let mut requested_accounts = AdditionalAccounts::new();
 ...
 requested_accounts.add_account(&amp;pubkey, is_writable)?;
 ...
 set_return_data(bytemuck::bytes_of(&amp;additional_accounts));
 Ok(())
 }
}
Off-chain clients can compose a valid TransactionInstruction for transfer by simulating aar_transfer with TransferReadonly’s accounts successively until requested_accounts.has_more is false. Each successive simulation of aar_transfer must include the accounts requested in the previous simulation. This is to allow for account derivation logic that requires account data, which is only available in simulation when the account is provided.
The typescript pseudo code looks like:
// Create base AAR instruction
let instruction: TransactionInstruction = await myProgram
 .methods
 .aarTransfer(args)
 .accounts({
 ...
}).instruction();
let originalKeys: AccountMeta[] = instruction.keys
let additionalAccounts: AccountMeta[] = [];
while (hasMore) {
 // Add previously requested accounts to instruction
 instruction.keys = originalKeys.concat(additionalAccounts.flat());
 // Simulate instruction and get return data
 let returnData = await simulateTransactionInstruction(
 connection,
 instruction,
 );
 // Deserialize return data into the Additional Accounts
 let additionalAccountsRequest = AdditionalAccountsRequest.fromBuffer(returnData);
 // Store requested accounts
 hasMore = additionalAccountsRequest.hasMore;
 additionalAccounts = additionalAccounts.concat(additionalAccountsRequest.accounts);
}
Here’s the structure of the return data to be set by any additional accounts request method.
pub const MAX_ACCOUNTS: usize = 30;
#[zero_copy]
#[derive(Debug, AnchorDeserialize, AnchorSerialize)]
pub struct AdditionalAccounts {
 pub protocol_version: u8,
 pub has_more: u8,
 pub _padding_1: [u8; 2],
 pub num_accounts: u32,
 pub accounts: [Pubkey; MAX_ACCOUNTS],
 pub writable_bits: [u8; MAX_ACCOUNTS],
 pub _padding_2: [u8; 26],
}
We chose AdditionalAccounts to have the maximum number of account meta information possible, while still keeping the total size under 1024 bytes (maximum amount of Solana return data).
An additional requirement was keeping the struct byte-aligned so that the entire struct could be used with zero copy to excess heap usage.
Passing exposed accounts requested by CPIs
The hard part here is knowing which of the requested accounts should be used for a given CPI, especially if you have multiple CPIs to unknown programs (e.g. swapping ownership of assets between two programs).
We propose a low compute solution that uses an “account delimiter” to group account segments together of variable length. The account delimiter for a program is a PDA with seeds &amp;[""DELIMITER"".as_ref()].
An example swap implementation is given here:
pub fn swap(ctx: Context&lt;Swap&gt;) -&gt; Result&lt;()&gt; {
 ...
 // Track consumed accounts
 let mut delimiter_idx = call(
 ix_name_0,
 cpi_ctx_0,
 args_0,
 get_delimiter(&amp;crate::id()),
 0
 )?;
 
 // Filter out consumed accounts
 delimiter_idx = call(
 ix_name_1,
 cpi_ctx_1,
 args_1,
 get_delimiter(&amp;crate::id()),
 delimiter_idx,
 )?;
 Ok(())
}
Exposing accounts requested by CPI
pub fn preflight_swap(ctx: Context&lt;SwapReadonly&gt;) -&gt; Result&lt;()&gt; {
 # find number of account delimiters
 let mut latest_delimiter_idx = 0;
 let mut stage = 0;
 ctx.remaining_accounts
 .iter()
 .enumerate()
 .for_each(|(i, acc)| {
 if acc.key() == delimiter {
 stage += 1;
 latest_delimiter_idx = i;
 }
 });
 # Based off # of delimiters, I can derive how many CPIs have finished requested account
 match stage {
 0 =&gt; {
 # CPI to `aar` instruction for unknown program A
 # pass all undelimited accounts to program A
 if done { 
 addtional_accounts.add_account(&amp;get_delimiter(&amp;crate::id()), false)?; 
 }
 }
 1 =&gt; {
 # CPI to `aar` instruction for unknown program B
 # pass all undelimited accounts to program B
 }
 _ =&gt; {
 msg!(""Too many delimiters passed"");
 Err(ProgramError::InvalidInstructionData.into())
 }
 }
}
Limitations
This spec does not support requesting additional signer accounts.
Allowing unknown programs to request the signature of accounts could be a security vulnerability, and would be technically challenging to implement. So supporting the signer accounts must come in the form of a new specification.
Resolving additional accounts can be slow
Successively simulating the aar instruction with results of the last call can be quite slow, and there is no maximum number of iterations defined by this specification. There is no current guide or set of heuristics on how to cache requested additional accounts yet either. This means that applications may see increased RPC calls for simulateTransaction and increased latency when showing end-users transactions.
Future Work
Anchor macros to build aar instructions
It is possible to design macros that build aar instruction at compile-time entirely from the anchor instruction struct and a list of CPI callsites. We hope that if the community adopts this sRFC, additional developer tooling will be made available here.
Support for state compression
Once this specification proves valuable, it would be quite possible to support state compression, since the proofs to a ConcurrentMerkleTree only require a list of accounts. However, doing so would require referencing off-chain indexers. This seems best suited for different spec and protocol version of AdditionalAccountsRequest.
Faster account resolution
It seems quite possible to write a thin wrapper around simulateTransaction that uses Geyser to stream account updates to Solana Banks Test, so that transaction simulations are faster and the results can be cached for quicker lookup. This would require a lot of work, but this would probably greatly increase adoption speed.
Code
Library with helper functions for implementing this spec is available at: docs.rs/additional-accounts-request
Integration tests for the library are available at GitHub - ngundotra/srfc-21-nested-account-resolution: Nested account resolution library, PoC, examples, and documentation with yarn install &amp;&amp; yarn test.",,ngundotra,585,0,0,1,2024-01-15T12:31:12.154Z,2024-01-15T12:31:12.231Z,sRFC,6,2024-01-15T12:31:12.231Z
149,sRFC 00011: A smart contract that allows for easy storage and retrieval of data on-chain,https://forum.solana.com/t/srfc-00011-a-smart-contract-that-allows-for-easy-storage-and-retrieval-of-data-on-chain/149,"On-chain Data Storage
Summary
A smart contract that allows for easy storage and retrieval of data on-chain
Motivation
Currently in Solana there is no standard for data storage on-chain. By data, I mean anything that can be stored as bytes like a text file, a HTML document, a PNG image, NFT JSON Metadata etc. This RFC proposes a solution for storing data on-chain and having it be retrieved easily.
Implementation
To solve this issue, I have developed the Solana Data Program. This is a Solana smart contract that allows users to initialize a Data Account to store any data as bytes and have its metadata be stored in a separate PDA
Flow Diagram
Here’s a flow diagram to describe how the accounts are linked:
Accounts
Metadata Account:
In the Metadata PDA Account, we store the metadata regarding the data in the Data Account. A few important fields stored in it are:
authority: The authority needs to be a signer in any instruction that involves the Data Account - updating the data and/or data type, finalizing the data, transferring the authority, and/or closing the account
data_type and serialization_status: Currently the supported data types are:
i. CUSTOM: To store custom data or a currently unsupported data type
ii. IMG: To store image data as raw bytes
iii. HTML: To store a HTML file as raw bytes
iv. JSON: To store minified JSON as raw bytes
The motivation behind having set data_types is that it helps the client application determine how to display the data. It also opens doors for data verification (denoted by serialization_status). Currently when JSON data being updated, the user could optionally verify to ensure it is valid JSON.
dynamic: A dynamic Data Account can realloc (up or down) while a static account will always stay a fixed size. This could be useful when users want to pay a fixed amount for the storage space (static) in the Data Account rather than a only-pay-how-much-is-needed approach (dynamic)
Data Account:
A Data Account is any Solana account that is owned by the Data Program and has an associated Metadata PDA Account. The data is stored directly as raw bytes so one could easily retrieve it using a single getAccountInfo RPC call as such:
const data = (await connection.getAccountInfo(dataKey, commitment)).data;
The data account need not be created by the Data Program. You can also pass in a previously created Data Account to the Initialize instruction and it will assign it to the Data Program
Instructions
InitializeDataAccount: This instruction creates and initializes the Metadata Account and optionally creates a Data Account.
UpdateDataAccount: This instruction updates the data_type field in the Metadata Account and the data in the Data Account.
UpdateDataAccountAuthority: This instruction updates the authority of the Data Account by updating the value in the Metadata Account. It requires both the old and new authority to be signers to prevent accidental transfers.
FinalizeDataAccount: This instruction finalizes the data in the Data Account by setting the data_status in the Metadata Account to be FINALIZED. Finalized data can no longer be updated.
CloseDataAccount: This instruction closes the Data Account and the Metadata Account and transfers the lamports to the authority.
Usage
SolD Website
SolD is a website that acts as an editor for the Data Program:
It allows users to view the metadata and data associated with a Data Account via the /&lt;datakey&gt;?cluster=&lt;cluster&gt; route
It allows users to connect with their wallet and upload files directly to the Data Program by going to the /upload route
If the user is logged in as the authority, it allows the user to edit the data, finalize it and/or close the metadata and data accounts.
Users can also view all data accounts they “own” via the /authority/&lt;authority&gt; route and perform group actions on them
Typescript SDK
solana-data-program is a Typescript SDK that exposes APIs to interact with the Data Program and helper methods to parse the data, metadata etc. of a Data Account
NFTs
One potential use case of the Data Program are fully on-chain dynamic NFTs. By this I mean an NFT with:
JSON metadata stored on-chain
Image data stored on-chain
JSON metadata that can be updated via an on-chain crank
Image data that can be updated via an on-chain crank
Here’s a link to such an NFT: Quine NFT
On clicking View original you will see the original HTML file that was pulled from on-chain. You can also inspect the Metadata and Image data on SolD separately:
Image: HoyEJgwKhQG1TPRB2ziBU9uGziwy4f25kDcnKDNgsCkg
Metadata JSON: Hb9vkWax5AeLWvCtYSjSvWrN6gTw324gKMa28kcBsgT3
P.S. The NFT image is a quine. The code on the surface of the rotating sphere is the code used to generate the sphere with code on its surface
Composability
An important consideration went into making sure that the Data Program is easy to use and composable. To demonstrate that, I have also made example smart contracts (two of which involve minting NFTs including the Quine NFT ) that CPI into the Data Program: solana-data-program/examples at main · nvsriram/solana-data-program · GitHub
URI Standard for Data Retrieval
Currently, to have the data be pulled from on-chain I have an API route /data/&lt;dataKey&gt;?cluster=&lt;cluster&gt;&amp;ext=&lt;ext&gt; that just returns the data as is (or in the extension format specified by ext). It would be more handy to have a URI standard which might look something of the sort:
sol://&lt;datakey&gt;?ext=&lt;ext&gt;
to get the data stored in datakey in the format specified by ext
OR
sol://meta/&lt;datakey&gt;
to get the metadata associated with the datakey
Conclusion
This RFC discusses the features of the Data Program and how it can be used to store data on-chain. It presents the SolD website editor and Typescript SDK that make it easy to interact with the Data Program. It showcases potential use cases in fully on-chain dynamic NFTs and proposes a URI standard for data retrieval.
References
Data Program Smart Contract: GitHub - nvsriram/solana-data-program: Solana smart contract that handles on-chain data storage
SolD Website Editor: https://sold-website.vercel.app/
Typescript SDK: solana-data-program - npm
Quine NFT: Solscan
Quine NFT Image: https://sold-website.vercel.app/HoyEJgwKhQG1TPRB2ziBU9uGziwy4f25kDcnKDNgsCkg?cluster=Devnet
Quine NFT Metadata: https://sold-website.vercel.app/Hb9vkWax5AeLWvCtYSjSvWrN6gTw324gKMa28kcBsgT3?cluster=Devnet
Examples: solana-data-program/examples at main · nvsriram/solana-data-program · GitHub","[joec]: Hey, this is actually really cool!
I have some qualms over the potential for this to be widely adopted as a standard. Instead, I could see this being either a protocol itself for data storage or a program library that can add a layer of abstraction over the management of the accounts/metadata.
How do you envision this being used as a standard?

[nvsriram]: Thank you, I am glad you like it! :))
I do agree that its role as a protocol or program library are more straightforward with how it currently is and it also makes it easier to adopt that way.
However, I do think that a standard for general data retrieval would be quite useful. The idea I had for this was to use the URL format to make any data stored on Solana easily accessible. This would be the same idea as with Solana pay URL and could also be an extension to the same solana URI scheme (would save the effort of having to register a new URI scheme).
But the end result of this would be that any user could just use this URL format and paste it in a browser to get the data associated with that data account. All the dApps would have to do to be compliant with this is parse the datakey part of the URL (sol://&lt;datakey&gt;) and return the data using a simple line of code like so:
return (await connection.getAccountInfo(dataKey, commitment)).data;
If this URL format were to be adopted as a standard, any user could “upload” their data directly into a data account and have it be easily retrieved via a simple URL. And because the data can be verified when uploaded, it has exciting interactions, say with validating NFT JSON metadata.

[Hamster]: This is really cool. I think its important to have a website to pull account data and has a lot of uses for on-chain nfts.
I have tested it out with my own on-chain nfts and I can pull the raw data but it does not parse correctly due to usage of anchor (hence the raw data will have bits in front). Any way for the implementation to detect data:image/URLs or “{” (used in json) and start parsing from that byte?

[nvsriram]: Thank you!
I didn’t really test it out with anchor so didn’t run into the bit padding issue with JSON so currently it just removes all the whitespace bits (that could be introduced when the account is resized etc.) and tries to parse as JSON. But like you mentioned, changing the implementation to parse data between first ‘{’ and last ‘}’ should be fairly straightforward and could be a easy fix.
As of right now, it should still display the invalid JSON in the error or in the HTML/CUSTOM data type (although not as nice).
As for reading data:image/URLs, that would be a bit tricky. Currently, if the data type is said to be of type image, it returns the raw bytes along with the appropriate content type. The website (under HTML datatype) would pass the URL returning the raw bytes to an iframe as its src. For it to read it as intended, it would need to pass the raw bytes as the URL instead.

",nvsriram,1838,2,4,5,2023-04-27T17:03:10.159Z,2023-10-01T17:46:34.192Z,sRFC,6,2023-10-01T17:46:34.192Z
402,Bank Capture File,https://forum.solana.com/t/bank-capture-file/402,"Summary
Resolving bank hash mismatches between different validators and validator releases is an arduous process.
The most widely used approach involves
dumping bank hash pre-images to the validator log files (shared with arbitrary other log output)
using a log parsing tool to extract information
accumulating the changes every slot, then constructing a diff
Background
The bank hash commits to the execution inputs and state changes of a slot
A bank hash mismatch occurs when two Solana runtime implementations output different bank hashes for the same inputs (same state, same slot)
This implies that these two runtime implementations are incompatible, which is a severe bug that has to be fixed
The bank hash pre-image refers to the raw inputs fed into the hash function
This pre-image is highly useful for debugging, as it pin-points the input that is different
There is no standard for encoding the pre-image; All solutions so far rely on hacks that are incompatible across different validator code bases
Requirements
The following pseudocode describes the declarations of the hash constructions part of the bank hash.
account_hash := blake3 {
 le u64 lamports
 le u64 slot
 le u64 rent_epoch
 []u8 data
 u8 executable
 [32]u8 owner
 [32]u8 key
}
accounts_delta_hash := merkle {
 leaf = [32]byte account_hash
 branch = sha256 {
 [1..=16][32]byte node
 }
}
bank_hash := merkle {
 leaf = [32]byte account_hash
 branch = sha256 {
 [1..=16][32]byte node
 }
}
The solution must be able to serialize all of the above data in a language-agnostic format. There should be consensus among validator developers, and every team should be willing to implement and work with this format.
The serialized size is estimated to be hundreds of megabytes per slot.
Therefore, the serialization scheme used should also be efficient.
Stretch Goals
Ideally, this file format should support streaming use and compress well.
Perhaps, we could wrap the Protobuf blobs in a binary container format, such as .tar.zst or a custom format.
Possible Solutions
Designing a data structure representing the above information is trivial.
It is not obvious which serialization scheme should be used however.
JSON
steviez at Solana Labs has been working on a JSON-based solution.
This format can be easily upgraded, but we’d argue it is a little too free form, and does not offer great performance.
Custom Binary Format
I’ve worked on a custom binary format for maximum performance.
There are a number of obvious shortcomings:
It is not easily upgradable
It is more difficult to implement and debug
Protobuf
After meeting with the Firedancer team on this topic, we settled on the mix between the above two. A Protobuf schema can be upgraded just like JSON structures, but it also features powerful cross-language tooling, a schema language for coordinating these upgrades, as well as decent performance. Finally, Solana validators already use the Protobuf stack for RPC.
We would like to request comments from client developers, and invite validator developers to collaborate on a solution.","[ripatel-jump]: Published a draft of the file format here: Initial solcap API by ripatel-fd · Pull Request #543 · firedancer-io/firedancer · GitHub

",ripatel-jump,545,0,1,2,2023-07-24T15:12:31.551Z,2023-07-24T23:38:12.686Z,sRFC,6,2023-07-24T23:38:12.686Z
104,sRFC 00009: Sign-In with Programmable (Smart) Wallets using Off-Chain Delegates,https://forum.solana.com/t/srfc-00009-sign-in-with-programmable-smart-wallets-using-off-chain-delegates/104,"Summary
This SRFC proposes an application standard for authenticating users of dapps, specifically those using Programmable Wallets (a.k.a Smart Wallets or Smart Contract Wallets), through the use of off-chain delegates. This standard addresses the challenge of authenticating users who interact with an app using a Programmable Wallet, which does not have a private key for signing messages. By enabling Programmable Wallets to appoint a delegate with a private key to sign messages on their behalf, a secure and verifiable way to authenticate users is achieved.
Introduction
A lot of dapps use the authentication process called off-chain message signing. The mechanism takes advantage of the fact that user keypair can be used to cryptographically sign any arbitrary messages such that anyone can verify that a message is signed by the party controlling the private key for the corresponding public key. The algorithm for this process is as follows:
The app produces a unique message and gives it to the user (wallet) to sign.
The user signs it and returns the signature back to the app.
The app verifies that the signature is indeed valid and is for the message it provided originally.
If true, the app issues an auth token that the client can use for future interactions with the app’s API.
The Problem of Programmable Wallets
Not every Solana address has a corresponding private key. A good example is Programmable Wallets - accounts owned by a Solana program that is typically controlled by one or many regular (keypair) wallets and interacts with the chain following the rules of the program.
For example, a multisig program owns a vault account shared by multiple owners of the multisig, tracks the status of voting on proposals, and allows executing the proposals once they are approved by the quorum. The vault account has a public key but lacks the private key; the program programmatically “signs” on its behalf.
How can we authenticate a user who uses a Programmable Wallet address to interact with a dapp?
Proposed Solution
Introducing Off-chain Delegates.
So Programmable Wallet accounts cannot sign a message off-chain as they lack a private key. However, they can appoint a delegate - a regular account that has a private key - authorized to sign messages on behalf of the Programmable Wallet account. This delegation can be fully registered on-chain, making it verifiable by anyone.
Let’s take a look at how the sign-in flow would work when modified to support Programmable Wallets:
The app produces a unique message and gives it to the user (Programmable Wallet) to sign.
The wallet knows which address controls the Programmable Wallet account and signs the message with the delegate keypair. It then returns the signature along with information about the Programmable Wallet account and delegate addresses to the app.
The app verifies that the signature is produced by the delegate.
The app verifies that there’s a delegation record (DelegateToken account) for the given delegate created by the account on-chain and that it has not expired.
If true, the app issues an auth token that the client can use for future interactions with the app’s API.
Implementation on the Programmable Wallet programs’ side can vary between Programmable Wallets. Each program can decide and implement the mechanism of creating delegate records in whatever way they want. They just need to make a CPI (cross-program invocation) into the Off-chain Delegate Program that manages those records and sign that with the PDA seeds of the Programmable Wallet account they control.
Drawbacks
This mechanism introduces a slight overhead compared to off-chain signing for regular wallets. The overhead is the DelegateToken account that must be created and stored on-chain for each delegate. The current reference implementation, Off-chain Delegate Program, uses 77 bytes, which is about 0.0014268 SOL per account. The rent can be reclaimed when the delegate is removed, but it’s still worth highlighting.
Another problem is additional logic that needs to be implemented on the dapp side. Unfortunately, I don’t see how the mechanism can be implemented entirely in the wallet apps/extensions without explicit support from the dapps. If someone has ideas, please feel free to comment.
Conclusion
The proposed SRFC standardizes the process of off-chain authentication for users with Programmable Wallets through the use of off-chain delegates. Although it introduces some overhead due to the on-chain storage of DelegateToken accounts, this mechanism provides a secure and verifiable way to authenticate users interacting with an app using a Programmable Wallet.
Implementation: Off-chain Delegate Program","[vovacodes]: Here’s a flow diagram that illustrates the proposed process of authentication.
off-chain-delegate-14000×2104 152 KB

[silo]: mentioned this on twitter but i’ll paste my comment here for proper discussion:
What if instead of having a DelegateToken account, there was a standard by which a key could be checked for membership (as a controller, or possibly for a particular role) of a programmable wallet?
This still wouldn’t solve the problem on the dapp side, as a check would still have to be issued, but having a programmable wallet membership verification mechanism could have a broader set of applications aside from delegated login.

[vovacodes]: silo:
having a programmable wallet membership verification mechanism could have a broader set of applications aside from delegated login.
@silo Thanks for your comment. Do you have any specific application in mind. Just curious whether DelegateToken isn’t able to cover that.
The thing is, DelegateToken is such a simple concept that it can be used in many cases. It might make sense to change the name of the account and the program if it isn’t universal enough to fit the use-cases, but in order to do so, we need to understand what those use-cases are.

[silo]: Sure. So in this proposal, the programmable wallet is asking a member wallet to act as a delegate on behalf of the pw. It knows who the delegates are and provides a delegate to the dapp when requested. And it needs to do this every time a delegate is used. I’m saying maybe we can flip that, and basically have a “registry” where anyone can look up the member wallets of pw. Now the dapp can easily verify membership without having to make the request to the pw every time. This also allows dapps to automatically bootstrap member wallets and link them to a pw account and they can observe changes over time and use that information accordingly vs making the request every time and essentially using a “snapshot” of data (just the single delegate at that instant). Could even use a reverse-lookup sort of mechanism, whereby given a member wallet, the pw information can be looked up.

[vovacodes]: First of all, with the current design it is still possible to fetch all the Programmable Wallets (PWs) where the user is a delegate of, it’s a relatively simple getProgramAddresses call with filtering by the delegate field.
vs making the request every time and essentially using a “snapshot” of data
In my view, this is a feature actually. As a dapp, you want to check the current snapshot always, because PWs might remove a delegate (e.g. a compromised key) at any moment.
allows dapps to automatically bootstrap member wallets and link them to a pw account
But what if a user account has multiple PWs associated with it. How would the dapp choose which one to link to? How I see the typical use case is that your wallet should tell the dapp which address to use (cuz that’s your interface with the web3 world), hence the PW → Delegate relationship rather than the other way around.
Maybe this is because I’m having s specific scenario in mind (signing in with a multisig wallet) but I’m having difficulty with understanding why the reverse relationship (Delegate → PW) would be easier to use.

[silo]: Gotcha. So a registry would never have stale data. It would always reflect current membership. The reason for having a reverse lookup (or registry), would be so that the dapp could have a “unified view” of the client. While a user might be using a specific delegate (member wallet) to access a dapp, the dapp may use information in the other wallets. For example, think of a something that has token-gated access. I might have a specific NFT that grants me access to something sitting safely in my hardware wallet, while I use my mobile wallet as a delegate to gain access. (This is actually the specific use case that we have with our gaming NFTs. Kinda like a delegated-login-with-NFT.)
The fact that the member wallet might have multiple PWs associated with it is a good point. For us, we actually don’t allow it and instead use a domain concept (a member can only be linked to a singe PW within a domain).

[vovacodes]: Thank you for sharing your viewpoint. I now completely understand your perspective. It seems that the use-case you mentioned is already being addressed through “Wallet Linking” by several existing dApps, such as Tensor, Magic Eden, and Dialect.
While I agree that this use-case might warrant its own standard, I suggest separating it from the current proposal. This is because, firstly, it doesn’t necessarily require support from the Wallet Standard side, as dApps can independently maintain a list of “Linked Wallets.” Secondly, this feature is already functional for many dApps, in contrast to the present proposal which aims to address a need that is not yet fulfilled by any dApp due to the absence of a standardized approach.

[silo]: Yea, if this is just for a delegated sign-in mechanism then a separate standard would make sense.
Right now the wallet linking most dapps use isn’t done on-chain and usually requires the wallets to be in the same browser as normal browser sessions are used for the linking. So there’s no way to link, say, a mobile wallet to an existing desktop wallet.

[joec]: Hey @vovacodes I’m wondering what you might think of the work we’re doing on sRFC 00007 and how that approach might compare to what you’ve suggested here.
Take a look at my response here and let me know what you think of the spec I added.
In short, I think we have some overlapping approaches to delegating signatures. In my spec for sRFC 00007, I’m obviously targeting encryption keys, but we might be able to accomplish something very similar for delegated wallets using the same functionality but adapted for on-chain registered delegation.
The idea would simply be to use this protocol spec (possibly even the same program?) to store on-chain the delegate keypair so anyone can verify.
I suggest leveraging the same program only because with your particular use case(s), we need only store a Solana address in the PDA, rather than the complex data layout I’ve designed for the Keying program. However, since our use cases are somewhat related (verifying some key/keypair is in fact owned by you and authorized by you), it might make sense to share the same program namespace.
Let me know!

[vovacodes]: @joec, I believe your program addresses this use-case perfectly. I suppose we can use a unique discriminator for the “message signing delegates” and use additional configuration section for adding parameters like expiration, etc

[joec]: Actually I’m curious how important you think the concept of “expiration” is? One potential issue is that’s really only an “honest man’s config”. What I mean is, sure the on-chain program can specify that they key is expired, but anyone querying that information must make the decision at their implementation (client) level to honor the expiration date or ignore it

[vovacodes]: Yeah it’s a fair concern. A pro-expiration argument would be that the proposed process already relies upon the “decision at their implementation (client) level to honor” the account stored on-chain, so expanding it slightly to include expiration doesn’t change the trust model much but adds a bit more flexibility.
I would personally like to hear what other folks in the ecosystem think about it, especially the app developers. We could also start without expiration and see if we really need it.

",vovacodes,1448,6,12,13,2023-04-16T22:36:10.054Z,2023-06-30T13:23:16.618Z,sRFC,6,2023-06-30T13:23:16.618Z
65,sRFC 00007 - Encryption Standard for Solana Keypairs,https://forum.solana.com/t/srfc-00007-encryption-standard-for-solana-keypairs/65,"As the Solana ecosystem evolves, the need for an encryption standard for Solana key-pairs becomes increasingly important for securing sensitive data such as account state or files on distributed file systems like IPFS and Arveawe. While Ed25519 key-pairs are effective for signing messages, they are not usable for asymmetric encryption, which is crucial for user privacy and data protection.
To address this issue, I propose creating a standard for converting Ed25519 key-pairs to Curve25519 key-pairs that are designed for the Diffie Hellman Key exchange protocol, which would enable asymmetric encryption without having to generate seperate keys to perform this kind of action. This conversion allows users and software wallets to leverage libraries like TweetNaCl for robust and easy-to-use encryption implementations.
Implementing an encryption standard for Solana key-pairs would give Solana one more use-case which could drive a lot of traction to the chain.
I made a reference implementation as a feature for this kind of conversion in the Wallet Adapter example repository. Check it out here: wallet-standard/solanaWallet.ts at encryption · valentinmadrid/wallet-standard · GitHub
While there is a consensus within the Solana community that encryption is essential, there may be different approaches to implementing it; thus, I welcome everyone to provide their feedback and suggestions on this proposal to ensure the most effective solution is adopted.","[valentin]: Here is a demo implementing the conversion from Ed25519 to Curve25519 keypairs showing off the performance and safety of using Tweetnacl using these: https://solana-encryption-demo.vercel.app/ 
The following paper suggests that such a “conversion”/usage for signing and encryption using the same key should be safe: On the Joint Security of Encryption and Signature in EMV
This answer on the cryptography Stackexchange suggests the same thing: What happen if the curve used in key agreement protocol also used in signature inside of protocol? - Cryptography Stack Exchange
Again, this proposal has been made to come to an agreement on how Solana Keypairs should be derived to be compatible for Diffie Hellman Key exchange or other kinds of encryption mecanisms. Once an agreement on this topic is reached, wallets could start to implement encryption features as part of the Wallet Standard.

[valentin]: As an alternative to deriving a Curve25519 keypair from a regular Solana Keypair to perform encryption, Jordan Sexton suggested that a keypair could be derived from a signed message by any wallet to derive an encryption key(including hardware wallets).
This could have upsides to my proposal, you can read about his suggestion here: https://twitter.com/jordaaash/status/1645173328624361472?s=46&amp;t=B3h7suAANXSyY_bAa_5fbw
A concern with this approach may be that the cryptography could be broken once two dApps request a signature with the same seed. There would be a need for some kind of one time nonce that has to be different on each encryption, which would be hard to achieve.

[Alchemize]: deriving a keypair from a signed message by a wallet as an encryption key looks nice.
can having 4-8 digit PIN manually entered by the user be used a seed to be signed ?
can this help in avoiding seed collision ?

[valentin]: This is how we’ve agreed on this proposal for now:
The user provides an input message (plaintext) intended for encryption, along with the recipient’s public key, and invokes the encryption function available in the browser’s window.
The plaintext message is passed on to the encryption module within the user’s digital wallet.
The wallet derives a Curve keypair from the signature generated by the user’s stored keypair. The size of the derived Curve keypair is determined by the byte length of the signature.
The wallet then performs a box encryption operation (asymmetric) utilizing the tweetnacl cryptographic library. The encryption process incorporates: a. The recipient’s public key b. The plaintext message c. A randomly generated nonce (number used once) by the wallet d. The Curve keypair, which has been derived from the signature
Upon successful encryption, the wallet outputs the resulting ciphertext and the generated nonce to the user.
This constraint implies that dApps must implement a mechanism to handle the storage, retrieval, and sharing of public keys associated with encryption. The dApps could maintain an on chain program or database, which stores public keys provided by wallet holders explicitly for encryption purposes.
Please give me your thoughts on this.

[Alchemize]: the Idea behind having a PIN than a fixed string was that ,when the user enters that specific PIN, the wallet signs it to derive a new Curve keypair. so the derivation of Encryption keypair is dependent on the PIN.
Advantage : PIN acts as another layer of security to the cipher text
DisAdvantage : Decryption cannot happen if the PIN is lost/ forgotten
FYI : just wanted to explore some new directions… might not be the best way…

[valentin]: A PIN would mean the encryption Keypair would not be recoverable using a private key if the PIN is lost, therefore the mecanism to prevent the encryption from being broken is that the wallet generates a random nonce additionally to the deterministic message for the encryption that is given back to the client afterwards. The random generated nonce by the wallet can be publicly shared.

[joec]: Just wanted to circle back here to share an update on this sRFC workflow!
 valentin:
This constraint implies that dApps must implement a mechanism to handle the storage, retrieval, and sharing of public keys associated with encryption. The dApps could maintain an on chain program or database, which stores public keys provided by wallet holders explicitly for encryption purposes.
@jordaaash and I have been working on an implementation for exactly this, with an on-chain program.
 
 
 GitHub
 
 
 
GitHub - buffalojoec/keyring-program: Keyring program
 
Keyring program. Contribute to buffalojoec/keyring-program development by creating an account on GitHub.
 
 
 
 
 
 
I’ve documented how this program works in the repository’s README, but the TL/DR is:
This program does exactly what @valentin suggested: stores public keys associated with encryption
It makes use of a “Keystore” PDA mapped to a particular wallet address
It’s specifically tailored to implement serialization/deserialization in the off-chain client, so that the program can remain frozen while new encryption algorithms may be added in the future
It’s inception should go hand-in-hand with a robust review process - akin to sRFCs or simply Pull Requests - to add support for new encryption algorithms
At the time of writing this reply, the encryption algorithms depicted in the client are for demonstration purposes only, and any associated configurations may look different than this spec in production.
The implementation is also in active development and subject to change.
Would love anyone’s feedback on the code, though!!

",valentin,2726,5,7,8,2023-04-08T13:02:39.224Z,2023-06-28T23:56:45.417Z,sRFC,6,2023-06-28T23:56:45.417Z
292,sRFC 18: Generalized Small State Compression,https://forum.solana.com/t/srfc-18-generalized-small-state-compression/292,"Generalized Small State Compression
The following repo describes an event interface and instruction behavior that allows programs to compress state up to 300 bytes per asset.
 
 
 GitHub
 
 
 
GitHub - ngundotra/srfc-compressed-state
 
Contribute to ngundotra/srfc-compressed-state development by creating an account on GitHub.","[joec]: This appears to iterate directly on sRFC 00016, yeah?

",ngundotra,654,0,1,2,2023-06-06T19:57:50.495Z,2023-06-27T13:21:48.990Z,sRFC,6,2023-06-27T13:21:48.990Z
340,sRFC 00019: Wallet Standard - Get Ephemeral Signers feature,https://forum.solana.com/t/srfc-00019-wallet-standard-get-ephemeral-signers-feature/340,"Wallet Standard - Get Ephemeral Signers feature
Summary
Ephemeral Signers (ES) is a pretty common pattern in the Solana apps ecosystem. ES are keys that are expected to sign the transaction and be discarded right after. An example of when they are needed is the SystemProgram.createAccount instruction. When creating a non-PDA account, this instruction requires the new account to be a signer to verify that whoever calls this instruction actually holds authority over this account.
Most of the time, app developers use Keypair.generate() to create Ephemeral Signers. Then they sign a transaction that requires an ES with the Keypair and throw it away immediately after. This pattern works pretty well when the signed transaction is immediately sent and executed. That said, it prevents a certain class of use-cases when the transaction is stored on chain and executed later when a certain condition is met, e.g. time lock is released, multisig approval threshold is reached, such as with some Smart Wallet implementations (Realms, Squads, Clockwork etc). In these cases, the Ephemeral Keypair will not be available to sign the “execute” transaction because it has already been “forgotten”.
Proposed Solution
Please refer to the linked Pull Request for the details
Implementation
 
 github.com/solana-labs/wallet-standard
 
 
 
 
 
 
 
 
 `solana:getEphemeralSigners` feature
 
 
 solana-labs:master ← vovacodes:feat/get-ephemeral-signers
 
 
 
 opened 05:04PM - 26 Jun 23 UTC
 
 
 
 
 vovacodes
 
 
 
 
 +27
 -0
 
 
 
 
 
 ## Problem
Ephemeral Signers (ES) is a pretty common pattern in the Solana ap…ps ecosystem. ES are keys that are expected to sign the transaction and be discarded right after. An example of when they are needed is the&nbsp;`SystemProgram.createAccount`&nbsp;instruction. When creating a non-PDA account, this instruction requires the new account to be a signer to verify that whoever calls this instruction actually holds authority over this account.
Most of the time, app developers use&nbsp;`Keypair.generate()`&nbsp;to create Ephemeral Signers. Then they sign a transaction that requires an ES with the Keypair and throw it away immediately after. This pattern works pretty well when the signed transaction is immediately sent and executed. That said, it prevents a certain class of use-cases when the transaction is stored on chain and executed later when a certain condition is met, e.g. time lock is released, multisig approval threshold is reached, such as with some Smart Wallet implementations (Realms, Squads, Clockwork etc). In these cases, the Ephemeral Keypair will not be available to sign the ""execute"" transaction because it has already been ""forgotten"".
## Proposed Solution
We suggest a new Wallet Standard feature -`getEphemeralSigners(numberOfSigners: number): Promise&lt;Array&lt;Pubkey&gt;&gt;` that would&nbsp;allow apps to request any number of ESs from the wallet. The wallet would return an array of public keys that the app developer can use in their transactions.
Implementation of this feature can vary depending on the type of the wallet being used. For instance, for a multisig wallet, an ES can be a PDA owned by the Multisig Program, enabling the Program to sign the transaction on behalf of that account. While for a regular wallet (Phantom, Glow, etc.), it can be a Keypair generated by the wallet itself and stored securely in the extension's background storage over the course of the browser session.
Regardless of the implementation, the app developer doesn't have to worry about how the ES was generated; they can assume that the public key will be a signer of the transaction when it comes to the execution time.
## Benefits for Apps
By detecting and using this Wallet Standard feature, app developers can be sure that their app will work with all existing and future wallet implementations out of the box and no hacks are needed.
There’s also a security benefit of never exposing the Ephemeral Signer Keypair to your app code directly - it is kept in the wallet secured storage, so a malicious dependency can’t steal it and use in any sort of re-initialization attacks.
## Benefits for Wallets
By exposing this functionality via the Wallet Standard interface, wallets reserve a place for themselves to innovate and add Smart Wallet features in the future without changing their public interface.
## Usage
Detecting the feature and requesting an Ephemeral Signer can be as easy as this:
```ts
const ephemeralSignerPubkey = ""standard"" in adapter &amp;&amp; ""solana:getEphemeralSigners"" in adapter.wallet.features &amp;&amp; await adapter.wallet 
 .features[""solana:getEphemeralSigners""]
 .getEphemeralSigners(1)[0]
```
Here we check that our wallet adapter is a Wallet Standard implementation and if it exposes the `solana:getEphemeralSigners` feature. If so we request 1 Ephemeral Signer public key from the wallet, which now can be passed as an account into a transaction that needs an extra signer.
## Proof of Concept
We ([Squads](https://squads.so)) implemented a proof of concept for this feature as part of our multisig-powered Smart Wallet - Fuse. We tested the integration with Jupiter Limit Orders product and are currently in conversation with other app developers regarding the adoption. We also received some positive initial feedback from other ecosystem participants (Realms). We believe, standardizing this feature will significantly help with its adoption and enable proliferation of Smart Wallets ecosystem on Solana.
## Next steps
- [ ] Create an sRFC for this feature proposal and refine the design if necessary",,vovacodes,740,0,0,1,2023-06-26T17:20:54.861Z,2023-06-26T17:20:55.066Z,sRFC,6,2023-06-26T17:20:55.066Z
215,sRFC 00013 - CPI Events,https://forum.solana.com/t/srfc-00013-cpi-events/215,"GitHub
 
 
 
GitHub - ngundotra/srfc-event-indexing: Event based indexing for Solana programs
 
Event based indexing for Solana programs. Contribute to ngundotra/srfc-event-indexing development by creating an account on GitHub.","[nickfrosty]: for check 1: why is it important for the data to start with those specific 8 bytes? is it just to have a standard discriminator to know it is a “cpi event” that should be indexed

[mschneider]: Appreciate work on this problem. Events are an essential but very neglected part of the runtime. I feel conflicted about trying another “runtime hack” for this purpose. Curious what a design from scratch would look like.

[mschneider]: I looked more in detail into the proposed spec, one issue I see, is that the CPI can reference accounts via ALT, which would break the current parser. Given how basic events are for client programs and how difficult it is to have accurate ALT copies on client side, I am more convinced that we should make events a first class citizen so that it’s easy to subscribe to them using a websocket similar to ETH:
 
 docs.ethers.org
 
 
 
Events
 
Documentation for ethers, a complete, tiny and simple Ethereum library.
 
 
 
 
 
 
Inner instructions are currently part of TransactionMeta, which means you can either subscribe to all txs in a block (very unscalable) or just one Transaction.

[mschneider]: Another argument against CPI would be that the current program is passed to the emit call as an account, this obviously creates a lot of overhead on the runtime, as loading the a fairly average program means loading 3MB of memory.

[jarry]: Triton has implemented a transactionSubscribe RPC call (can filter by accounts, success/failure) which helps a lot in mitigating the problem of streaming CPI events

",ngundotra,890,1,5,6,2023-05-08T20:22:55.153Z,2023-06-16T17:51:10.908Z,sRFC,6,2023-06-16T17:51:10.908Z
66,sRFC 00008: IDL Standard,https://forum.solana.com/t/srfc-00008-idl-standard/66,"sRFC 00008: IDL Standard
Summary
It’s no question that the introduction of an IDL alongside a Solana program was one of the biggest value-adds provided by Anchor.
With an IDL, indexers, explorers, and many other tools can gain insight into a deployed program that they otherwise couldn’t have gotten - especially deserialization of PDA data.
Right now, we continually run into problems when trying to develop tools or solutions that encounter two types of programs: one with an IDL published, and one without an IDL.
Immediately these projects have to essentially “skip” or “omit” programs that are deployed without an IDL, because their solution simply will not work without one.
We need a standard way to manage IDLs for programs across Solana, and it shouldn’t matter which framework or tools you use to build your program.
Questions
What should an IDL’s interface look like?
Two existing and popular tools for creating IDLs - Anchor and Shank - actually generate very similar IDLs.
Here’s part of a simple example:
{
 ""version"": ""0.1.0"",
 ""name"": ""car_rental_service"",
 ""instructions"": [
 ...
 {
 ""name"": ""BookRental"",
 ""accounts"": [
 {
 ""name"": ""rentalAccount"",
 ""isMut"": true,
 ""isSigner"": false,
 ""desc"": ""The account that will represent the actual order for the rental""
 },
 {
 ""name"": ""carAccount"",
 ""isMut"": false,
 ""isSigner"": false,
 ""desc"": ""The account representing the Car being rented in this order""
 },
 {
 ""name"": ""payer"",
 ""isMut"": true,
 ""isSigner"": false,
 ""desc"": ""Fee payer""
 },
 {
 ""name"": ""systemProgram"",
 ""isMut"": false,
 ""isSigner"": false,
 ""desc"": ""The System Program""
 }
 ],
 ""args"": [
 {
 ""name"": ""bookRentalArgs"",
 ""type"": {
 ""defined"": ""BookRentalArgs""
 }
 }
 ],
 ""discriminant"": {
 ""type"": ""u8"",
 ""value"": 1
 }
 },
 ...
 ],
 ""accounts"": [
 ...
 {
 ""name"": ""RentalOrder"",
 ""type"": {
 ""kind"": ""struct"",
 ""fields"": [
 {
 ""name"": ""car"",
 ""type"": ""publicKey""
 },
 {
 ""name"": ""name"",
 ""type"": ""string""
 },
 {
 ""name"": ""pickUpDate"",
 ""type"": ""string""
 },
 {
 ""name"": ""returnDate"",
 ""type"": ""string""
 },
 {
 ""name"": ""price"",
 ""type"": ""u64""
 },
 {
 ""name"": ""status"",
 ""type"": {
 ""defined"": ""RentalOrderStatus""
 }
 }
 ]
 }
 }
 ],
 ""types"": [
 ...
 {
 ""name"": ""RentalOrderStatus"",
 ""type"": {
 ""kind"": ""enum"",
 ""variants"": [
 {
 ""name"": ""Created""
 },
 {
 ""name"": ""PickedUp""
 },
 {
 ""name"": ""Returned""
 }
 ]
 }
 }
 ],
 ""metadata"": {
 ""origin"": ""shank"",
 ""address"": ""8avNGHVXDwsELJaWMSoUZ44CirQd4zyU9Ez4ZmP4jNjZ"",
 ""binaryVersion"": ""0.0.12"",
 ""libVersion"": ""0.0.12""
 }
}
(Full IDL here)
Ultimately, we want to figure out if we can use this (or Anchor’s IDL) as the interface for all IDLs across Solana.
What Should be the Standard for Implementing the IDL Interface?
If we consider the IDL above - or something similar - to serve as our IDL interface, what should be the standard for implementing?
Can you just add whatever fields you want, as long as you still have the fields from the interface? For example:
""accounts"": [
 ...
 {
 ""name"": ""RentalOrder"",
 ""type"": {
 ""kind"": ""struct"",
 ""fields"": [
 {
 ""name"": ""car"",
 ""type"": ""publicKey""
 },
 {
 ""name"": ""name"",
 ""type"": ""string""
 },
 ],
 ""myCustomConfiguration"": {
 ""someConfig"": 1,
 ""someOtherConfig"": ""2"",
 }
 }
 }
 ],
Considerations:
This would break any type-oriented IDL parsers by introducing new fields that otherwise weren’t part of the type schema
A lack of limits might inflate the size of IDLs unexpectedly
What crate/lib/types Should be Used?
We could do something like introduce a standard crate with the interface types as the basis for all IDLs on Solana.
We then could modify these types to allow for pluggable custom configurations, or some other means for easily implementing the interface for an IDL leveraging this crate.
A great candidate for this standard library of IDL types (interface) is Iron Forge’s Solana IDL - an open-source crate containing the IDL types from Shank compatible with serde.
It would be great to hear thoughts on:
Electing this crate as the IDL type interface
Modifying this crate to serve as an implementable interface, with customizable type configurations
Implementations &amp; Ideas
Here’s a PR introduced by Noah from Solana Labs to introduce the idea of an IDL program within the Solana runtime.
Here’s a crate produced by Thorsten from Iron Forge to lay down the Rust types for creating an IDL leveraging serde.
Conclusion
In short, like any other interface or standard proposal, this is a migration that could be done in a way that hurts only once, but allows for easy integrations and added support in the future.
A quick recap on the questions proposed:
What should an IDL’s interface look like?
What should be the standard for implementing the IDL interface?
What crate/lib/types should be used?","[kevinheavey]: I have a few thoughts about this. The first one is: what will typed bytecode (BTF) give us, and does it cover any things that IDLs cover?

[jarry]: I really like the shank + anchor IDL format. Specifically, I like the fact that it makes the discriminant (i.e. instruction identifier) explicit. One of the nice things (some may disagree) about Solana is how much control it gives to the developer to come up with the semantics of how a program instruction should be dispatched. Anchor makes a lot of opinionated decisions on behalf of the devs, which ends up improving devex + security and reducing boilerplate, but I think those decisions should be explicit in the IDL rather than inferred from the client.
IMO the IDL should explicitly have 100% of the information needed to generate an instruction from a client as opposed forcing the client to read the instruction name and compute some bespoke hash before generating the instruction.
Note that these IDLs can never capture 100% of all execution paths because there is a bunch of custom logic that can be injected in a program to dictate how accounts should be deserialized. However, the shank + anchor IDL seems usable for like 99% of programs, so I think it makes sense as a standard.

[callensm]: For reference (coming from twitter thread) something that I had previously helped work on related to this topic:
 
 
 GitHub
 
 
 
GitHub - solana-idl-foundation/solana-idl-spec: Specifying a Solana IDL...
 
Specifying a Solana IDL Standard, inspired from the Anchor IDL Standard - GitHub - solana-idl-foundation/solana-idl-spec: Specifying a Solana IDL Standard, inspired from the Anchor IDL Standard

[joec]: This is where the gray area comes into play, though.
I agree Shank/Anchor IDLs cover a wide range of use cases and would be a great start for an interface, but what if you start to see an increase in alternate ways of deserializing accounts or instructions?
How do we begin to lobby indexers and tools to support these added configs in the IDL?
We may need to - at that time - introduce an interface/standard on top of an interface 
Something like deserializing data is integral to a lot of the tools that would need to know about these added configs, so that’s a use case where you might weigh “should ser/deser configs go into every IDL then? Or just ones that vary from the norm? What is the norm anyhow?”
A different custom config to an IDL - such as details on how your PDA’s addresses relate to each other (ie. seeds expanded) might be something that matters less to an indexer/explorer/UI of sorts. In this case, it might make more sense for the developer(s) and their amassed following to lobby these tooling providers to support their custom configs for some value-add reason, but ultimately it’s no sweat to the indexer tools if they don’t.

[joec]: I guess I’ll be the one to ask what you’re talking about, since I don’t know lol. Care to elaborate?

[kevinheavey]: twitter.com
 
 
 
toly 🇺🇸
@aeyakovenko
 Typed bytecode is coming to @solana with ABIv2. It will be so f'ing awesome! So what is it? 🧵
 6:47 PM - 9 Feb 2023
 
 
 
 
 360
 
 
 
 
 
 80

[joec]: Do we know the timeline for ABIv2?
I don’t really see IDL interfacing/standardizing as a huge lift compared to other standards, so it might not hurt to sort out despite this impending change. Timeline-dependent, of course.

[Marche]: While working on soda, have given significant consideration to the standardization of IDLs within the Solana ecosystem. I often find myself contemplating an expanded version of an IDL, including supplementary information that enables the incorporation of specific features. However, it is crucial to address a fundamental question: What is the primary purpose of an IDL, and what should it encompass? With this in mind, I believe we should carefully consider the following points:
Should the IDL strictly adhere to its namesake and serve solely as an interface description, or should it serve additional purposes?
Is it necessary to make seed information a mandatory component of the IDL?
Should the IDL incorporate information about the inner workings of the smart contract in some manner?
While I don’t possess definitive answers to these questions, I firmly believe it is imperative to establish clear boundaries regarding the purpose of an IDL. Instead of blindly adding various elements to the IDL, we should explore alternative sources and leverage additional data structures when necessary. If every developer starts incorporating the keys they deem necessary, we run the risk of ending up with divergent and non-standardized implementations that result in overlapping keys.

",joec,1863,4,8,9,2023-04-10T17:11:26.948Z,2023-06-09T18:17:18.294Z,sRFC,6,2023-06-09T18:17:18.294Z
267,sRFC 00015: Interfaces,https://forum.solana.com/t/srfc-00015-interfaces/267,"Interfaces for programs
Summary
Goal is to define a simple interface specifications for programs that avoid creating additional CPIs during execution.
Interfaces must be discoverable from the program elf.
Discovery shouldn’t require a CPI
Calling an interface should be as fast as calling a non interfaced program
Transactions should be the same size
There needs to be some way to add interfaces to current set of programs on solana
Implementation: 
1 interface defines 1 method. So instead of a “Token” interface, there is a specific interface for the Transfer method. The interface is identified by a u128 GUID, a random number that can be generated by a dev.
Publishing Interfaces
The interface is published as in .rs files as documentation for the GUID in mod interfaces
For Example:
 /// interfaces.rs
 /// Transfers tokens from one account to another either directly or via a
 /// delegate. If this account is associated with the native mint then equal
 /// amounts of SOL and Tokens will be transferred to the destination
 /// account.
 ///
 /// Accounts expected by this instruction:
 ///
 /// * Single owner/delegate
 /// 0. `[writable]` The source account.
 /// 1. `[writable]` The destination account.
 /// 2. `[signer]` The source account's owner/delegate.
 ///
 /// * Multisignature owner/delegate
 /// 0. `[writable]` The source account.
 /// 1. `[writable]` The destination account.
 /// 2. `[]` The source account's multisignature owner/delegate.
 /// 3. ..3+M `[signer]` M signer accounts.
 /// Transfer {
 /// /// The amount of tokens to transfer.
 /// amount: u64,
 ///},
 const TRANSFER: u128 = 0x2423423fda2344321u128;
Registering Interfaces
///program's instruction.rs
interfaces!([(interfaces::TRANSFER, interface::Instruction::U8(TokenInstruction:Transfer))])
This macro takes an array of mapping the GUID to the instruction id. The instruction type is coerced into the right format by Instruction::U8, so types of any kind of instruction index can be handled. The macro generates a segment that is included in the program ELF file that can be easily parsed from the program bytecode.
Discovering Interfaces
///implementation code
fn get_interface(program_account:: Account, guid: u128) -&gt; Result&lt;interface::Instruction&gt;
This helper function finds the lookup table for the interfaces at a well defined spot in the program byte code and finds the interface instruction index.","[toly]: Github Issues related to BTF changes that would make interfaces discoverable
 
 github.com/solana-labs/solana
 
 
 
 
	 
 
 
 
 Loader v3 Built-In Program
 
 
 
 opened 05:27PM - 24 Jan 23 UTC
 
 
 
 
 Lichtso
 
 
 
 
 
 runtime
 
 
 
 
 - New loader built-in program
 - Pin account allocations host ptrs by reservin…g (without allocating) host address space for account resizing
 - Support multiple entrypoints (generalized methods instead of one `main`) per executable in RBPF
 - [Optional] Use page table `dirty` bit to track which parts of accounts were actually modified and report that back to the accounts DB to allow for a partial write back to disk. This could be done using either using [`/proc/PID/pagemap`](https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/mm/soft-dirty.rst) or using CPU virtualization.
- Actual ABI changes:
 - Rework executable / program account ownership chain and `is_executable` flag (workaround for avoiding finalization)
 - [Not MVP] Dynamic ABI memory layout using BTF relocations
 - Unify the currently separate virtual address spaces and remove address translation at runtime
 - Share the host address space for all programs in a transaction (similar to [Native Client](https://developer.chrome.com/docs/native-client/))
 - Dynamic function calls replacing CPI and Syscalls
 - Replace VM nesting by dynamic linking using two levels of indirection
 - [Not MVP] Replace syscalls by CPI to built-in programs
 - Allocation and lifetime tracking
 - Stack allocation for internal types
 - Normal pointers with memory layout information: Load always possible, store only if pointer is mutable
 - Heap allocation (transaction global) for external types and persistent structures (accounts)
 - Opaque pointers: No load or store possible
 - Runtime provides table of these opaque pointers for programs to lookup their members
 - [Not MVP] How to inline dynamic arrays / vectors into structs (especially accounts)?
 
 
 
 
 
 
 
 
 github.com/solana-labs/solana
 
 
 
 
	 
 
 
 
 Generate and Verify BTF
 
 
 
 opened 05:22PM - 24 Jan 23 UTC
 
 
 
 
 Lichtso
 
 
 
 
 
 runtime
 
 
 
 
 - BTF type info in toolchain
 - [Not MVP] ELF dynamic loader instructions for …defining on-chain addresses of dependencies
 - [Optional] cargo (dependencies)
 - rustc (attribute)
 - LLVM: Lift C type restriction, inject runtime code for lifetime tracking
- New stricter verifier in RBPF
 - [Not MVP] Reject cyclic dependencies when deploying a program
 - [Not MVP] Enforce redeployment maintains existing interfaces (function signatures and types), optionally support type migration
 - Type inference
 - Emit type line info for transmutation / reinterpretation
 - Reject ptr transmutation / reinterpretation
 - Forbid ptr introspection or order based comparison, only allow equality test (check for aliasing)
 - Track types of stack slots
 - Canonicalize bounds checks of sub-slices
 - Rustc needs to emit the canonical conditional branch
 - Verifier checks that all sub-slicing has such a runtime bound check
 - Restrict control flow
 - Forbid jumps outside of the current function
 - Enforce that functions end with an `exit` instruction
 - Constrain `call` and `callx` to functions with the same signature

[toly]: Github Issues related to BTF changes that would make interfaces discoverable
 
 github.com/solana-labs/solana
 
 
 
 
	 
 
 
 
 Program-Runtime v2 - Road Map
 
 
 
 opened 05:32PM - 06 Nov 22 UTC
 
 
 
 
 Lichtso
 
 
 
 
 
 enhancement
 
 
 work in progress
 
 
 runtime
 
 
 
 
 The performance optimizations we have been working on inside the runtime under t…he ABIv2 project the past year will be deployed separately as [""account data direct mapping""](https://github.com/solana-labs/solana/pull/28053). The redesign of the ABI between runtime and on-chain program is pushed out further as we changed the goals again. A discussion with @alessandrod and @pgarg66 led to the following:
 
### Tasks
- [ ] #29803
 - [x] #29654
 - [x] #30154
 - [x] #30139
 - [x] #30282
 - [x] #30336
 - [x] #30337
 - [x] #30348
 - [x] #30371
 - [x] #30275
 - [x] #30425
 - [x] #30803
 - [x] #30900
 - [x] #30902
 - [x] #30561
 - [x] #30703
 - [x] #30900
 - [x] #30902
 - [x] #30940
 - [x] #30945
 - [x] #30950
 - [x] #30959
 - [x] #31034
 - [x] #31036
 - [x] #31116
 - [x] #31118
 - [x] #31142
 - [x] #31311
 - [x] #31331
 - [x] #31395
 - [x] #31413
 - [x] #31465
 - [x] #31493
 - [x] #31494
- [ ] #20323
 - [x] https://github.com/solana-labs/rbpf/pull/454
 - [x] https://github.com/solana-labs/rbpf/pull/460
- [ ] #29863
- [ ] #29864
 - [x] #29728
 - [x] #30579
 - [x] #30614
 - [x] #30464
 - [x] #30693
 - [x] #30893
 - [x] #31007
 - [x] #31088
 - [x] #31244
 - [x] #31221
 - [x] #31244
 - [x] #31324
 - [x] #31329
 - [x] #31345
 - [x] #31429
 - [x] #31488
- [Not MVP] Adjust other built-in programs and the testing framework

[ripatel-jump]: The addition of a GUID and the interfaces macro seems redundant. It also seems susceptible to type confusion security issues if an attacker manages to create two distinct function signatures with the same ID.
How about the following?
Adjust the compiler to generate BTF for all public and externally linked entrypoints
Adjust the compiler to generate BTF for all imported entrypoints
When generating BTF for a function, also generate BTF for all transitive types (the types of the function arguments and the return type, recursively)
The ELF format only supports the specification of one entrypoint, so we could instead signal what is public through a custom flag in the dynamic symbol table, e.g. STV_PUBLIC_ENTRYPOINT.
The Solana SDK could make this more developer-friendly through a macro annotations, like so:
// Callee
#[solana_program::entrypoint]
pub fn transfer(bla: u32) -&gt; Result&lt;u64, String&gt;
// Caller
use callee::transfer;
fn bla() {
 transfer(...);
}
In both the caller and callee, the ELF of both programs would contain the BTF definitions of types
Result&lt;u64, u64&gt;
String
type of callee::transfer
The runtime would then check both types for equality before execution.
This avoids the use of GUIDs and brings it more in line with regular dynamic linking.
The drawback of this is that it uses more space, as the caller ELFs will now have to store copies of the BTF. I would expect the BTF footprint to be negligible though unless devs make excessive use of templates.

[toly]: Security issues aren’t a concern because the callee never trusts the caller and has to validate all inputs.
In the BTF approach the runtime does that at link time. I generally think it’s the better option, but we need an actual design for the conventions we want programs to use. Something needs to do the dispatching from a wallet signed message string to the public entry points.

[alessandrod]: ""ripatel-jump:
How about the following?
Adjust the compiler to generate BTF for all public and externally linked entrypoints
Adjust the compiler to generate BTF for all imported entrypoints
When generating BTF for a function, also generate BTF for all transitive types (the types of the function arguments and the return type, recursively)
[snip]
The drawback of this is that it uses more space, as the caller ELFs will now have to store copies of the BTF. I would expect the BTF footprint to be negligible though unless devs make excessive use of templates.
This is all already planned and even mostly implemented in LLVM: emitting BTF for a type recursively triggers BTF emission of all the referenced types - this includes function prototypes and definitions. The footprint is indeed negligible - BTF for the whole linux kernel (millions of LOC) is 4.5MB today. Also since BTF is only emitted for types reachable from public entrypoints - and not emitted for unused types - even depending on crates with a large API surface like solana_program won’t significantly impact ELF size.
For CPI, the idea is that this will work completely transparently. There’s no explicit dynamic dispatch or discovery needed. We’re planning to teach rustc, cargo and the linker to use the type info generated when compiling programs, so you’ll be able invoke a callee program just like any other function, you’ll get compile time errors if you try to misuse something etc. Compile time errors are just for improved developer experience, but obviously the runtime will still not trust the resulting bytecode.
At load time then the program runtime will resolve links, apply BTF relocations and enforce whatever security constraints can be enforced based on type info. Higher level properties that can’t be expressed via the type system will be enforced at runtime.
Having said all that, as @toly pointed out we do still need a way to invoke entrypoints from a tx, so we do need a &lt;something&gt; =&gt; &lt;entrypoint&gt; mapping. Since the names of public entrypoints will likely not be mangled - the rust mangling scheme is not stable yet and we need to interop with C and one day move programs too - could we use symbol names? If we can use symbol names then essentially we don’t have to do anything special in the runtime, we already have a symbol table so we can just lookup.

[toly]: So with Runtime V2 something like this should work
///token.rs
///Token Transfer interface
struct Token {
 authority: Pubkey,
 balance: u64
};
trait Token {
 fn transfer(from: &amp;mut Token, to: &amp;mut Token, amount: u64) -&gt; Result&lt;()&gt;
}
then the implementation can look like this
///mytoken.rs
///my token implementation, counts the number of balance transfers
struct MyToken {
 token: Token,
 counter: u64,
}
impl Token {
 fn transfer(from: &amp;mut Token, to: &amp;mut Token, amount: u64) {
 let from: &amp;mut Account = to_account(from);
 let from: &amp;mut MyToken = from_account(from);
 from.counter += 1;
 from.token.transfer(to, amount);
 }
}

[ripatel-jump]: @toly would PRv2 support dynamically dispatching this trait? This looks very interesting. How would we address (lack of) ABI stability in rustc v1.x.x?

[toly]: It should be equivalent to a global extern function in C.
Do you mean dispatching from the transaction message processor or between programs? I think the tricky part will be figuring out which token implementation gets called when more then one is present.
I think we will need to be able call the extern functions from the program object.

[splintr]: I’d like to suggest that interface GUID’s should be the first 128 bits of a hash of the specification detailing what accounts/data the interface expects. This convention should prevent contention/confusion over specific ID’s (ie low # ID’s).
Additionally, I’d like to note that there is a need for data interfaces. For example, for any ownable object, it should be possible to determine the owning address. In different implementations that owner may be stored at different offsets into an account. It would be a huge burden on indexers/applications to require that these offsets be found manually. Additionally, a single program may have multiple different account types, so these offsets might be different within a single program.
I propose a solution in two parts: Account discriminators and an offsets section within the ELF. The idea would be that, within the binary, each different discriminator would be followed by a list of interface-offset pairs, designating the location within each account that the interface’s data can be found.
Note: If I remember correctly, some current implementations vary account type via account size. These implementations will need to be manually grandfathered in by indexers. In fact, since these implementations already exist and are indexed, it would require minimal work. However, all future programs would need to adhere to the discriminator system.

[cavemanloverboy]: if I understand correctly, it seems to me like this goes against last year’s trend of composability (since everyone has their own impl of a primitive), will contribute to chain bloat, and opens up a can of worms re: vulnerability.
Why are these issues not a concern?
How big is the byte code for each interface implementation (e.g., for these simple transfers)?
Are there any guarantees about state transitions that can be provided beyond Rust/Move aliasing rules? For example, instead of a &amp;mut self, can we mark only part of state as mutable for a given implementation? Also, as another example, can we provide default implementations for particular methods that cannot be overwritten.
To illustrate this last point, consider another take on your last example:
struct Token {
 authority: Pubkey,
 balance: u64
};
trait Token {
 fn transfer_hook(from: &amp;mut Token, to: &amp;mut Token, amount: u64) -&gt; Result&lt;()&gt;
 
 #[immutable]
 fn transfer(from: &amp;mut Token, to: &amp;mut Token, amount: u64) -&gt; Result&lt;()&gt; {
 let to_account: &amp;mut MyToken = to_account(to);
 let from_account: &amp;mut MyToken = from_account(from);
 from_account.token.transfer(to_account, amount);
 transfer_hook(from, to, amount)
 }
}
and
struct MyToken {
 token: Token,
 counter: u64,
}
impl Token for MyToken {
 fn transfer_hook(from: &amp;mut Token, to: &amp;mut Token, amount: u64) {
 let from: &amp;mut MyToken = from_account(from)
 from.counter += 1;
 }
}
which provides the same functionality while providing a base implementation that need only be audited once.

",toly,1517,5,10,11,2023-05-24T14:29:01.363Z,2023-06-09T04:11:10.671Z,sRFC,6,2023-06-09T04:11:10.671Z
246,sRFC 00014: Rethinking SPL Token,https://forum.solana.com/t/srfc-00014-rethinking-spl-token/246,"sRFC 00014: Rethinking SPL Token
Summary
This sRFC highlights a critical issue with the current implementation of the SPL token program: the tightly coupled interface and implementation. This results in performance degradation and significant barriers to innovation. To overcome these challenges, the sRFC proposes a modular token program that offers flexible functionality while maintaining security.
Problem
One of the biggest differences between the SVM and the EVM is the SVM’s separation of state and code. This mechanism is useful because it enables parallel processing of transactions. However, it has also had a less-desirable consequence: tightly coupling interfaces and implementations.
This issue is particularly relevant to Solana tokens. Unlike EVM chains with a standard ERC20 interface and multiple implementations, Solana requires all projects to utilize the same shared implementation for token adoption within the ecosystem. Consequently, all token instructions flow through a common codepath. This presents two major drawbacks:
Performance degradation due to ‘lowest common denominator’ approach: The shared implementation must accommodate all possible token features, even if they are unnecessary for certain tokens. As a result, token instructions often traverse unnecessary codepaths, leading to performance inefficiencies and increased data storage requirements. For example, all transfers check if the source and destination accounts are frozen, even if a token doesn’t need / isn’t using the freeze functionality. Further, 66 bytes of the 75-byte mint account (88%) and 88 bytes of the 136-byte token account (65%) are only used for a subset of tokens. Given the mass-market adoption that Solana aims to achieve, these differences could add up to tens of millions of dollars of added cost to Solana end-users.
Raised barriers to innovation: While the Token-2022 initiative aims to add more features, it fails to address the core problem of inhibiting permissionless innovation. Many projects on Ethereum, such as MakerDAO with DAI, Compound with cTokens, OlympusDAO with OHM, and others, have required custom token implementations tailored to their specific needs. The inability to create alternative token programs easily limits experimentation and adoption within the Solana ecosystem.
Opaque governance: As the token program is singleton and unpredictable features are expected, it must be upgradeable. Currently, the governance of the token program is driven by Solana Labs, with validators theoretically having the ability to prevent an upgrade. However, this arrangement raises concerns about centralization and the lack of practical oversight by validators.
Solutions
To address these issues, several potential solutions are proposed:
Wrapper contract
One idea discussed by @joncinque is the utilization of a wrapper contract that automatically freezes token accounts. Additional logic can be implemented in this wrapper contract, imposing it on users during token transfers.
However, this approach does not sufficiently address the problem of permissionless innovation, as widespread support for custom wrappers from various applications and wallets is unlikely.
Change the runtime to allow fine-grained control over CPIs
Enhancing the Solana runtime to provide developers with fine-grained control over Cross-Program Invocations (CPIs) is another solution. Developers could specify specific access rights for callees, such as restricting the ability to pass signed accounts to other programs via CPIs. Similar to Linux’s containerization tools, these runtime additions would offer enhanced security and control.
A modular token program
The advocated approach in this sRFC is the adoption of a modular token program. This program would allow anyone to create a token handler program and register it with the main token program. The token program would then pass along all calls to the relevant handler (this is defined at the mint), taking care to never pass along signed user accounts. It would do so through two means:
Upon receiving an initialize_x call, such as initialize_token_account, it would pre-allocate any accounts and pass ownership to the handler. This way, users can prevent passing any account initialization ‘payer’ to the handler.
Upon receiving a call where someone needs to be authorized (e.g., from.authority, in the case of a transfer), the token program would authorize the user on behalf of the handler and pass the handler a different signed account to signify that the relevant user has signed.
A proof-of-concept of such a program can be found here.
Open questions
Should a token handler be able to specify extra accounts required for basic instructions like transfers? If so, how should this be standardized?
How important is backwards-compatibility, and what steps can be taken to ensure compatibility with the existing SPL token? How would migration be facilitated?
Are there any security vulnerabilities in this design, and if so, how can they be addressed?
Conclusion
This proposal introduces a method for Solana developers to create new token mechanisms while preserving end-user security. Feedback and questions within this forum are greatly appreciated, particularly from esteemed SPL contributors such as @joncinque, criesofcarrots, and mvines.","[joncinque]: Thanks for bringing this up and thinking so much about the problem. Certainly, the lack of composability with token programs is a huge hindrance to open development in the Solana ecosystem, and I would love to see a better solution than the current monolith of Tokenkeg....
I view this in a very similar way, but rather than having everything go through a centralized program, I’ve always preferred simply having programs that implement many interfaces. For example, Tokenkeg... can really be broken down into a program that contains many different interfaces:
transferable: transfer and transfer_checked
mintable: mint_to
freezable: freeze and thaw
closeable: close_account
approvable: approve and revoke
burnable: burn
initialize mint / account
If we can write and implement program interfaces for each of these, then we can re-compose everything. sRFC 00010: Program Trait - Transfer Spec is the first step towards this future.
In the interface specs, we can allow for an arbitrarily more accounts, to be implemented through an instruction, or through some sort of lookup account as in the “transfer-hook” interface created for token-2022 https://github.com/solana-labs/solana-program-library/pull/4147.
We also need to figure out “state interfaces”, ie for defining initialize_mint and initialize_account, similar to your example, but doing it through a spec / interface rather than a centralized program.
What do you think about this interface approach?

[metaproph3t]: This interface approach makes a lot of sense and pairs nicely with Solana’s use of Rust.
Regarding state interfaces, I’ve pondered potential solutions. One idea is to incorporate a preflight function that returns a structure resembling:
[
 {
 field_name: ""mint_authority"",
 type: Pubkey,
 },
 {
 field_name: ""supply"",
 type: u64,
 },
 {
 field_name: ""freeze_authority"",
 type: Option&lt;Pubkey&gt;,
 }
]
The calling program could then match the field names with its own knowledge (e.g., desiring an Alice mint_authority and a supply of 1000), use None for unknown Option fields, and trigger a revert if encountering non-optional, unknown fields.
However, my primary concern with the interface approach, unless mediated by a program like the one I’ve developed, lies in security. How can we ensure implementors don’t misuse signed inputs? This becomes crucial, especially considering implementors can request additional accounts. Without safeguards, a malicious program might grief the user by creating a 1KB account when it only needs 100 bytes. Couldn’t a program also use the pre-flight mechanism to request a legitimate token program and the user’s token account at that program, thus allowing them to steal the user’s legitimate tokens?
To some extent, user wallets simulate transactions to prevent such risks (e.g., identifying a transaction attempting to steal SOL and aborting it). However, there are ways to bypass these protections, such as malicious codepaths dependent on semi-random events (e.g., stealing funds if wallclock time % 1000 == 0). We could argue that it’s the user’s responsibility to verify the code they interact with, but that weakens Solana’s value proposition, as EVM users, for example, don’t face similar concerns when purchasing tokens on Uniswap.
Thus, it appears necessary to introduce a mediator between the client and implementor, preventing the client’s signed account from leaking through. I would appreciate your thoughts on this matter.

[joncinque]: These concerns are definitely all valid, and could be a model that’s used on top of interfaces, a sort of “safe-interface-wrapper” program that’ll enforce all of the correct signer / writable flags on accounts, downgrade signers, and CPI to the next program.
With the model you’re proposing, since you don’t want a signature to propagate down, then you’ll have to also provide some signed PDA from the interface wrapper program to ensure that this is a “valid” call to the program, which may be a bit restrictive.
Rather than restricting program design, I’d prefer to make the interfaces well-designed, the wallets to catch potentially dangerous situations, and for everyone to make heavy use of token delegates.
For example, you should never send an instruction that requires your wallet to be signer and writable, along with the system program. This is the current pattern with PDA creation, but it stinks! The program should only allocate + assign the PDA, and your wallet can do a direct system transfer of the required lamports to the PDA at the top-level of the transaction, so only the system program gets your wallet as a signer.
If an interface needs to create a PDA from a wallet, it’s very risky for the reasons that you’ve mentioned, and should not be used.
For tokens, the best option is to use the CPI guard extension on token-2022 solana-program-library/instruction.rs at 8f9c33b3a04250938a573809cd9dfdb698025972 · solana-labs/solana-program-library · GitHub
But otherwise, wallets / clients should always use delegates when transferring tokens. A client should never sign a transaction containing an instruction to a program that requires an owner’s signature, their token account, and the token program. Unless that’s the token program, of course.
If an interface requires these things, it should also be changed. And wallets can catch if there’s a potentially risky set of accounts in an instruction.
Or we can consider expanding the runtime / transaction format to “scope” signatures so they can’t go past a few levels. Bad actors can abuse the privilege extension feature for Cross-Program Invocations via system_instruction::transfer, spl_token::instruction::approve, spl_token::instruction::transfer · Issue #17762 · solana-labs/solana · GitHub has some interesting ideas on that.

[metaproph3t]: Thank you for your thoughtful response, sire. I am also not yet convinced that the solution I’ve presented here is the best one.
@ngundotra maybe want to offer your counsel as well, given your commendable work on sRFCs 2, 3, and 10? @joec as well, given your leadership of Nautilus and your involvement in the interfaces project?
For the purposes of clarity, I am going to enumerate the options that have emerged from this discussion thus far.
Option 1: Status quo
In the status quo option, users would continue passing in signed accounts directly to programs. For example, if someone wanted to submit an order to a CLOB, they would need to directly pass their signed token account, which the CLOB would in turn pass to a token program so that the CLOB may claim the user’s funds.
CLOB-status-quo699×131 7.02 KB
Here, a user needs to trust every program that they pass their signed account into. Hence, in order to remain secure, the user would need to do due diligence on programs.
Programs, on the other hand, could adopt a more relaxed posture, since a PDA generally only has control over 1 asset (e.g., a quote_vault of a CLOB holding only quote tokens, so that even a malicious token program wouldn’t be able to steal their other assets). Still, programs would need to ensure that their PDAs are not drained of lamports, which could be done like so:
let pda_balance_before = pda.balance;
solana_program::invoke_signed(/* CPI here */);
assert!(pda.balance == pda_balance_before);
Option 2: Discourage passing in signed accounts
Another option is to discourage passing signed accounts into programs. In the CLOB example, a user would first delegate an amount to the CLOB before their trade, and then the CLOB could pull the funds even without the user’s signed account. This is analogous to the EVM approve and transferFrom combination, although this could be done in a single transaction because Solana transactions can contain multiple instructions.
To create accounts, the program would Allocate and Assign the account, and the user would Transfer lamports to the account in a separate instruction.
Of course, the user would still need to trust the token program they interact with and any other programs that require an account to authorize itself (e.g., an NFT program).
Option 3: Proxy
This is what I originally proposed in this sRFC. Interfaced programs would sit behind a proxy that allocates accounts on behalf of the user and signs on behalf of the user.
proof of concept code
Option 4: Runtime changes
Allow users to sandbox their program invocations.
Some ideas here include:
Allow users to call programs in a way that the program can see the user’s signed account, but it can’t pass it along (this is analagous to msg.sender in the EVM)
Limit how many lamports can be transferred out of an account in an instruction
Further ideas discussed here: Bad actors can abuse the privilege extension feature for Cross-Program Invocations via system_instruction::transfer, spl_token::instruction::approve, spl_token::instruction::transfer · Issue #17762 · solana-labs/solana · GitHub

[metaproph3t]: I personally am unconvinced that any of these approaches is ideal.
Option #1 is possibly the worst one, as it requires users to completely trust programs that they interact with. As a result, users will naturally limit their interactions to a small set of programs.
I consider option #2 a step-function improvement over option #1, but insufficient by itself. In the CLOB example, one must still trust the token program, even if one may not need to trust the CLOB program. In the EVM, on the other hand, you needn’t place such a high degree of trust in token programs.
At first, I considered option #3 estimable (hence the sRFC ), but it also seems insufficient. In the CLOB example, one would still need to pass one’s signed account into the CLOB, even if it will never reach the token program. Doing every CPI through a proxy would also double the number of CPIs, which is likely to remain expensive until runtime v2.
Option #4 seems the least poor choice. However, I am not well-versed enough in the validator codebase to determine what can be introduced without adding performance degradation.

[ngundotra]: metaproph3t:
However, my primary concern with the interface approach, unless mediated by a program like the one I’ve developed, lies in security. How can we ensure implementors don’t misuse signed inputs? This becomes crucial, especially considering implementors can request additional accounts.
The solution to this is to open-source programs, and verify that the code compiles to the program executable that we see on-chain. There’s on going work funded by Foundation to make this an easy-to-use tool.
Realistically, I think we’ll end up in 2 different worlds with some amount of interoperability.
1 - Tokenkeg + Token22 programs
2 - ERC 721 world where each program is its own token with innovative or malicious rules around how it requests accounts and transfers balance
With Token22’s being the bridge between the two worlds (cc @joncinque).
 metaproph3t:
A modular token program
The advocated approach in this sRFC is the adoption of a modular token program. This program would allow anyone to create a token handler program and register it with the main token program. The token program would then pass along all calls to the relevant handler (this is defined at the mint), taking care to never pass along signed user accounts. It would do so through two means:
I think this is a great idea, and would love to see more research here. @austbot came up with a similar program structure he called Digital Asset Spec that has the same spirit: shared state, modular programs control specific logic (royalty payout, transfer rules, trait swaps, etc), and a slim program that defines this lifecycle.
If you continue to build this out, I’d recommend pursuing 2 checkpoints:
Can you build a marketplace that swaps / lists assets built with your modular token interface?
Can you index assets appropriately given your token interface?
Thanks for your research so far @metaproph3t !

",metaproph3t,1959,4,6,7,2023-05-16T10:54:29.986Z,2023-06-07T15:34:07.370Z,sRFC,6,2023-06-07T15:34:07.370Z
268,sRFC 00016: Generalized Ownable Indexable Assets,https://forum.solana.com/t/srfc-00016-generalized-ownable-indexable-assets/268,"Generalized Ownable Indexable Assets
Summary
This is a standard protocol to enables RPCs to track assets that programs to create, replace, update, and delete for users. Indexers will be able to interpret “owned” program data by executing a pre-determined simulated view function on the program.
Motivation
We want to support forking of commonly used protocols, so that big enterprises can have full control and maintainability over their contracts, while also retaining the ability to have their program’s data indexed &amp; shown in wallets.
Centralized control of commonly used protocols leads to catastrophic failure scenarios when the trusted operator is compromised. Short term work-arounds in previous upheavals, e.g. the genesis of OpenBook, have failed to produce meaningful abstraction patterns on Solana. As developers on Solana begin to realize the risks of centralized protocols, they may want to fork even the SPL Token program.
However they will quickly run into issues getting adoption into marketplaces, dApps, and wallets. This is both a social and technical problem. We hope that this standard for indexing will solve the technical problem, and thus reduce the tension in the social problem of adoption.
Specification
Require the usage of CPI events. See sRFC #00013.
We propose that programs control ownable assets for wallets using the following 4 CRUD event structs.
The payload of these events is used to manage an Asset which has an ID of type Pubkey, and consists of an ordered array of Pubkeys.
// Inform indexers that a new Asset Group was created for an authority
pub struct CrudCreate {
 asset_id: Pubkey,
 authority: Pubkey,
 pubkeys: Vec&lt;Pubkey&gt;,
 data: Vec&lt;u8&gt;
}
// Inform indexers to change both authority &amp; pubkeys
pub struct CrudReplaceKeys {
 asset_id: Pubkey,
 authority: Pubkey,
 pubkeys: Vec&lt;Pubkey&gt;
}
// Inform indexers to update the bytes for an asset group
pub struct CrudUpdateBytees {
 asset_id: Pubkey,
 owner: Pubkey
}
// Inform indexers to delete the asset 
pub struct CrudDestroy {
 asset_id: Pubkey
}
Indexers will store assets issued by programs, so that you will always be able to query /getAssetsForOwner { program_id, wallet } and have it return a list of Asset { id: Pubkey, pubkeys: Vec&lt;Pubkey&gt;, data: Vec&lt;u8&gt;}.
Optionally, indexers can ask the program for a human readable interpretation of the Asset’s data by simulating view functions on the program that it belongs to. This is done by sending the getAssetData anchor instruction with no arguments, and deserializing the return_value as a JSON.
Programs that comply with this spec will have similar implementation as below:
#[program]
pub mod my_program {
 ...
 pub fn get_asset_data(ctx: Context&lt;GetAssetData&gt;, data: Vec&lt;u8&gt;) -&gt; Ok(()) {
 let asset_id_account = &amp;ctx.accounts.asset_id.to_account_info();
 let asset_accounts = &amp;ctx.remaining_accounts.to_vec();
 // interpret asset data
 let my_json_bytes = // serialize asset data here
 set_return_data(my_json_bytes)
 Ok(())
 }
}
#[derive(Accounts)]
pub struct GetAssetData&lt;'info&gt; {
 /// CHECK:
 pub asset_id: AccountInfo&lt;'info&gt;,
 /// CHECK:
 pub authority: AccountInfo&lt;'info&gt;,
}
This will allow indexers to serve JSON data from each Asset that the program has issued.
Implementation:
TBD","[nickfrosty]: Does this concept require indexers to all be running an implementation of the Digital Asset Standard API (set forth by Metaplex)?
Or would there be a more generalized implementation of this endpoint/view function calling getAssetsForOwner and getAssetData that gets baked into RPC nodes/indexers?

[ngundotra]: There would be a more generalized version of the indexer that supports those two RPC calls (getAssetsForOwner and getAssetData).

",ngundotra,548,1,2,3,2023-05-24T14:48:18.095Z,2023-05-29T14:35:49.036Z,sRFC,6,2023-05-29T14:35:49.036Z
51,sRFC 00006: Writing SVG Images as PDAs of a Solana Program to implement On-chain images for NFTs,https://forum.solana.com/t/srfc-00006-writing-svg-images-as-pdas-of-a-solana-program-to-implement-on-chain-images-for-nfts/51,"This proposal discusses storing image data in base64 SVG format directly to a Solana address, using a data URL header to make it natively readable by browsers. The image storage account is a PDA generated with the NFT token pubkey as a seed, making it easy to retrieve the image knowing just the programID and the NFT token pubkey.
Showcased below is the code used to implement Blockrons on-chain images, open sourced by me under CC1 (just give a thanks/attribution or something)
1. Main program - Lib.rs (written in solpg)
use anchor_lang::prelude::*;
declare_id!(""BLKRNygNc7mWMpxQPXs4UoVPyApvyqy4v8uK9F3iJmqS"");
#[program]
// Smart contract functions
pub mod imager {
 use super::*;
 // creates the image storage account
 pub fn create_imager(ctx: Context&lt;CreateImager&gt;,token: Pubkey) -&gt; Result&lt;()&gt; {
 let imager = &amp;mut ctx.accounts.imager;
 imager.authority = ctx.accounts.authority.key();
 imager.token = token;
 imager.img = ("""").to_string();
 Ok(())
 }
 // puts data inside the image storage account by concatenating strings &lt;- client-side to fit the solana txn limit
 pub fn update_imager(ctx: Context&lt;UpdateImager&gt;,token: Pubkey, image: String) -&gt; Result&lt;()&gt; { 
 let imager = &amp;mut ctx.accounts.imager;
 imager.authority = ctx.accounts.authority.key();
 imager.token = token;
 imager.img = format!(""{}{}"", imager.img, image); 
 Ok(())
 }
}
// Data validators
//token input (nft token addy) + programID are the main seeds for PDA
#[derive(Accounts)]
#[instruction(token: Pubkey)]
pub struct CreateImager&lt;'info&gt; {
#[account(mut)]
 authority: Signer&lt;'info&gt;,
 #[account(
 init_if_needed,
 seeds = [token.as_ref()],
 bump,
 payer = authority,
 space = 10000
 )]
 imager: Account&lt;'info, Imager&gt;,
 system_program: Program&lt;'info, System&gt;,
}
#[derive(Accounts)]
pub struct UpdateImager&lt;'info&gt; {
 authority: Signer&lt;'info&gt;,
 #[account(mut, has_one = authority)]
 imager: Account&lt;'info, Imager&gt;,
}
// Data structures
#[account]
pub struct Imager {
 authority: Pubkey,
 token: Pubkey,
 img: String,
}
^^On-chain program is simple and only uses 2 functions - 1. create the PDA and 2. store data to the PDA thru string concatenation with multiple txns (to bypass solana’s byte txn limit)
2. Client side implementation - also deployed using solpg:
const systemProgram = anchor.web3.SystemProgram;
// EDIT THIS: token = public nft account addy you want to tie your on-chain image to
const token = new web3.PublicKey(""NFT PUBKEY HERE"");
// EDIT THIS: dataS = string of the base64 info
const dataS = new String(""BASE64 SVG DATA HERE"");
// do not touch, automatically divide dataS into strings to split transactions
var string1 = new String(dataS.slice(0,800));
var string2 = new String(dataS.slice(800,1600));
var string3 = new String(dataS.slice(1600,2400));
var string4 = new String(dataS.slice(2400,3200));
var string5 = new String(dataS.slice(3200,4000));
var string6 = new String(dataS.slice(4000,4800));
var string7 = new String(dataS.slice(4800,5600));
// program logic
 const [imagerPubkey, _] = await anchor.web3.PublicKey.findProgramAddress(
 [token.toBytes()],
 pg.program.programId
 );
 console.log(""Your imager address"", imagerPubkey.toString());
// create image storage account
 const [imager, _imagerBump] =
 await anchor.web3.PublicKey.findProgramAddress(
 [token.toBytes()],
 pg.program.programId
 );
 const tx = await pg.program.methods
 .createImager(token)
 .accounts({
 authority: pg.wallet.publicKey,
 imager: imager,
 systemProgram: systemProgram.programId,
 })
 .rpc();
// transact all 7 string parts 
 const tx1 = await pg.program.methods
 .updateImager(token,string1.toString())
 .accounts({
 imager: imagerPubkey,
 })
 .rpc(); 
 const tx2 = await pg.program.methods
 .updateImager(token,string2.toString())
 .accounts({
 imager: imagerPubkey,
 })
 .rpc();
 const tx3 = await pg.program.methods
 .updateImager(token,string3.toString())
 .accounts({
 imager: imagerPubkey,
 })
 .rpc();
 const tx4 = await pg.program.methods
 .updateImager(token,string4.toString())
 .accounts({
 imager: imagerPubkey,
 })
 .rpc();
 const tx5 = await pg.program.methods
 .updateImager(token,string5.toString())
 .accounts({
 imager: imagerPubkey,
 })
 .rpc();
 const tx6 = await pg.program.methods
 .updateImager(token,string6.toString())
 .accounts({
 imager: imagerPubkey,
 })
 .rpc();
 const tx7 = await pg.program.methods
 .updateImager(token,string7.toString())
 .accounts({
 imager: imagerPubkey,
 })
 .rpc();
 
 console.log(""Done!""); 
// displays current data 
// console.log(""Your imager"", imager);
^^Client side program is a little bit more complex as it is built to breakdown a long string into 7 diff txns of 800 bytes each - to fit solana’s txn limit. Currently handles for strings upto 5600 bytes but of course this can be extended until the PDA max of 10,000 bytes.
Typically, each Blockron image is 32x32 pixels and takes up 3-4kb of space.","[Hamster]: Example of a data stored inside the string, inside the variable dataS:
data:image/svg+xml;base64,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
This is an SVG file in base 64 format, once you copy this entire string and paste it into a new browser window, it will automatically resolve into an image.

[blockiosaurus]: I like this! And I think a base64 image data image string nested in a JSON file would be the way to go, or at least the easiest to implement. As far as I can tell this doesn’t need to be a contract change. If we just check the protocol format (https:// or solana://) and use a getAccount instead of a fetch in the Metaplex JS SDK then it should work perfectly. Both already return buffers so interpretation would work the same way.

[Hamster]: Appreciate the comment, what would the JSON file look like? It will be directly stored in a solana account as a json variable?
Then inside the JSON, can we put another solana address for the image link? or should that be the image string in data URL format directly?
That way we can reduce the complexity to 1 solana account but limit the image size further.

[blockiosaurus]: It would just be the standard JSON metadata. All Metaplex NFTs have an on-chain URI which points to the JSON file, which points to the image URI. We’d just be replacing the image URI with a data:image string instead of a pointer to another account. You don’t even need a special program or anything, you just encode the JSON bytes directly in the Solana account.

[Hamster]: blockiosaurus:
you just encode the JSON bytes directly in the Solana account.
How do you do this? Have not seen anything in solana documents all the tutorials I can find are how to put a counter variable on-chain lmao. It took a lot of testing to even get a string working

[blockiosaurus]: I guess you’d still have to write a program to do the actual write, but strings are just series of bytes so when you write the account (assuming non-anchor) you shove them directly into the account info data.

[Hamster]: Okay doing some testing to store the json object, on-chain. First, we create a suitable JSON that has no off-chain or exterior links.
{""name"":""Blockrons 020"",""symbol"":""BLKRNS"",""description"":""Each Blockrons NFT has its art asset stored directly on-chain in an image container address. \n\nBlockrons 020 - \""Nomad\"" \n\nImage container address: 4kHmoHh6FYp4ae8a3Nj5N64hbs2hB2AGfuhraCaNrGg7"",""seller_fee_basis_points"":0,""image"":""data:image/svg+xml;base64,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"",""attributes"":[],""properties"":{""creators"":[{""address"":""F1QyW2RiabaUTHYYMZs6kVQmjw3QzhRWtAJNUp6ifWAe"",""share"":100}]}}
Above is the json file used by the Blockrons 020 nft, solscan here: Solscan. We can successfully use url-data formatted base64 svg code as an image instead of a file link - it is read normally by browsers and wallets.
Next step is to put this json on-chain and make it readable. Lemme try by storing it as a basic string if that works, should still be readable on buffer

[Hamster]: Okay stored the whole JSON as a string into this pda account:
4kHmoHh6FYp4ae8a3Nj5N64hbs2hB2AGfuhraCaNrGg7
On typescript clients, if you use connection.getAccountInfo(key), it should be able to show up in the read buffer data.
Would this work with:
 blockiosaurus:
(https:// or solana://) and use a getAccount
?

",Hamster,985,6,8,9,2023-03-21T15:07:05.314Z,2023-04-08T16:42:08.534Z,sRFC,6,2023-04-08T16:42:08.534Z
206,sRFC 00012: Wallet Delegation Standard for Secure Proof of Ownership,https://forum.solana.com/t/srfc-00012-wallet-delegation-standard-for-secure-proof-of-ownership/206,"Wallet Delegation Standard
Summary
This RFC introduces a delegation standard to mitigate loss, theft, and unauthorized access to funds stored in cold wallets. The standard maintains the offline nature of cold wallets, allowing them to delegate ownership of assets to dedicated hot wallets, thus allowing secure and controlled proof of ownership of assets.
Motivation: Proof of Ownership
Proof of ownership is a critical yet highly neglected problem in the Solana ecosystem. In its current form, it’s highly insecure
users must connect their cold wallets to unverified websites, or
transfer their valuable assets to warm wallets
Goals
This standard aims to remedy this by providing a secure way to delegate assets from cold to hot wallets. This allows users to prove ownership of their assets without risking them by connecting to insecure websites or moving them around.
Background
Delegation
Delegating an asset from a cold wallet → hot wallet: The hot wallet is shadowing as the owner of the asset, and the hot wallet does NOT own any write/transfer rights over this asset, nor does it have the ability to transfer these rights to anyone else. The cold wallet remains the true owner of this asset on-chain.
Actions
Two main actions can be performed using the proposed protocol:
Delegation
Revoking Delegation
Delegation Types
Full Wallet Delegation: The entire wallet gets delegated, including all assets that are owned by the wallet.
Token Delegation: Tokens having the same mint address can be delegated. This includes fungibles, semi-fungibles, and non-fungibles.
Proposed Implementation
PDAs can be used to derive delegation accounts for full wallet or individual token Delegation.
DelegationAccount
To delegate wallet A to another wallet B, a DelegationAccount can be derived using the address of wallet A.
#[account]
pub struct DelegateAccount {
 // Cold Wallet
 pub authority: Pubkey,
 pub hot_wallet: Pubkey,
}
where seeds for DelegateAccount:
[
 DelegateAccount::PREFIX.as_bytes(),
 authority.key().as_ref(),
]
TokenDelegationAccount
In the case of token Delegation, i.e., delegating a token in wallet A with ATA (associated token account) T to wallet B, a TokenDelegationAccount can be derived using the address of wallet A and the ATA T.
#[account]
pub struct DelegateTokenAccount {
 pub authority: Pubkey,
 pub hot_wallet: Pubkey,
 pub token_account: Pubkey,
}
where seeds for DelegateTokenAccount:
[
 DelegateTokenAccount::PREFIX.as_bytes(),
 authority.key().as_ref(),
 token_account.key().as_ref(),
]
Custos Delegatation Protocol1434×1116 54.2 KB
Revoking Delegations
The authority of the Delegation can revoke delegations by closing the PDA.
#[derive(Accounts)]
pub struct RevokeDelegate&lt;'info&gt; {
 #[account(mut)]
 pub authority: Signer&lt;'info&gt;,
 #[account(mut, has_one=authority, close=authority)]
 pub delegate_account: Account&lt;'info, DelegateAccount&gt;,
}
#[derive(Accounts)]
pub struct RevokeTokenDelegate&lt;'info&gt; {
 pub authority: Signer&lt;'info&gt;,
 #[account(mut, has_one=authority, close=authority)]
 pub delegate_token_account: Account&lt;'info, DelegateTokenAccount&gt;,
}
Fetching Delegations
This standard would require an SDK for client-side fetching delegated wallets. When a user connects their wallet, the SDK can fetch all the assets delegated to the given (connected) hot wallet. Here is a reference implementation of the SDK:
getUserDelegates = async (hotWallet: PublicKey) =&gt; {
 const data = await this.delegationProgram.account.DelegateAccount.all([
 {
 memcmp: {
 offset: 8 + 32,
 bytes: hotWallet.toBase58(),
 },
 },
 ]);
 return data;
};
getTokenDelegates = async (hotWallet: PublicKey) =&gt; {
 const data = await this.delegationProgram.account.DelegateAccount.all([
 {
 memcmp: {
 offset: 8 + 32,
 bytes: hotWallet.toBase58(),
 },
 },
 ]);
 return data;
};
Potential Use-Cases
Claiming Airdrops
Claiming Allowlists
Token-gated mints
Token-gated games
DAO Governance
On-chain reputation
Verifiable Wallet aggregation
Ongoing Investigation
Supporting Programmable Wallets: Smart contract wallets like Multisigs are not system accounts; hence, the current implementation for the standard can be changed to support PDAs that represent programmable wallets.
Allowing tightly scoped rights to the delegated hot wallet for making transactions on behalf of the cold wallet.
Delegation Expiration: Current implementation requires users to manually revoke a delegation if they no longer want a given hot wallet to be the Delegation for a cold wallet. Automatic expiration could be introduced to let users set a dedicated interval, after which the Delegation/TokenDelegationAccount will be closed.
Call For Action
Readers: Requesting readers to provide feedback, audit reference implementation, and contribute to the codebase.
Dapps: Requesting dapps to explore the SDK reference implementation and provide feedback for making integration of this standard easy.
Wallets: Requesting wallets to explore the SDK reference implementation to provide users an easy way to perform the Delegation inside the wallets themselves - connecting cold wallets to any website (even one that is created specifically for this standard) goes against the very core principle behind this standard.
Reference Implementation
Delegation Program: GitHub - custosprotocol/custos: Custos Protocol
JS SDK: GitHub - custosprotocol/custos-js-sdk: JavaScript SDK for Custos Protocol
Citation
Anvit Mangal &lt;@0xprof_lupin&gt;, Pratik Saria &lt;@PratikSaria&gt;. “sRFC 00012: Wallet Delegation Standard for Secure Proof of Ownership,” https://forum.solana.com/t/srfc-00012-wallet-delegation-standard-for-secure-proof-of-ownership, May 2023.","[silo]: Why limit delegation to a single delegate?
There’s definitely a case to be made for having multiple delegates for a single cold wallet, especially in this mobile era.

[anvit]: One to many delegation was designed for a specific reason: in use cases where a user needs to be airdropped a single token / whitelisted only once per token/wallet, many to many delegation would fail as it the delegate fetcher wont know which wallet to whitelist/airdrop/allowed to enter a token-gated website.
But as you said, there are use cases where many to many mappings would be beneficial.

[AdeleneJennifer]: Is a single delegate to limit delegation?

",anvit,1405,1,3,4,2023-05-07T11:06:05.612Z,2023-05-08T20:41:29.963Z,sRFC,6,2023-05-08T20:41:29.963Z
122,sRFC 00010: Program Trait - Transfer Spec,https://forum.solana.com/t/srfc-00010-program-trait-transfer-spec/122,"Code: GitHub - ngundotra/additional-accounts-request-transfer-spec: Proof of Concept for Additional Accounts Request for Transfers
SRFC 00010 - Program Trait - Transfer Spec
This spec is currently alpha and subject to change
Summary
A standard protocol to enable on-chain and client communication with Solana programs to “transfer” assets that allows target programs to require additional accounts.
Motivation
A standard protocol for enabling programs to support ""transfer""ring assets while also allowing a flexible number of accounts into the program allows for a better user experience across apps and wallets in the Solana ecosystem.
By defining a protocol to resolve additional accounts required for programs to adhere to the same instruction interface, developers can build applications that are compatible with a wide range of programs.
Calling programs should ensure that called programs are using the additional accounts appropriately, or otherwise fail instruction execution.
Developers implementing this specification should be prepared to chew glass.
By standardizing a simple approach to solving program abstraction, we ensure basic compatibility of programs and clients so developers can focus on higher level abstractions.
Specification: Program Trait - Transfer
Executing a “transfer” instruction against a program that implements ProgramTraitTransferV1 requires two CPIs from the caller program to the callee program.
The first CPI from the caller to the callee is to determine which (if any) additional accounts are require for the 2nd CPI.
The second CPI from the caller to the callee is with the same list of accounts from the 1st call, but also passes the list of accounts requested by the first CPI.
The ProgramTraitTransferV1 trait requires that programs implement two instructions, described below.
use anchor_lang::prelude::*;
/// Accounts required by ProgramTraitTransferV1
#[derive(Accounts)]
pub struct ITransfer&lt;'info&gt; {
 /// CHECK:
 pub owner: AccountInfo&lt;'info&gt;,
 /// CHECK:
 pub to: AccountInfo&lt;'info&gt;,
 pub authority: Signer&lt;'info&gt;,
 /// CHECK:
 pub mint: AccountInfo&lt;'info&gt;,
}
#[derive(Accounts)]
pub struct MyProgramTransfer {
 /// CHECK:
 pub owner: AccountInfo&lt;'info&gt;,
 /// CHECK:
 pub to: AccountInfo&lt;'info&gt;,
 pub authority: Signer&lt;'info&gt;,
 /// CHECK:
 pub mint: AccountInfo&lt;'info&gt;,
 
 // Additional optional accounts follow here
 pub my_special_account: AccountInfo&lt;'info&gt;,
 // etc
}
#[program]
pub mod MyProgram {
 pub fn preflight_transfer(ctx: Context&lt;ITransfer&gt;, amount: u64) -&gt; Result&lt;()&gt; {
 // Your code goes here 
 set_return_data(
 &amp;PreflightPayload {
 accounts: vec![
 IAccountMeta {
 pubkey: *my_special_account_key,
 // You cannot request additional signer accounts
 signer: false, 
 // You may however request additional writable or readonly accounts
 writable: true,
 },
 ]
 }.try_to_vec()?
 )?;
 Ok(())
 }
 pub fn transfer(ctx: Context&lt;MyTransfer&gt;, amount: u64) -&gt; Result&lt;()&gt; {
 // Your code goes here
 Ok(())
 }
}
enum MyProgramInstruction {
 ...,
 PreflightTransfer(u64)=solana_program::hash::hash(""global:preflight_transfer"")[..8],
 Transfer(u64)=solana_program::hash::hash(""global:transfer"")[..8]
}
Executing “transfer” against a conforming program is interactive because optional accounts may be sent in the 2nd CPI. The optional accounts are derived from the 1st CPI by Borsh deserializing return data as Vec&lt;AccountMeta&gt;.
Accounts
The accounts list required for adhering to ProgramTraitTransferV1 is simply a list of account metas, that have no direct relationship to each other.
We overlay semantic descriptions to give advice on how this should be used, but ultimately we expect that there will be program implementations that abuse the
semantic descriptions.
Owner
isSigner: false
isWritable: false
This is the owner of the asset to be transferred.
To
isSigner: false
isWritable: false
This is the intended recipient of the transferred asset.
Authority
isSigner: true
isWritable: false
This is the account that has the authority to transfer from owner to the recipient. For example, this may be the same pubkey as owner.
Mint
isSigner: false
isWritable: false
This account was included for Token* compatability.
This account is meant to be your implementing program’s program id, so calling programs know which program to execute.
Or, it can be used as a token* Mint account, which allows programs to decide if they need to execute a token* CPI or a ProgramTraitTransferV1.
Instructions
The instructions formats are described below
Amount
Both instructions have a single parameter amount which must be serialized &amp; deserialized as a little-endian u64.
preflight_transfer
This instruction’s data has an 8 byte discriminantor: [0x9d, 0x84, 0xf5, 0x5a, 0x61, 0xea, 0x7b, 0xe2], followed by u64 serialized in little-endian format.
And no other bytes.
The accounts to this instruction are:
vec![
 // owner
 AccountMeta {
 pubkey: owner,
 isSigner: false,
 isWritable: false,
 }
 // to
 AccountMeta {
 pubkey: to,
 isSigner: 
 isWritable:
 }
 // authority
 AccountMeta {
 pubkey: authority,
 isSigner: true,
 isWritable: false,
 }
 // mint
 AccountMeta {
 pubkey: mint
 isSigner: false,
 isWritable: false
 }
]
Return data for this instruction is a vector of AccountMetas, serialized as ReturnData.
#[derive(BorshSerialize, BorshDeserialize)]
pub struct IAccountMeta {
 pub pubkey: Pubkey,
 pub signer: bool,
 pub writable: bool,
}
pub type ReturnData = Vec&lt;IAccountMeta&gt;;
transfer
This instruction’s data has an 8 byte discriminantor: [0xa3, 0x34, 0xc8, 0xe7, 0x8c, 0x03, 0x45, 0xba], followed by u64 serialized in little-endian format.
And no other bytes.
The accounts to this instruction are:
vec![
 // owner
 AccountMeta {
 pubkey: owner,
 isSigner: false,
 isWritable: false,
 }
 // to
 AccountMeta {
 pubkey: to,
 isSigner: 
 isWritable:
 }
 // authority
 AccountMeta {
 pubkey: authority,
 isSigner: true,
 isWritable: false,
 }
 // mint
 AccountMeta {
 pubkey: mint
 isSigner: false,
 isWritable: false
 },
]
Additional account metas returned from the previous call to preflight_transfer must be appended to the list of accounts, in the order they were deserialized.
Off-Chain Usage
In order to craft a transfer TransactionInstruction to a program that adheres to ProgramTraitTransferV1, you can simulate the
preflight_transfer instruction with the required accounts, in order to get the list of additional AccountMetas.
Then you can append those AccountMetas to the remaining accounts.
Reference code is provided below, written using @coral-xyz/anchor.
import * as anchor from '@coral-xyz/anchor';
async function resolveRemainingAccounts&lt;I extends anchor.Idl&gt;(
 program: anchor.Program&lt;I&gt;,
 simulationResult: RpcResponseAndContext&lt;SimulatedTransactionResponse&gt;
): Promise&lt;AccountMeta[]&gt; {
 let coder = program.coder.types;
 let returnDataTuple = simulationResult.value.returnData;
 let [b64Data, encoding] = returnDataTuple[""data""];
 if (encoding !== ""base64"") {
 throw new Error(""Unsupported encoding: "" + encoding);
 }
 let data = base64.decode(b64Data);
 // We start deserializing the Vec&lt;IAccountMeta&gt; from the 5th byte
 // The first 4 bytes are u32 for the Vec of the return data
 let numBytes = data.slice(0, 4);
 let numMetas = new anchor.BN(numBytes, null, ""le"");
 let offset = 4;
 let realAccountMetas: AccountMeta[] = [];
 const metaSize = 34;
 for (let i = 0; i &lt; numMetas.toNumber(); i += 1) {
 const start = offset + i * metaSize;
 const end = start + metaSize;
 let meta = coder.decode(""ExternalIAccountMeta"", data.slice(start, end));
 realAccountMetas.push({
 pubkey: meta.pubkey,
 isWritable: meta.writable,
 isSigner: meta.signer,
 });
 }
 return realAccountMetas;
}
This is used like so:
// Simulate the `preflight_transfer` instruction
const preflightInstruction = await wrapper.methods
 .preflightTransfer(new anchor.BN(1))
 .accounts({
 to: destination,
 owner: wallet,
 authority: wallet,
 mint: iProgram.programId,
 })
 .remainingAccounts([])
 .instruction();
let message = MessageV0.compile({
 payerKey: wallet,
 instructions: [preflightInstruction],
 recentBlockhash: (
 await wrapper.provider.connection.getRecentBlockhash()
 ).blockhash,
});
let transaction = new VersionedTransaction(message);
// Deserialize the `AccountMeta`s from the return data
// We have to use VersionedTransactions to get `returnData`
// back from simulated transactions 
let keys = await resolveRemainingAccounts(
 wrapper,
 await wrapper.provider.connection.simulateTransaction(transaction)
);
// Send the actual `transfer` instruction with the required additional
// accounts
const tx = await wrapper.methods
 .transfer(new anchor.BN(1))
 .accounts({
 owner: wallet,
 to: destination,
 authority: wallet,
 mint: iProgram.programId,
 })
 .remainingAccounts(keys)
 .rpc({ skipPreflight: true });
console.log(""Transferred with tx:"", tx);
Compatability: SPL Token
SPL tokens are compatible with this format.
There is a provided program programs/token-wrapper that shows how to “wrap” tokenkeg to make it compatible with ProgramTraitTransferV1.
Limitations
When returning a vector of account metas in the preflight_transfer instruction, additional account metas must have isSigner: false.
Requiring additional signer account metas must come in the form of a new ProgramTrait specification.
Reference
There is a reference implementation of a program adhering to ProgramTraitTransferV1 under programs/token-program of a program that records which pubkey owns how much of a token in a singleton address.
Calling transfer on this program will change decrement the owner’s stored balance by amount and increment the recipient’s balance by amount.
Tests
To run a test against this program, run anchor test.","[joec]: What’re your thoughts on adding the version of the implementing program you’re hitting?
For example, you could include the version of the program in the return data, so that PreflightPayload will capture the version of the implementing program.
This way, if developers introduce breaking changes to the program, you can validate the version upon preflight.
ie:
#[derive(Debug, Clone, AnchorDeserialize, AnchorSerialize)]
pub struct PreflightVersionPayload {
 major: u8,
 minor: u8,
}
#[derive(Debug, Clone, AnchorDeserialize, AnchorSerialize)]
pub struct PreflightPayload {
 pub version: PreflightVersionPayload
 pub accounts: Vec&lt;IAccountMeta&gt;,
}

[joncinque]: This looks really great. It addresses the most important use case for interfaces in a very approachable way.
In this example, it comes with the assumption that all information must be present between the 4 accounts provided and the program logic in order to derive the additional accounts.
This is fine for transfers that CPI once into a well-known program, but for transfer programs that CPI to another interface during transfer, it doesn’t address passing additional accounts in the preflight to derive the next level of additional accounts.
For example, if my transfer program CPIs into a “pausable” interface, and the pausable interface requires another account that gives the “paused” info, the preflight is insufficient. TLV structures will help, but if the “pausable” program ends up requiring a sysvar account, we’re kinda screwed.
This might be addressable by saying that the preflight gets the required accounts PLUS all “remaining accounts” in Anchor speak. What do you think?
A nit: the term owner is overloaded, why not just from or source or bag (j/k)? Especially for people already comfortable with Solana development, it’s confusing to change its meaning from “the address that can fully authorize transfers and delegations” to “the address of the token account”.
Also also: owner and to must be writable, correct?

",ngundotra,1069,0,2,3,2023-04-22T22:38:28.494Z,2023-04-27T16:13:14.965Z,sRFC,6,2023-04-27T16:13:14.965Z
123,Standard for supporting multiple SVM chains on Solana wallets,https://forum.solana.com/t/standard-for-supporting-multiple-svm-chains-on-solana-wallets/123,"sRFC 00011: A standard for implementing multi-chain SVM support in Solana Wallets
Motivation
This sRFC proposes a standard for implementing multi-chain SVM(eg: Eclipse chains) support in Solana Wallets, allowing Solana applications (“dapps”) to suggest chains to be added to the user’s wallet application. This enables convenient interaction with dapps on testnets, canary chains, or other SVM chains without requiring complex manual custom RPC changes. The standard aims to streamline the process and prevent inconsistencies across multiple standards and one-off implementations.
Inspiration
Inspired by EIP-3085, this design allows Solana applications to suggest chains to be added to the user’s wallet application. The wallet application may arbitrarily refuse or accept the request. We have received a request for this feature from many dapps.
The problem
Unlike Ethereum, Solana doesn’t have chain ids that are used to prevent replay attacks. Here is the solution we propose to have multichain support while not introducing breaking changes to Solana.
Solution A: Whitelist Registry
eg:- On Keplr, when you first create a wallet, you get 10 of the most popular cosmos chains added by default; it would also make sense to do this for Solana wallets with SVM chains.
The workflow for adding new chains should be like this: User clicks add chain → Wallet checks if the RPC is whitelisted → New chain is added.
Question: Where would we want to store the registry of the whitelisted RPCs?
A. We could ship it with the wallet if it’s a low number(&lt;50).
B. If it’s larger, put it on a database like Postgres or something decentralized like Arweave, IPFS, etc.
More technical details
Implementation
Solana Wallet Standard: At first glance there are some changes needed to the Solana wallet standard. There are places where the wallet is required to return all chains that it supports, so we would need some interface for the dapp to pass back a new chain to be added: wallet-standard/account.ts at master · solana-labs/wallet-standard · GitHub This change seems like it could be made immediately if it is done in a non-breaking way. The wallet should not be required to add a chain even if the dapp requests it.
Wallet Adapter: We would need to make some change to the wallet adapter so the dapp can specify the desired RPC network. This would need to be rolled out after the Wallet Standard change.
Dapp Support: The dapp updates their Wallet Adapter library and specify their desired RPC network. If no RPC network is specified, by default this change has no effect - the behavior is exactly the same as SVM wallets function right now (default to Solana mainnet). This is the last change to occur.
More info.
Github discussion: https:// github. com/solana-labs/wallet-standard/issues/9
Backpack PR: https:// github. com/coral-xyz/backpack/pull/3730",,PrasoonPratham,949,0,0,1,2023-04-23T19:41:55.578Z,2023-04-23T19:41:55.658Z,sRFC,6,2023-04-23T19:41:55.658Z
31,sRFC 00003: On-chain interface account resolution,https://forum.solana.com/t/srfc-00003-on-chain-interface-account-resolution/31,"On-chain interface account resolution
Summary
As the Solana program ecosystem matures, program interfaces will become the main means of building the composable future. Developers will still be able to innovate with their protocols to do anything, but by having their programs also adhere to interfaces, they can get immediate support with the rest of the ecosystem.
For example, as long as a program implements instruction processors for all of the possible “spl-token” instructions, and their structures conform to the Mint and TokenAccount types, then any marketplace or trading program can also use that program without any additional work.
This approach currently works, but it’s very limited. For example, if a program that implements the “spl-token” interface needs one more account to properly process a transfer (for example, the instructions sysvar), then both the client and on-chain programs need to figure out how to resolve the required accounts. SRFC 00002 solves the problem during transaction creation, but programs must also be able to construct CPI instructions on-chain. To put it differently, given a list of accounts, a program must be able to construct instructions in order to perform a CPI, without the program knowing everything about the target program.
Let’s walk through a concrete example.
Permissioned transfer for two different token types
Let’s say there’s a token marketplace program with some form of bids and asks on tokens. It doesn’t matter how exactly the program works, but at some point, it needs to transfer two different token types in one instruction, which we’ll call A and B. A and B could belong to different token programs, that all implement the “spl-token” interface.
As part of the “spl-token” transfer interface, a token program may also CPI into another program, which adheres to a “permission-transfer-check” interface. This nested “permissioned-transfer-check” interface requires at least the program, the token mint account, a PDA derived from the mint, and any number of additional accounts to validate the transfer.
Let’s assume that the client has properly constructed the transaction, so that all necessary accounts are available to the program. How does the program figure out which accounts are needed to construct the CPI instruction to transfer token A?
Possible solution
The “spl-token” transfer interface specifies certain “guaranteed” accounts: source token account, destination token account, mint, and authority. Also, the accounts in the program (the token account and mint) must conform to a certain structural definition.
After that, any other required account must be derivable from those required four. Additional account addresses may be derived as new program-derived addresses or read from account data. These additional accounts must also be provided after all of the “guaranteed” accounts in the marketplace interface.
For example, in Rust pseudo-code, that could be:
MarketplaceSwap {
 token_a_source: TokenAccount,
 token_a_destination: TokenAccount,
 token_a_mint: Mint,
 token_a_authority: Signer,
 token_b_source: TokenAccount,
 token_b_destination: TokenAccount,
 token_b_mint: Mint,
 token_b_authority: Signer,
 additional_accounts: &amp;[AccountInfo]
}
To create an instruction to transfer token A, the token interface exposes an instruction creator:
fn create_transfer_instruction(
 program_id: &amp;Pubkey,
 source: &amp;TokenAccount,
 destination: &amp;TokenAccount,
 mint: &amp;Mint,
 authority: &amp;AccountInfo,
 additional_accounts: &amp;[AccountInfo]
) -&gt; Instruction;
The interface instruction creator looks inside the mint, finds that it needs a CPI into the “permission-transfer-check” interface, finds the program in additional_accounts, along with a required PDA, and passes it down to one more nested function to extract the next level of additional accounts:
fn get_additional_accounts_for_permission_transfer_check(
 program_id: &amp;Pubkey,
 mint: &amp;Mint,
 additional_accounts: &amp;[AccountInfo],
) -&gt; [AccountMeta];
Given all of these, the marketplace program can construct the full instruction with all of the required accounts, and finally pass them all to the token program.
One level deeper, the token program will need to perform just one round on-chain account resolution to get the “permission-transfer-check” instruction:
fn create_permission_transfer_check_instruction(
 program_id: &amp;Pubkey,
 mint: &amp;Mint,
 additional_accounts: &amp;[AccountInfo],
) -&gt; Instruction;
This function is very similar to get_additional_accounts_for_permission_transfer_check, but instead it actually gives the full instruction, not just the additional account metas.
Conclusion
While this is just one approach to dynamic on-chain instruction creation / account resolution, with well-defined interfaces, this recursive approach allows programs to construct even the most complicated instructions while on-chain, and without additional CPIs into other programs. Everything must be derivable from the interface, the program, and the required accounts, or the whole model fails.
Implementation: still WIP, will update when it’s ready!","[ngundotra]: Hey Jon!
Here’s a proof of concept that does an on-chain “round trip” for account resolution written in Anchor, but doesn’t require introspecting Mint data.
 
 github.com
 
 
 ngundotra/solana-interface-permissioned-tfer/blob/c87b8fe66c29a59e4b0f9770cc87732d21a9f883/interface/src/lib.rs#L63-L93
 
 
 pub fn call&lt;
 'info,
 C1: ToAccountInfos&lt;'info&gt; + ToAccountMetas + ToTargetProgram&lt;'info, TargetCtx&lt;'info&gt; = C2&gt;,
 C2: ToAccountInfos&lt;'info&gt; + ToAccountMetas,
 &gt;(
 ix_name: String,
 ctx: CpiContext&lt;'_, '_, '_, 'info, C1&gt;,
 log_info: bool,
 ) -&gt; Result&lt;()&gt; {
 msg!(""Preflight"");
 // preflight
 call_preflight_interface_function(ix_name.clone(), &amp;ctx)?;
 
 
 msg!(""Parse return data"");
 // parse cpi return data
 let additional_interface_accounts = get_interface_accounts(&amp;ctx.accounts.to_target_program())?;
 
 
 // execute
 msg!(""Convert into target context"");
 let cpi_ctx: CpiContext&lt;C2&gt; = ctx
 
 
 This file has been truncated. show original
 
 
 
 
 
 
I think this is a super cool paradigm that can be extended beyond Token implementations.
For those interested typescript tests to drive this on-chain account resolution:
test with program A:
 
 github.com
 
 
 ngundotra/solana-interface-permissioned-tfer/blob/main/tests/caller.ts#L155-L181
 
 
 it(""Can lock user token account"", async () =&gt; {
 tokenRecord = PublicKey.findProgramAddressSync(
 [tokenAccount.toBuffer(), Buffer.from(""token_record"")],
 program.programId
 )[0];
 
 
 const lockCtx: LockContext = {
 token: tokenAccount,
 mint,
 delegate: program.provider.publicKey!,
 payer: program.provider.publicKey!,
 tokenProgram: TOKEN_PROGRAM_ID,
 permProgram: program.programId,
 };
 
 
 const builder = caller.methods.lock().accounts(lockCtx);
 let keys = await builder.pubkeys();
 let { accounts: remainingAccounts } = await resolveRemainingAccounts(
 program.provider,
 ""lock"",
 
 
 This file has been truncated. show original
 
 
 
 
 
 
test with program B:
 
 github.com
 
 
 ngundotra/solana-interface-permissioned-tfer/blob/main/tests/caller.ts#L316-L350
 
 
 it(""Can lock user token account"", async () =&gt; {
 tokenRecord = PublicKey.findProgramAddressSync(
 [tokenAccount.toBuffer(), Buffer.from(""token_record"")],
 program.programId
 )[0];
 
 
 let lockCtx: LockContext = {
 token: tokenAccount,
 mint,
 delegate: program.provider.publicKey!,
 payer: program.provider.publicKey!,
 tokenProgram: TOKEN_PROGRAM_ID,
 permProgram: program.programId,
 };
 
 
 const builder = caller.methods.lock().accounts(lockCtx);
 let keys = await builder.pubkeys();
 let { accounts: remainingAccounts } = await resolveRemainingAccounts(
 program.provider,
 ""lock"",
 
 
 This file has been truncated. show original

",joncinque,1315,0,1,2,2023-03-15T15:19:36.966Z,2023-04-04T21:40:41.500Z,sRFC,6,2023-04-04T21:40:41.500Z
25,sRFC 00002: Off-Chain Instruction Account Resolution,https://forum.solana.com/t/srfc-00002-off-chain-instruction-account-resolution/25,"Off-Chain Accounts Resolution for Transaction Creation
Summary
This RFC proposes a solution for collecting all the Accounts needed for a Solana transaction before the transaction is sent to the network. By introducing a new field to the Anchor IDL called invocations and providing a mechanism to insert accounts directly from an on-chain account, we aim to enable clients to resolve all required accounts for a transaction with a single round-trip call to RPC operators for on-chain information.
Goals
Enable clients to resolve all required accounts for a transaction using a single round-trip call to RPC operators.
Provide a deterministic account ordering during transaction creation, allowing programs on-chain to unpack accounts deterministically.
Background
Currently, Solana transactions require all accounts used during execution to be passed in when the transaction is created by the client. Programs can expose information about how to deserialize and serialize their state information and program instructions through their IDL (interface description language).
Proposed Implementation
We propose adding a new field to each instruction description in the Anchor IDL called invocations. This field is an ordered array of program address, instruction data, and accounts. Clients can then recursively traverse this list to determine the accounts each invoked program needs to complete execution.
Anchor IDL with invocations field
{
 ""name"": ""transfer"",
 ""accounts"": [...],
 ""args"": [...],
 ""invocations"": [
 {
 ""program_address"": ""pubkey"",
 ""instruction_data"": ""data"",
 ""accounts"": [...]
 },
 ...
 ]
}
To handle cases where accounts have to be hardcoded, we should provide a mechanism that allows clients to insert accounts into a transaction directly from an on-chain account. This can be helpful but may also present a potential vector for crafting malicious transactions.
With these two tools, it should be possible to symbolically define the invocations ordered list so that clients can determine the full list of accounts that a program uses in a single RPC call. The full symbolic language will need further specification and design, focusing on conditionals based on instruction data and account names.
Example IDL Instruction for NFT Program’s “transfer”
{
 ""name"": ""transfer"",
 ""accounts"": [
 ...
 ],
 ""args"": [
 ...
 ],
 ""invocations"": [
 {
 ""program_address"": ""pubkey"",
 ""instruction_data"": ""data"",
 ""accounts"": [...]
 },
 ...
 ]
}
By implementing the proposed solution, we aim to improve the process of resolving accounts required for a Solana transaction and ensure deterministic ordering for unpacking accounts by programs on-chain.
Implementation
TBD","[blockiosaurus]: Random suggestion, but what if instead of manually recording invocations we add a more automated approach that collated retrieved accounts at simulation time? It’s a more involved approach, but something I’ve been mulling over and something I think would be scalable and require less work by users in the long run.
Add compilation to WASM for instructions/processors so Solana programs can be called from a client.
Add a mock interface for all Solana specific tasks so basically NOOP so the program can be executed on the client. This is the most work but probably has use cases beyond account resolution.
Add extra code to the #[account] attribute that adds an expression to the deserialization step on simulation. When the data is retrieved from the “account” during simulation, the account address that is retrieved from will be stored in a Set. Therefore any account read from, whether it’s a dynamically determined PDA or static account, will be tracked and added to the set.
After this simulation step, a Set is returned with a complete list of accounts used by the instruction.
Limitations: Account addresses that depend on non-deterministic data (random numbers, slot time, or derivation from on-chain state that is subject to change) may not function with this method.

[ngundotra]: blockiosaurus:
we add a more automated approach that collated retrieved accounts at simulation time? It’s a more involved approach, but something I’ve been mulling over and something I think would be scalable and require less work by users in the long run.
I think this would work technically, but I have 2 concerns:
Brute forcing transaction simulation to figure out missing accounts breaks the account design constraints that all Solana programs are built with. Accounts are a first-class constraint across the entirety of the Solana runtime. All program development implicitly requires knowledge of the accounts needed during instruction execution. Changing the runtime to support a method of execution to resolve accounts via simulation feels like a direct violation of this primary design constraint for programs.
Figuring out a format for programs to emit missing accounts during simulation will end up being the same as putting required accounts into the IDL. At the point where we start adding branching logic to which accounts are required for any given instruction, then we might as well go all the way and design a language to express which accounts are needed, conditioned on other account data or instruction data. In my head, this ends up being the same approach I have described above.
 blockiosaurus:
Add extra code to the #[account] attribute that adds an expression to the deserialization step on simulation. When the data is retrieved from the “account” during simulation, the account address that is retrieved from will be stored in a Set. Therefore any account read from, whether it’s a dynamically determined PDA or static account, will be tracked and added to the set.
I think this is really smart, and very doable. However, I think this approach adds compute cost to programs that implement this approach, and is otherwise untenable for frozen programs. For compute-restricted programs, like DeFi, this will probably be unusable.
But I think if you hack away at this, you may discover solutions around this 
Quick note on the original sRFC feasibility:
I think implementing this sRFC could literally be as simple as adding #[idl_annotation(cpi(program, args))] over instruction enums, like shank does. This would not require modifying on-chain code to generate an updated IDL, but would automatically generate the corresponding invocation graph in the IDL. ¯_(ツ)_/¯ hope this helps add some concrete detail until I get around to working on an implementation

[blockiosaurus]: I think I overloaded the term “simulation” in my original post . My meaning was that all IX/Processors would be compiled to WASM and executable on the client without using RPC or on-chain simulation. Simulation would be done on the client.The #[account] attribute code that keeps track of needed accounts would only be added when compiled for WASM.
But I agree that the original sRFC makes the most sense and would hugely beneficial.

",ngundotra,764,2,3,4,2023-02-24T22:18:40.146Z,2023-03-25T21:13:02.558Z,sRFC,6,2023-03-25T21:13:02.558Z
32,sRFC 00004: Native Events Program,https://forum.solana.com/t/srfc-00004-native-events-program/32,"Solana Events for Programs
Summary
This RFC proposes a native “Event” program built into the Solana runtime to address the current limitations in log storage and validation. By implementing a standardized event interface, we aim to achieve the following goals:
Logs should be available as long as Solana blocks are available.
There must be a way to validate that logs (or events) returned by the RPC operators are stored in the block and are truthful.
There must be a clear pricing curve for permanent log storage on Solana, either implicit or explicit.
Problem Description
Currently, Solana logs are truncated to 10kb max per transaction, and there is no process to find “untruncated” logs stored in blocks or to validate the logs returned by RPC nodes. Solana applications work around log-truncation by calling another program (like a “no operation” program) and sending the desired log as instruction data. Such invocations are reliably returned by RPC operators, but they could maliciously (or accidentally) drop history of invocations, and nobody would be able to verify without replaying the historical transaction.
It’s worth noting that EVM chains index their programs only through events, since program data layout is theoretically unspecified. Indexing solana programs only through logs is currently not feasible until there is a reliable way to retrieve them.
Goals
Define an “Event” interface for Solana programs that allows logs to be stored for as long as Solana blocks are available.
Implement a mechanism to validate that the logs returned by RPC operators are accurate and stored in the block.
Establish a clear pricing curve for log storage on Solana.
Possible Implementation
High level summary
Expose a new program built-in to the runtime, that merkelizes logs emitted over the course of a block and writes the merkle tree root to a read-only account via PDA (e.g. [block_number.to_le_bytes()]) at the end of the block. These are referred to as EventStorage accounts. This should allow dApps to reliably use this built-in program’s logs for app-critical needs such as indexing.
Event Instruction and Storage
We can define an event program for Solana that stores event data into a Solana block, hashes the event data in the whole block into a 32 byte hash, and stores the hash into an EventStorage account. This will allow logs to be recovered from block information, and allow downstream clients to reliably verify RPC responses.
/// Event program instruction
enum EventProgramInstruction {
 /// `LogEvent` instruction that takes an `Event` as input 
 /// and logs the event to the on-chain storage.
 LogEvent(Vec&lt;u8&gt;)
}
/// EventStorage account
pub struct EventStorage {
 pub block_id: u64,
 pub events_hash: u32
}
/// Event struct returned by RPC operators
struct Event {
 pub block_index: u16, 
 pub event_data: Vec&lt;u8&gt;
}
Log Validation
To validate that the logs returned by RPC operators are accurate and stored in the block, we can ask RPC nodes to return the Merkle proof along with the requested logs, allowing clients to verify the logs’ authenticity against the Merkle root stored in the EventStorage account.
Pricing Curve
To establish a clear pricing curve for log storage on Solana, we can implement the following:
Modify the LogEvent instruction to charge a storage fee, paid in native tokens, based on the size of the event data to the feePayer of the executing transaction.
Introduce a StorageFee struct that contains the base storage fee and the additional fee per byte of event data that can be modified via SIMD proposal.
Alternative Solutions
We could instead just add a pricing mechanism for existing logs emitted via sol_log_data that requires RPC operates to store them permanently. This could be as simple as increasing the CU for log syscalls to reflect an explicit “price per byte” of a transaction. If transaction logs are usually capped at 10 KiB, and a single transaction has a flat cost of 5000 lamports (at minimum, for a single signer), then each byte of transaction execution record is implicitly priced at 0.5 lamports per byte.
Currently, this low cost of adding to the Solana ledger incentivizes applications to run their own validator to retrieve logs their programs have emitted. This can be both a time-expensive and financially-expensive operation for an application to run their own validator to simply index &amp; debug their own program.
Perhaps by introducing a separate fee schedule for logging that has price per byte &gt; 0.5 lamports/byte, we can discourage the volume of logging per transaction, to less than 10 KiB. This would allow most currently existing RPC operators to comply with an SLA to consistently deliver complete logs for each transaction.
Recommended Implementation
We recommend implementing the built-in event program and storage as described above. This approach would address the current limitations in log storage and validation while also providing clear pricing for writing data to the Solana ledger.
There are 2 issues with the alternative approach of simply adding explicit transaction fees for logging.
One, existing programs that perform logging will now have increased fees, which may cause unfair competitive disadvantage between DeFi dApps. Two, the only mechanism to validate transaction logs returned by RPC operators is to run your own validator &amp; compare transaction logs after transaction replay.
In comparison, the built-in program also provides a new mechanism to define log storage, but provides a superior method to validate logs that reduces burden on dApps. Thus we recommend the built-in event program solution, and encourage further implementation research.","[joec]: Cool!
So, just for clarity: you basically take all of the emitted logs for a block, hash them into a merkle tree, store the root in the account for that blockhash, and can verify all logs emitted during that block are valid via the merkle proof?
So with this implementation, one could get their program logs from their RPC provider, and the RPC provider can also provide the merkle proof to validate that those logs are represented in the stored account?
aka, “proof of logging” 
Since the suggested program is proposed to be embedded in the runtime, I’m curious how we could possibly expand/leverage this proposal to solve for the issue of truncating the logs as well.
Thinking about something like persistent logging or a logging retention pipeline, if an enterprise-scale large program wanted to retain lots of logs for a period of time and also have the validation proposed here that those logs are accurate and represented within the block, how can we possibly integrate un-truncating logs?

[ngundotra]: joec:
you basically take all of the emitted logs for a block, hash them into a merkle tree, store the root in the account for that blockhash, and can verify all logs emitted during that block are valid via the merkle proof?
Yes and instead of logging, they’ll be executing a syscall to the event program, which does the hashing at the end of the block, and posts the merkle root to an EventStorage account on-chain. This should also increase the fee charged to the transaction feePayer, proportional to the amount of data that was emitted to the event program.
 joec:
Thinking about something like persistent logging or a logging retention pipeline, if an enterprise-scale large program wanted to retain lots of logs for a period of time and also have the validation proposed here that those logs are accurate and represented within the block, how can we possibly integrate un-truncating logs?
If an enterprise-scale large programs want to store “logs” (aka sol_log_data logs) then they should run their own validator &amp; only trust the logs that are emitted from replaying their programs own transactions. This could potentially be a good business for RPC operators.
I think it’s fine to make current “logs” a very transient &amp; unreliable source of program metadata, and then encourage RPC operators to charge customers to serve program logs, since they are additional burden on RPC operation.

[santhosh]: I think it would be great if there was a reliable way to associate events with programs that emitted them!

[ngundotra]: Here’s a concrete example of program control flow that is uninterpretable without access to full logs for transaction history.
These two transaction look like they both have the same control flow, but the second inner instruction is actually different.
 
 
 explorer.solana.com
 
 
 
Explorer | Solana
 
Look up transactions and accounts on the various Solana clusters
 
 
 
 
 
 
Program A → Program B
Program A → Program C
 
 
 explorer.solana.com
 
 
 
Explorer | Solana
 
Look up transactions and accounts on the various Solana clusters
 
 
 
 
 
 
Program A → Program B
Program B → Program C
With full logs, we would be able to index the 2nd inner instruction properly, but when we only have access to truncated logs, like in the 2nd transaction, we cannot tell which program invoked program C.
Just wanted to add this example to the discussion to emphasize the importance of this primitive for indexing programs. Personally I don’t think there are any RPC providers that offer “full logs” as a service, but having a few different ones offering that service would solve this problem entirely.
Code for the repro transactions here: GitHub - ngundotra/solana-event-logging-repro: indexing is hard

",ngundotra,1298,1,4,5,2023-03-15T15:52:01.989Z,2023-03-25T00:00:39.830Z,sRFC,6,2023-03-25T00:00:39.830Z
44,sRFC 00005: An implementation of On-chain image storage to the current metaplex nft standard,https://forum.solana.com/t/srfc-00005-an-implementation-of-on-chain-image-storage-to-the-current-metaplex-nft-standard/44,"An implementation of On-chain image storage to the current metaplex nft standard 
This standard utilizes the existing fields of the metaplex nft program to be able to store an on-chain image
Implementation: 
Creating this topic to discuss how I stored images on-chain on solana with the NFT project known as Blockrons as a proof of concept.
Intro:
Blockrons was built in december 2022, before there were much interest on on-chain solana nfts. The goal of the project is 2 fold, implement a true on-chain nft as well as educate the degen/twitter audience on why/how on-chain nfts are done on solana.
Image Data:
The image data is stored as a string directly to a solana address. The format is base 64 SVG, which is a great format to store pixel art. The idea was not to complicate things with advanced struct or json’s but rather simply store the data in an address in such a way that a simple data retrieval call on the solana address will return the image data without needing additional decoding or struct deserialization.
The string also uses a data URL header, this allows browsers to native read the string data into an image. Therefore all strings start with: “data:image/svg+xml;base64,(data here)”
Connection to the NFT token:
To connect this PDA image storage account to an existing metaplex NFT, we use 3 methods.
Method 1 is write the image storage account on the NFT description, however since the description itself is in a json stored off-chain, this is not a good implementation.
Method 2 is the fact that the image storage account is a PDA generated with the NFT token pubkey as a seed (the other half of the seed is the programID). This makes it easy to retrieve the image storage account knowing just the programID and the nft token pubkey.
Method 3 is writing the image storage account directly to the URI field. This is done by appending the solana account pubkey after the arweave (off-chain) link. Wallets and marketplace still resolve to the arweave link, we can keep the image storage account written there for manual image retrieval.
Screenshot 2023-03-19 at 11-55-24 Solscan - The most intuitive Solana explorer2474×1492 349 KB
You can see the highlighted green area above to show how the image storage account was added to the URI field.
Testing:
If you would like to test how images are stored, visit this link: https://onchain-datagrabber-developerhamster.vercel.app/
This website was built to showcase to the audience how the images are stored and retrieved in a step by step process. If I were to automatically retrieve the image on-chain, then how can you illustrate/educate to the audience that the images are on-chain?
This proposal aims to introduce the “/?onchain=” append to the metaplex URI field as a standard way to store on-chain image accounts.","[ngundotra]: Hey Hamster!
Thank you for your proposal on writing Solana NFT images into Solana accounts. After reviewing your RFC, I think to highlight that it actually comprises two separate RFCs.
1. Writing &amp; Reading SVG Images as PDAs of a Solana Program
This portion of the proposal discusses storing image data in base64 SVG format directly to a Solana address, using a data URL header to make it natively readable by browsers. The image storage account is a PDA generated with the NFT token pubkey as a seed, making it easy to retrieve the image knowing just the programID and the NFT token pubkey.
It’s very important that you attach a Proof of Concept implementation, or an existing program’s implementation to define this RFC.
2. URI Structure of Metaplex NFTs with Optional Fetching of Image Data
This part of the proposal outlines three methods to connect the PDA image storage account to an existing Metaplex NFT. The suggested approach is to append the Solana account pubkey after the Arweave (off-chain) link in the URI field, and introduce the “/?onchain=” append to the Metaplex URI field as a standard way to store on-chain image accounts.
I think it’d be awesome if you could rewrite #1 as a separate RFC, and #2 is fine as an RFC here, but I think it would need to be submitted to Metaplex separately as well, so they have some awareness.
Looking forward to hearing from you soon!

[Hamster]: I appreciate the response. It seems like metaplex will be going their own direction with this as they are in full control of the standard currently.
I will still continue my sRFC proposal as they might be of use to any future developer looking to learn more or tackle this subject.
For RFC #2: URI Structure of Metaplex NFTs with Optional Fetching of Image Data
I used a solana account that directly contains an image but its better to follow the current standard of first pointing to solana account containing a .json struct, then inside that .json we can have another solana account containing the image.
So I propose we use 2 solana accounts, one for the json and one for the image, similar to how arweave is used now.

",Hamster,1163,1,2,3,2023-03-19T05:00:01.151Z,2023-03-21T14:58:02.737Z,sRFC,6,2023-03-21T14:58:02.737Z
13,About the sRFC category,https://forum.solana.com/t/about-the-srfc-category/13,Solana Request For Comments category is geared towards discussions between Solana developers about application standards.,"[jacobcreech]: 

",jacobcreech,432,0,1,2,2023-02-23T01:34:03.721Z,2023-02-24T05:13:14.541Z,sRFC,6,2023-02-24T05:13:14.541Z
